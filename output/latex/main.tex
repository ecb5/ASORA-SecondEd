%********************************************%
%*       Generated from PreTeXt source      *%
%*       on 2025-06-06T13:34:30-04:00       *%
%*   A recent stable commit (2022-07-01):   *%
%* 6c761d3dba23af92cba35001c852aac04ae99a5f *%
%*                                          *%
%*         https://pretextbook.org          *%
%*                                          *%
%********************************************%
\documentclass[oneside,10pt,]{book}
%% Custom Preamble Entries, early (use latex.preamble.early)
%% Default LaTeX packages
%%   1.  always employed (or nearly so) for some purpose, or
%%   2.  a stylewriter may assume their presence
\usepackage{geometry}
%% Some aspects of the preamble are conditional,
%% the LaTeX engine is one such determinant
\usepackage{ifthen}
%% etoolbox has a variety of modern conveniences
\usepackage{etoolbox}
\usepackage{ifxetex,ifluatex}
%% Raster graphics inclusion
\usepackage{graphicx}
%% Color support, xcolor package
%% Always loaded, for: add/delete text, author tools
%% Here, since tcolorbox loads tikz, and tikz loads xcolor
\PassOptionsToPackage{dvipsnames,svgnames,table}{xcolor}
\usepackage{xcolor}
%% begin: defined colors, via xcolor package, for styling
%% end: defined colors, via xcolor package, for styling
%% Colored boxes, and much more, though mostly styling
%% skins library provides "enhanced" skin, employing tikzpicture
%% boxes may be configured as "breakable" or "unbreakable"
%% "raster" controls grids of boxes, aka side-by-side
\usepackage{tcolorbox}
\tcbuselibrary{skins}
\tcbuselibrary{breakable}
\tcbuselibrary{raster}
%% We load some "stock" tcolorbox styles that we use a lot
%% Placement here is provisional, there will be some color work also
%% First, black on white, no border, transparent, but no assumption about titles
\tcbset{ bwminimalstyle/.style={size=minimal, boxrule=-0.3pt, frame empty,
colback=white, colbacktitle=white, coltitle=black, opacityfill=0.0} }
%% Second, bold title, run-in to text/paragraph/heading
%% Space afterwards will be controlled by environment,
%% independent of constructions of the tcb title
%% Places \blocktitlefont onto many block titles
\tcbset{ runintitlestyle/.style={fonttitle=\blocktitlefont\upshape\bfseries, attach title to upper} }
%% Spacing prior to each exercise, anywhere
\tcbset{ exercisespacingstyle/.style={before skip={1.5ex plus 0.5ex}} }
%% Spacing prior to each block
\tcbset{ blockspacingstyle/.style={before skip={2.0ex plus 0.5ex}} }
%% xparse allows the construction of more robust commands,
%% this is a necessity for isolating styling and behavior
%% The tcolorbox library of the same name loads the base library
\tcbuselibrary{xparse}
%% The tcolorbox library loads TikZ, its calc package is generally useful,
%% and is necessary for some smaller documents that use partial tcolor boxes
%% See:  https://github.com/PreTeXtBook/pretext/issues/1624
\usetikzlibrary{calc}
%% We use some more exotic tcolorbox keys to restore indentation to parboxes
\tcbuselibrary{hooks}
%% Save default paragraph indentation and parskip for use later, when adjusting parboxes
\newlength{\normalparindent}
\newlength{\normalparskip}
\AtBeginDocument{\setlength{\normalparindent}{\parindent}}
\AtBeginDocument{\setlength{\normalparskip}{\parskip}}
\newcommand{\setparstyle}{\setlength{\parindent}{\normalparindent}\setlength{\parskip}{\normalparskip}}%% Hyperref should be here, but likes to be loaded late
%%
%% Inline math delimiters, \(, \), need to be robust
%% 2016-01-31:  latexrelease.sty  supersedes  fixltx2e.sty
%% If  latexrelease.sty  exists, bugfix is in kernel
%% If not, bugfix is in  fixltx2e.sty
%% See:  https://tug.org/TUGboat/tb36-3/tb114ltnews22.pdf
%% and read "Fewer fragile commands" in distribution's  latexchanges.pdf
\IfFileExists{latexrelease.sty}{}{\usepackage{fixltx2e}}
%% shorter subnumbers in some side-by-side require manipulations
\usepackage{xstring}
%% Footnote counters and part/chapter counters are manipulated
%% April 2018:  chngcntr  commands now integrated into the kernel,
%% but circa 2018/2019 the package would still try to redefine them,
%% so we need to do the work of loading conditionally for old kernels.
%% From version 1.1a,  chngcntr  should detect defintions made by LaTeX kernel.
\ifdefined\counterwithin
\else
    \usepackage{chngcntr}
\fi
%% Text height identically 9 inches, text width varies on point size
%% See Bringhurst 2.1.1 on measure for recommendations
%% 75 characters per line (count spaces, punctuation) is target
%% which is the upper limit of Bringhurst's recommendations
\geometry{letterpaper,total={340pt,9.0in}}
%% Custom Page Layout Adjustments (use publisher page-geometry entry)
%% This LaTeX file may be compiled with pdflatex, xelatex, or lualatex executables
%% LuaTeX is not explicitly supported, but we do accept additions from knowledgeable users
%% The conditional below provides  pdflatex  specific configuration last
%% begin: engine-specific capabilities
\ifthenelse{\boolean{xetex} \or \boolean{luatex}}{%
%% begin: xelatex and lualatex-specific default configuration
\ifxetex\usepackage{xltxtra}\fi
%% realscripts is the only part of xltxtra relevant to lualatex 
\ifluatex\usepackage{realscripts}\fi
%% end:   xelatex and lualatex-specific default configuration
}{
%% begin: pdflatex-specific default configuration
%% We assume a PreTeXt XML source file may have Unicode characters
%% and so we ask LaTeX to parse a UTF-8 encoded file
%% This may work well for accented characters in Western language,
%% but not with Greek, Asian languages, etc.
%% When this is not good enough, switch to the  xelatex  engine
%% where Unicode is better supported (encouraged, even)
\usepackage[utf8]{inputenc}
%% end: pdflatex-specific default configuration
}
%% end:   engine-specific capabilities
%%
%% Fonts.  Conditional on LaTex engine employed.
%% Default Text Font: The Latin Modern fonts are
%% "enhanced versions of the [original TeX] Computer Modern fonts."
%% We use them as the default text font for PreTeXt output.
%% Automatic Font Control
%% Portions of a document, are, or may, be affected by defined commands
%% These are perhaps more flexible when using  xelatex  rather than  pdflatex
%% The following definitions are meant to be re-defined in a style, using \renewcommand
%% They are scoped when employed (in a TeX group), and so should not be defined with an argument
\newcommand{\divisionfont}{\relax}
\newcommand{\blocktitlefont}{\relax}
\newcommand{\contentsfont}{\relax}
\newcommand{\pagefont}{\relax}
\newcommand{\tabularfont}{\relax}
\newcommand{\xreffont}{\relax}
\newcommand{\titlepagefont}{\relax}
%%
\ifthenelse{\boolean{xetex} \or \boolean{luatex}}{%
%% begin: font setup and configuration for use with xelatex
%% Generally, xelatex is necessary for non-Western fonts
%% fontspec package provides extensive control of system fonts,
%% meaning *.otf (OpenType), and apparently *.ttf (TrueType)
%% that live *outside* your TeX/MF tree, and are controlled by your *system*
%% (it is possible that a TeX distribution will place fonts in a system location)
%%
%% The fontspec package is the best vehicle for using different fonts in  xelatex
%% So we load it always, no matter what a publisher or style might want
%%
\usepackage{fontspec}
%%
%% begin: xelatex main font ("font-xelatex-main" template)
%% Latin Modern Roman is the default font for xelatex and so is loaded with a TU encoding
%% *in the format* so we can't touch it, only perhaps adjust it later
%% in one of two ways (then known by NFSS names such as "lmr")
%% (1) via NFSS with font family names such as "lmr" and "lmss"
%% (2) via fontspec with commands like \setmainfont{Latin Modern Roman}
%% The latter requires the font to be known at the system-level by its font name,
%% but will give access to OTF font features through optional arguments
%% https://tex.stackexchange.com/questions/470008/
%% where-and-how-does-fontspec-sty-specify-the-default-font-latin-modern-roman
%% http://tex.stackexchange.com/questions/115321
%% /how-to-optimize-latin-modern-font-with-xelatex
%%
%% end:   xelatex main font ("font-xelatex-main" template)
%% begin: xelatex mono font ("font-xelatex-mono" template)
%% (conditional on non-trivial uses being present in source)
%% end:   xelatex mono font ("font-xelatex-mono" template)
%% begin: xelatex font adjustments ("font-xelatex-style" template)
%% end:   xelatex font adjustments ("font-xelatex-style" template)
%%
%% Extensive support for other languages
\usepackage{polyglossia}
%% Set main/default language based on pretext/@xml:lang value
%% document language code is "en-US", US English
%% usmax variant has extra hypenation
\setmainlanguage[variant=usmax]{english}
%% Enable secondary languages based on discovery of @xml:lang values
%% Enable fonts/scripts based on discovery of @xml:lang values
%% Western languages should be ably covered by Latin Modern Roman
%% end:   font setup and configuration for use with xelatex
}{%
%% begin: font setup and configuration for use with pdflatex
%% begin: pdflatex main font ("font-pdflatex-main" template)
\usepackage{lmodern}
\usepackage[T1]{fontenc}
%% end:   pdflatex main font ("font-pdflatex-main" template)
%% begin: pdflatex mono font ("font-pdflatex-mono" template)
%% (conditional on non-trivial uses being present in source)
%% end:   pdflatex mono font ("font-pdflatex-mono" template)
%% begin: pdflatex font adjustments ("font-pdflatex-style" template)
%% end:   pdflatex font adjustments ("font-pdflatex-style" template)
%% end:   font setup and configuration for use with pdflatex
}
%% Micromanage spacing, etc.  The named "microtype-options"
%% template may be employed to fine-tune package behavior
\usepackage{microtype}
%% Symbols, align environment, commutative diagrams, bracket-matrix
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
%% allow page breaks within display mathematics anywhere
%% level 4 is maximally permissive
%% this is exactly the opposite of AMSmath package philosophy
%% there are per-display, and per-equation options to control this
%% split, aligned, gathered, and alignedat are not affected
\allowdisplaybreaks[4]
%% allow more columns to a matrix
%% can make this even bigger by overriding with  latex.preamble.late  processing option
\setcounter{MaxMatrixCols}{30}
%%
%%
%% Division Titles, and Page Headers/Footers
%% titlesec package, loading "titleps" package cooperatively
%% See code comments about the necessity and purpose of "explicit" option.
%% The "newparttoc" option causes a consistent entry for parts in the ToC 
%% file, but it is only effective if there is a \titleformat for \part.
%% "pagestyles" loads the  titleps  package cooperatively.
\usepackage[explicit, newparttoc, pagestyles]{titlesec}
%% The companion titletoc package for the ToC.
\usepackage{titletoc}
%% Fixes a bug with transition from chapters to appendices in a "book"
%% See generating XSL code for more details about necessity
\newtitlemark{\chaptertitlename}
%% begin: customizations of page styles via the modal "titleps-style" template
%% Designed to use commands from the LaTeX "titleps" package
%% Plain pages should have the same font for page numbers
\renewpagestyle{plain}{%
\setfoot{}{\pagefont\thepage}{}%
}%
%% Single pages as in default LaTeX
\renewpagestyle{headings}{%
\sethead{\pagefont\slshape\MakeUppercase{\ifthechapter{\chaptertitlename\space\thechapter.\space}{}\chaptertitle}}{}{\pagefont\thepage}%
}%
\pagestyle{headings}
%% end: customizations of page styles via the modal "titleps-style" template
%%
%% Create globally-available macros to be provided for style writers
%% These are redefined for each occurence of each division
\newcommand{\divisionnameptx}{\relax}%
\newcommand{\titleptx}{\relax}%
\newcommand{\subtitleptx}{\relax}%
\newcommand{\shortitleptx}{\relax}%
\newcommand{\authorsptx}{\relax}%
\newcommand{\epigraphptx}{\relax}%
%% Create environments for possible occurences of each division
%% Environment for a PTX "acknowledgement" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{acknowledgement}{mmmmmmm}
{%
\renewcommand{\divisionnameptx}{#1}%
\renewcommand{\titleptx}{#2}%
\renewcommand{\subtitleptx}{#3}%
\renewcommand{\shortitleptx}{#4}%
\renewcommand{\authorsptx}{#5}%
\renewcommand{\epigraphptx}{#6}%
\chapter*{#2}%
\addcontentsline{toc}{chapter}{#4}
\label{#7}%
}{}%
%% Environment for a PTX "preface" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{preface}{mmmmmmm}
{%
\renewcommand{\divisionnameptx}{#1}%
\renewcommand{\titleptx}{#2}%
\renewcommand{\subtitleptx}{#3}%
\renewcommand{\shortitleptx}{#4}%
\renewcommand{\authorsptx}{#5}%
\renewcommand{\epigraphptx}{#6}%
\chapter*{#2}%
\addcontentsline{toc}{chapter}{#4}
\label{#7}%
}{}%
%% Environment for a PTX "part" at the level of a LaTeX "part"
\NewDocumentEnvironment{partptx}{mmmmmmm}
{%
\renewcommand{\divisionnameptx}{#1}%
\renewcommand{\titleptx}{#2}%
\renewcommand{\subtitleptx}{#3}%
\renewcommand{\shortitleptx}{#4}%
\renewcommand{\authorsptx}{#5}%
\renewcommand{\epigraphptx}{#6}%
\part[{#4}]{#2}%
\label{#7}%
}{}%
%% Environment for a PTX "chapter" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{chapterptx}{mmmmmmm}
{%
\renewcommand{\divisionnameptx}{#1}%
\renewcommand{\titleptx}{#2}%
\renewcommand{\subtitleptx}{#3}%
\renewcommand{\shortitleptx}{#4}%
\renewcommand{\authorsptx}{#5}%
\renewcommand{\epigraphptx}{#6}%
\chapter[{#4}]{#2}%
\label{#7}%
}{}%
%% Environment for a PTX "section" at the level of a LaTeX "section"
\NewDocumentEnvironment{sectionptx}{mmmmmmm}
{%
\renewcommand{\divisionnameptx}{#1}%
\renewcommand{\titleptx}{#2}%
\renewcommand{\subtitleptx}{#3}%
\renewcommand{\shortitleptx}{#4}%
\renewcommand{\authorsptx}{#5}%
\renewcommand{\epigraphptx}{#6}%
\section[{#4}]{#2}%
\label{#7}%
}{}%
%% Environment for a PTX "subsection" at the level of a LaTeX "subsection"
\NewDocumentEnvironment{subsectionptx}{mmmmmmm}
{%
\renewcommand{\divisionnameptx}{#1}%
\renewcommand{\titleptx}{#2}%
\renewcommand{\subtitleptx}{#3}%
\renewcommand{\shortitleptx}{#4}%
\renewcommand{\authorsptx}{#5}%
\renewcommand{\epigraphptx}{#6}%
\subsection[{#4}]{#2}%
\label{#7}%
}{}%
%% Environment for a PTX "references" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{references-chapter}{mmmmmmm}
{%
\renewcommand{\divisionnameptx}{#1}%
\renewcommand{\titleptx}{#2}%
\renewcommand{\subtitleptx}{#3}%
\renewcommand{\shortitleptx}{#4}%
\renewcommand{\authorsptx}{#5}%
\renewcommand{\epigraphptx}{#6}%
\chapter[{#4}]{#2}%
\label{#7}%
}{}%
%% Environment for a PTX "references" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{references-chapter-numberless}{mmmmmmm}
{%
\renewcommand{\divisionnameptx}{#1}%
\renewcommand{\titleptx}{#2}%
\renewcommand{\subtitleptx}{#3}%
\renewcommand{\shortitleptx}{#4}%
\renewcommand{\authorsptx}{#5}%
\renewcommand{\epigraphptx}{#6}%
\chapter*{#2}%
\addcontentsline{toc}{chapter}{#4}
\label{#7}%
}{}%
%% Environment for a PTX "index" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{indexptx}{mmmmmmm}
{%
\renewcommand{\divisionnameptx}{#1}%
\renewcommand{\titleptx}{#2}%
\renewcommand{\subtitleptx}{#3}%
\renewcommand{\shortitleptx}{#4}%
\renewcommand{\authorsptx}{#5}%
\renewcommand{\epigraphptx}{#6}%
\chapter*{#2}%
\addcontentsline{toc}{chapter}{#4}
\label{#7}%
}{}%
%%
%% Styles for six traditional LaTeX divisions
\titleformat{\part}[display]
{\divisionfont\Huge\bfseries\centering}{\divisionnameptx\space\thepart}{30pt}{\Huge#1}
[{\Large\centering\authorsptx}]
\titleformat{\chapter}[display]
{\divisionfont\huge\bfseries}{\divisionnameptx\space\thechapter}{20pt}{\Huge#1}
[{\Large\authorsptx}]
\titleformat{name=\chapter,numberless}[display]
{\divisionfont\huge\bfseries}{}{0pt}{#1}
[{\Large\authorsptx}]
\titlespacing*{\chapter}{0pt}{50pt}{40pt}
\titleformat{\section}[hang]
{\divisionfont\Large\bfseries}{\thesection}{1ex}{#1}
[{\large\authorsptx}]
\titleformat{name=\section,numberless}[block]
{\divisionfont\Large\bfseries}{}{0pt}{#1}
[{\large\authorsptx}]
\titlespacing*{\section}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\titleformat{\subsection}[hang]
{\divisionfont\large\bfseries}{\thesubsection}{1ex}{#1}
[{\normalsize\authorsptx}]
\titleformat{name=\subsection,numberless}[block]
{\divisionfont\large\bfseries}{}{0pt}{#1}
[{\normalsize\authorsptx}]
\titlespacing*{\subsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titleformat{\subsubsection}[hang]
{\divisionfont\normalsize\bfseries}{\thesubsubsection}{1em}{#1}
[{\small\authorsptx}]
\titleformat{name=\subsubsection,numberless}[block]
{\divisionfont\normalsize\bfseries}{}{0pt}{#1}
[{\normalsize\authorsptx}]
\titlespacing*{\subsubsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titleformat{\paragraph}[hang]
{\divisionfont\normalsize\bfseries}{\theparagraph}{1em}{#1}
[{\small\authorsptx}]
\titleformat{name=\paragraph,numberless}[block]
{\divisionfont\normalsize\bfseries}{}{0pt}{#1}
[{\normalsize\authorsptx}]
\titlespacing*{\paragraph}{0pt}{3.25ex plus 1ex minus .2ex}{1.5em}
%%
%% Styles for five traditional LaTeX divisions
\titlecontents{part}%
[0pt]{\contentsmargin{0em}\addvspace{1pc}\contentsfont\bfseries}%
{\Large\thecontentslabel\enspace}{\Large}%
{}%
[\addvspace{.5pc}]%
\titlecontents{chapter}%
[0pt]{\contentsmargin{0em}\addvspace{1pc}\contentsfont\bfseries}%
{\large\thecontentslabel\enspace}{\large}%
{\hfill\bfseries\thecontentspage}%
[\addvspace{.5pc}]%
\dottedcontents{section}[3.8em]{\contentsfont}{2.3em}{1pc}%
\dottedcontents{subsection}[6.1em]{\contentsfont}{3.2em}{1pc}%
\dottedcontents{subsubsection}[9.3em]{\contentsfont}{4.3em}{1pc}%
%%
%% Begin: Semantic Macros
%% To preserve meaning in a LaTeX file
%%
%% \mono macro for content of "c", "cd", "tag", etc elements
%% Also used automatically in other constructions
%% Simply an alias for \texttt
%% Always defined, even if there is no need, or if a specific tt font is not loaded
\newcommand{\mono}[1]{\texttt{#1}}
%%
%% Following semantic macros are only defined here if their
%% use is required only in this specific document
%%
%% Used to markup initialisms, text or titles
\newcommand{\initialism}[1]{\textsc{\MakeLowercase{#1}}}
\DeclareRobustCommand{\initialismintitle}[1]{\texorpdfstring{#1}{#1}}
%% Used for warnings, typically bold and italic
\newcommand{\alert}[1]{\textbf{\textit{#1}}}
%% Used for inline definitions of terms
\newcommand{\terminology}[1]{\textbf{#1}}
%% Titles of longer works (e.g. books, versus articles)
\newcommand{\pubtitle}[1]{\textsl{#1}}
%% Style of a title on a list item, for ordered and unordered lists
%% Also "task" of exercise, PROJECT-LIKE, EXAMPLE-LIKE
\newcommand{\lititle}[1]{{\slshape#1}}
%% End: Semantic Macros
%% Equation Numbering
%% Controlled by  numbering.equations.level  processing parameter
%% No adjustment here implies document-wide numbering
\numberwithin{equation}{part}
%% "tcolorbox" environment for a single image, occupying entire \linewidth
%% arguments are left-margin, width, right-margin, as multiples of
%% \linewidth, and are guaranteed to be positive and sum to 1.0
\tcbset{ imagestyle/.style={bwminimalstyle} }
\NewTColorBox{tcbimage}{mmm}{imagestyle,left skip=#1\linewidth,width=#2\linewidth}
%% Wrapper environment for tcbimage environment with a fourth argument
%% Fourth argument, if nonempty, is a vertical space adjustment
%% and implies image will be preceded by \leavevmode\nopagebreak
%% Intended use is for alignment with a list marker
\NewDocumentEnvironment{image}{mmmm}{\notblank{#4}{\leavevmode\nopagebreak\vspace{#4}}{}\begin{tcbimage}{#1}{#2}{#3}}{\end{tcbimage}%
}%% Footnote Numbering
%% Specified by numbering.footnotes.level
%% Undo counter reset by chapter for a book
\counterwithout{footnote}{chapter}
%% Global numbering, since numbering.footnotes.level = 0
%% Multiple column, column-major lists
\usepackage{multicol}
%% More flexible list management, esp. for references
%% But also for specifying labels (i.e. custom order) on nested lists
\usepackage{enumitem}
%% Lists of references in their own section, maximum depth 1
\newlist{referencelist}{description}{4}
\setlist[referencelist]{leftmargin=!,labelwidth=!,labelsep=0ex,itemsep=1.0ex,topsep=1.0ex,partopsep=0pt,parsep=0pt}
%% Description lists as tcolorbox sidebyside
%% "dli" short for "description list item"
\newlength{\dlititlewidth}
\newlength{\dlimaxnarrowtitle}\setlength{\dlimaxnarrowtitle}{11ex}
\newlength{\dlimaxmediumtitle}\setlength{\dlimaxmediumtitle}{18ex}
\tcbset{ dlistyle/.style={sidebyside, sidebyside align=top seam, lower separated=false, bwminimalstyle, bottomtitle=0.75ex, after skip=1.5ex, boxsep=0pt, left=0pt, right=0pt, top=0pt, bottom=0pt} }
\tcbset{ dlinarrowstyle/.style={dlistyle, lefthand width=\dlimaxnarrowtitle, sidebyside gap=1ex, halign=flush left, righttitle=10ex} }
\tcbset{ dlimediumstyle/.style={dlistyle, lefthand width=\dlimaxmediumtitle, sidebyside gap=4ex, halign=flush right} }
\NewDocumentEnvironment{descriptionlist}{}{\par\vspace*{1.5ex}}{\par\vspace*{1.5ex}}%
%% begin enviroment has an if/then to open the tcolorbox
\NewDocumentEnvironment{dlinarrow}{mm}{%
\settowidth{\dlititlewidth}{{\textbf{#1}}}%
\ifthenelse{\dlititlewidth > \dlimaxnarrowtitle}%
{\begin{tcolorbox}[title={\textbf{#1}}, phantom={\hypertarget{#2}{}}, dlinarrowstyle]\tcblower}%
{\begin{tcolorbox}[dlinarrowstyle, phantom={\hypertarget{#2}{}}]\textbf{#1}\tcblower}%
}%
{\end{tcolorbox}}%
%% medium option is simpler
\NewDocumentEnvironment{dlimedium}{mm}%
{\begin{tcolorbox}[dlimediumstyle, phantom={\hypertarget{#2}{}}]\textbf{#1}\tcblower}%
{\end{tcolorbox}}%
%% Support for index creation
%% imakeidx package does not require extra pass (as with makeidx)
%% Title of the "Index" section set via a keyword
%% Language support for the "see" and "see also" phrases,
%% but to do so presumes exactly one "index-list" generator is present
\usepackage{imakeidx}
\makeindex[title=Index, intoc=true]
\renewcommand{\seename}{See}
\renewcommand{\alsoname}{See also}
%% hyperref driver does not need to be specified, it will be detected
%% Footnote marks in tcolorbox have broken linking under
%% hyperref, so it is necessary to turn off all linking
%% It *must* be given as a package option, not with \hypersetup
\usepackage[hyperfootnotes=false]{hyperref}
%% configure hyperref's  \href{}{}  and  \nolinkurl  to match listings' inline verbatim
\renewcommand\UrlFont{\small\ttfamily}
%% Hyperlinking active in electronic PDFs, all links without surrounding boxes and blue
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,filecolor=blue,urlcolor=blue}
%% Less-clever names for hyperlinks are more reliable, *especially* for structural parts
%% See comments in the code to learn more about the importance of this setting
\hypersetup{hypertexnames=false}
%%The  hypertexnames  setting then confuses the hyperlinking from the index
%%This patch resolves the incorrect links, see code for StackExchange post.
\makeatletter
\patchcmd\Hy@EveryPageBoxHook{\Hy@EveryPageAnchor}{\Hy@hypertexnamestrue\Hy@EveryPageAnchor}{}{\fail}
\makeatother
\hypersetup{pdftitle={A Contextual Introduction to Real Analysis: How we Got from There to Here}}
%% If you manually remove hyperref, leave in this next command
%% This will allow LaTeX compilation, employing this no-op command
\providecommand\phantomsection{}
%% Division Numbering: Chapters, Sections, Subsections, etc
%% Division numbers may be turned off at some level ("depth")
%% A section *always* has depth 1, contrary to us counting from the document root
%% The latex default is 3.  If a larger number is present here, then
%% removing this command may make some cross-references ambiguous
%% The precursor variable $numbering-maxlevel is checked for consistency in the common XSL file
\setcounter{secnumdepth}{1}
%%
%% AMS "proof" environment is no longer used, but we leave previously
%% implemented \qedhere in place, should the LaTeX be recycled
\newcommand{\qedhere}{\relax}
%%
%% A faux tcolorbox whose only purpose is to provide common numbering
%% facilities for most blocks (possibly not projects, 2D displays)
%% Controlled by  numbering.theorems.level  processing parameter
\newtcolorbox[auto counter, number within=section]{block}{}
%%
%% This document is set to number PROJECT-LIKE on a separate numbering scheme
%% So, a faux tcolorbox whose only purpose is to provide this numbering
%% Controlled by  numbering.projects.level  processing parameter
\newtcolorbox[auto counter]{project-distinct}{}
%% A faux tcolorbox whose only purpose is to provide common numbering
%% facilities for 2D displays which are subnumbered as part of a "sidebyside"
\makeatletter
\newtcolorbox[auto counter, number within=tcb@cnt@block, number freestyle={\noexpand\thetcb@cnt@block(\noexpand\alph{\tcbcounter})}]{subdisplay}{}
\makeatother
%%
%% tcolorbox, with styles, for THEOREM-LIKE
%%
%% theorem: fairly simple numbered block/structure
\tcbset{ theoremstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, before upper app={\setparstyle}, } }
\newtcolorbox[use counter from=block]{theorem}[4]{title={{#1~\thetcbcounter\notblank{#2#3}{\space}{}\notblank{#2}{\space#2}{}\notblank{#3}{\space(#3)}{}}}, phantomlabel={#4}, breakable, after={\par}, fontupper=\itshape, theoremstyle, }
%% lemma: fairly simple numbered block/structure
\tcbset{ lemmastyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, before upper app={\setparstyle}, } }
\newtcolorbox[use counter from=block]{lemma}[4]{title={{#1~\thetcbcounter\notblank{#2#3}{\space}{}\notblank{#2}{\space#2}{}\notblank{#3}{\space(#3)}{}}}, phantomlabel={#4}, breakable, after={\par}, fontupper=\itshape, lemmastyle, }
%% corollary: fairly simple numbered block/structure
\tcbset{ corollarystyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, before upper app={\setparstyle}, } }
\newtcolorbox[use counter from=block]{corollary}[4]{title={{#1~\thetcbcounter\notblank{#2#3}{\space}{}\notblank{#2}{\space#2}{}\notblank{#3}{\space(#3)}{}}}, phantomlabel={#4}, breakable, after={\par}, fontupper=\itshape, corollarystyle, }
%%
%% tcolorbox, with styles, for PROOF-LIKE
%%
%% proof: title is a replacement
\tcbset{ proofstyle/.style={bwminimalstyle, fonttitle=\blocktitlefont\itshape, attach title to upper, after title={\space}, after upper={\space\space\hspace*{\stretch{1}}\(\blacksquare\)},
} }
\newtcolorbox{proof}[3]{title={\notblank{#2}{#2}{#1.}}, phantom={\hypertarget{#3}{}}, breakable, before upper app={\setparstyle}, after={\par}, proofstyle }
%%
%% tcolorbox, with styles, for AXIOM-LIKE
%%
%% principle: fairly simple numbered block/structure
\tcbset{ principlestyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, before upper app={\setparstyle}, } }
\newtcolorbox[use counter from=block]{principle}[4]{title={{#1~\thetcbcounter\notblank{#2#3}{\space}{}\notblank{#2}{\space#2}{}\notblank{#3}{\space(#3)}{}}}, phantomlabel={#4}, breakable, after={\par}, fontupper=\itshape, principlestyle, }
%% conjecture: fairly simple numbered block/structure
\tcbset{ conjecturestyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, before upper app={\setparstyle}, } }
\newtcolorbox[use counter from=block]{conjecture}[4]{title={{#1~\thetcbcounter\notblank{#2#3}{\space}{}\notblank{#2}{\space#2}{}\notblank{#3}{\space(#3)}{}}}, phantomlabel={#4}, breakable, after={\par}, fontupper=\itshape, conjecturestyle, }
%% axiom: fairly simple numbered block/structure
\tcbset{ axiomstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, before upper app={\setparstyle}, } }
\newtcolorbox[use counter from=block]{axiom}[4]{title={{#1~\thetcbcounter\notblank{#2#3}{\space}{}\notblank{#2}{\space#2}{}\notblank{#3}{\space(#3)}{}}}, phantomlabel={#4}, breakable, after={\par}, fontupper=\itshape, axiomstyle, }
%%
%% tcolorbox, with styles, for DEFINITION-LIKE
%%
%% definition: fairly simple numbered block/structure
\tcbset{ definitionstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, before upper app={\setparstyle}, after upper={\space\space\hspace*{\stretch{1}}\(\lozenge\)}, } }
\newtcolorbox[use counter from=block]{definition}[3]{title={{#1~\thetcbcounter\notblank{#2}{\space\space#2}{}}}, phantomlabel={#3}, breakable, after={\par}, definitionstyle, }
%%
%% tcolorbox, with styles, for EXAMPLE-LIKE
%%
%% problem: fairly simple numbered block/structure
\tcbset{ problemstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, before upper app={\setparstyle}, after upper={\space\space\hspace*{\stretch{1}}\(\square\)}, } }
\newtcolorbox[use counter from=block]{problem}[3]{title={{#1~\thetcbcounter\notblank{#2}{\space\space#2}{}}}, phantomlabel={#3}, breakable, after={\par}, problemstyle, }
%% example: fairly simple numbered block/structure
\tcbset{ examplestyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, before upper app={\setparstyle}, after upper={\space\space\hspace*{\stretch{1}}\(\square\)}, } }
\newtcolorbox[use counter from=block]{example}[3]{title={{#1~\thetcbcounter\notblank{#2}{\space\space#2}{}}}, phantomlabel={#3}, breakable, after={\par}, examplestyle, }
%%
%% tcolorbox, with styles, for ASIDE-LIKE
%%
%% aside: fairly simple un-numbered block/structure
\tcbset{ asidestyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, before upper app={\setparstyle}, } }
\newtcolorbox{aside}[3]{title={\notblank{#2}{#2}{}}, phantomlabel={#3}, breakable, before upper app={\setparstyle}, asidestyle}
%% historical: fairly simple un-numbered block/structure
\tcbset{ historicalstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, before upper app={\setparstyle}, } }
\newtcolorbox{historical}[3]{title={\notblank{#2}{#2}{}}, phantomlabel={#3}, breakable, before upper app={\setparstyle}, historicalstyle}
%%
%% tcolorbox, with styles, for FIGURE-LIKE
%%
%% figureptx: 2-D display structure
\tcbset{ figureptxstyle/.style={bwminimalstyle, middle=1ex, blockspacingstyle, fontlower=\blocktitlefont} }
\newtcolorbox[use counter from=block]{figureptx}[4]{lower separated=false, before lower={{\textbf{#1~\thetcbcounter}\space#2}}, phantomlabel={#3}, unbreakable, figureptxstyle, }
%%
%% xparse environments for introductions and conclusions of divisions
%%
%% introduction: in a structured division
\NewDocumentEnvironment{introduction}{m}
{\notblank{#1}{\noindent\textbf{#1}\space}{}}{\par\medskip}
%% Graphics Preamble Entries

%% If tikz has been loaded, replace ampersand with \amp macro
%% tcolorbox styles for sidebyside layout
\tcbset{ sbsstyle/.style={raster before skip=2.0ex, raster equal height=rows, raster force size=false, raster after skip=0.7\baselineskip} }
\tcbset{ sbspanelstyle/.style={bwminimalstyle, fonttitle=\blocktitlefont, before upper app={\setparstyle}} }
%% Enviroments for side-by-side and components
%% Necessary to use \NewTColorBox for boxes of the panels
%% "newfloat" environment to squash page-breaks within a single sidebyside
%% "xparse" environment for entire sidebyside
\NewDocumentEnvironment{sidebyside}{mmmm}
  {\begin{tcbraster}
    [sbsstyle,raster columns=#1,
    raster left skip=#2\linewidth,raster right skip=#3\linewidth,raster column skip=#4\linewidth]}
  {\end{tcbraster}}
%% "tcolorbox" environment for a panel of sidebyside
\NewTColorBox{sbspanel}{mO{top}}{sbspanelstyle,width=#1\linewidth,valign=#2}
%% Custom Preamble Entries, late (use latex.preamble.late)
%% extpfeil package for certain extensible arrows,
%% as also provided by MathJax extension of the same name
%% NB: this package loads mtools, which loads calc, which redefines
%%     \setlength, so it can be removed if it seems to be in the 
%%     way and your math does not use:
%%     
%%     \xtwoheadrightarrow, \xtwoheadleftarrow, \xmapsto, \xlongequal, \xtofrom
%%     
%%     we have had to be extra careful with variable thickness
%%     lines in tables, and so also load this package late
\usepackage{extpfeil}
%% Begin: Author-provided macros
%% (From  docinfo/macros  element)
%% Plus three from PTX for XML characters
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\imp}{\ \Rightarrow\ }
\newcommand{\dx}[1]{\,{\rm d}#1}


\newcommand{\dfdx}[2]{\frac{\text{d}{#1}}{\text{d}{#2}}}
\newcommand{\abs}[1]{\left|#1\right|}
\def\limit#1#2#3{{\displaystyle\lim_{#1\rightarrow #2}#3}}
\def\limitt#1#2#3{{\displaystyle\lim_{#1\rightarrow #2}\textstyle #3}}
\newcommand{\eps}{\varepsilon}
\newcommand{\unif}{\stackrel{unif}{\longrightarrow}}
\newcommand{\ptwise}{\stackrel{ptwise}{\longrightarrow}}
\newcommand{\CC}{\mathbb {C}}
\newcommand{\DD}{\mathbb {D}}
\newcommand{\RR}{\mathbb {R}}
\newcommand{\QQ}{\mathbb {Q}}
\newcommand{\NN}{\mathbb {N}}
\newcommand{\ZZ}{\mathbb {Z}}
\def\eval#1#2#3{\left.\strut{}#1\right|_{#2=#3}}
\def\geneval#1#2#3{\left.\strut{}#1\right|_{#2#3}}
\def\mineval#1#2{\left.\strut{}#1\right|_{#2}}
\newcommand{\lt}{<}
\newcommand{\gt}{>}
\newcommand{\amp}{&}
%% End: Author-provided macros
\begin{document}
%% bottom alignment is explicit, since it normally depends on oneside, twoside
\raggedbottom
\frontmatter
%% begin: half-title
\thispagestyle{empty}
{\titlepagefont\centering
\vspace*{0.28\textheight}
{\Huge A Contextual Introduction to Real Analysis: How we Got from There to Here}\\}
\clearpage
%% end:   half-title
%% begin: title page
%% Inspired by Peter Wilson's "titleDB" in "titlepages" CTAN package
\thispagestyle{empty}
{\titlepagefont\centering
\vspace*{0.14\textheight}
%% Target for xref to top-level element is ToC
\addtocontents{toc}{\protect\hypertarget{ASORA}{}}
{\Huge A Contextual Introduction to Real Analysis: How we Got from There to Here}\\[3\baselineskip]
{\Large Robert Rogers, (Emeritus)}\\[0.5\baselineskip]
{\Large SUNY, Fredonia}\\[3\baselineskip]
{\Large Eugene Boman, (Emeritus)}\\[0.5\baselineskip]
{\Large The Pennsylvania State University}\\[3\baselineskip]
{\Large June 6, 2025}\\}
\clearpage
%% end:   title page
%% begin: copyright-page
\thispagestyle{empty}
\hypertarget{front-colophon}{}\vspace*{\stretch{2}}
\noindent{\bfseries Edition}: 2.0.2, ISBN: 978-1-956862-03-4\par\medskip
\noindent{\bfseries Website}: \href{https://knightscholar.geneseo.edu/oer-ost/20/}{Milne Open Textbooks, State University of New York at Geneseo, Geneseo, NY 14454}\footnote{\nolinkurl{https://knightscholar.geneseo.edu/oer-ost/20/}\label{ASORA-2-1-5-2}}\par\medskip
\noindent\textcopyright{}2020\textendash{}2025\quad{}Eugene Boman and Robert Rogers\\[0.5\baselineskip]
This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License.  To view a copy of this license, visit \href{http://creativecommons.org/licenses/by-nc/4.0/}{CreativeCommons.org}\footnote{\nolinkurl{creativecommons.org/licenses/by-nc/4.0}\label{ASORA-2-1-6-3-2}}\par\medskip
\vspace*{\stretch{1}}
\null\clearpage
%% end:   copyright-page
%
%
\typeout{************************************************}
\typeout{Acknowledgements  Acknowledgements}
\typeout{************************************************}
%
\begin{acknowledgement}{Acknowledgements}{Acknowledgements}{}{Acknowledgements}{}{}{ASORA-2-4}
%
\begin{descriptionlist}
\begin{dlimedium}{Mactutor}{ASORA-2-4-1-1-1}%
While we have tried to tell the story of the development of Real Analysis as completely as possible, our overriding goal was always to teach mathematics, not history. Thus we have necessarily left the history incomplete.%
\par
The interested student can fill in the gaps we have left by making use of the extensive resources that can be found at the \href{https://mathshistory.st-andrews.ac.uk/}{MacTutor}\footnotemark{} history of mathematics repository.%
\par
All of the portraits of mathematicians used in this text have been taken from MacTutor.%
\par
MacTutor was created, and is maintained by Professor Edmund Robertson (Emeritus), and Professor John O'Connor (Emeritus), both of the University of St. Andrews in Scotland.%
\end{dlimedium}%
\footnotetext[3]{\nolinkurl{mathshistory.st-andrews.ac.uk}\label{ASORA-2-4-1-1-1-3-2}}%
\end{descriptionlist}
%
\end{acknowledgement}
%
%
\typeout{************************************************}
\typeout{Preface  To the Instructor}
\typeout{************************************************}
%
\begin{preface}{Preface}{To the Instructor}{}{To the Instructor}{}{}{Instructor}
The irony of this section is that it exists to tell you that this book was not written for you; it was written for your students. After all, we don't need to teach you about Real Analysis.  You already understand the subject.  The purpose of this text is to help your students make sense of the formal definitions, theorems, and proofs that they will encounter in your course.  We do this by immersing the student in the story of how what is usually called Calculus evolved into modern Real Analysis.  Our hope and intention is that this will help the student to appreciate why their intuitive understanding of topics encountered in Calculus needs to be replaced by the formalism of Real Analysis.%
\par
The traditional approach to this topic (what we might call the ``logical'' story of Real Analysis), starts with a rigorous development of the real number system and uses this to provide rigorous definitions of limits, continuity, derivatives and integrals, and convergence of series; typically in that order. This is a perfectly legitimate story of Real Analysis and, logically, it makes the most sense.  Indeed, this is a view of the subject that every mathematician-in-training should eventually attain.  However, it is our contention that your students will appreciate the subject more, and hopefully retain it better, if they see how the subject developed from the intuitive notions of Leibniz, Newton and others in the seventeenth and eighteenth centuries to the more modern approach developed in the nineteenth and twentieth centuries.  After all, they are coming at it from a viewpoint very similar to that of the mathematicians of the seventeenth and eighteenth centuries.  Our goal is to bring them into the nineteenth and early twentieth centuries, mathematically speaking.%
\par
We hasten to add that this is not a history of analysis book.  It is an introductory textbook on Real Analysis which uses the historical context of the subject to frame the concepts and to show why mathematicians felt the need to develop rigorous, non-intuitive definitions to replace their intuitive notions.%
\par
You will notice that most of the problems are embedded in the chapters, rather than lumped together at the end of each chapter. This is done to provide a context for the problems which, for the most part, are presented on an as-needed basis.%
\par
Thus the proofs of nearly all of the theorems appear as problems in the text.  Of course, it would be very unfair to ask most students at this level to prove, say, the Bolzano-Weierstrass Theorem without some sort of guidance.  So in each case we provide an outline of the proof and the subsequent problem will be to use the outline to develop a formal proof.  Proof outlines will become less detailed as the students progress.  We have found that this approach helps students develop their proof writing skills.%
\par
We state in the text, and we encourage you to emphasize to your students, that often they will use the results of problems as tools in subsequent problems.  Trained mathematicians do this naturally, but it is our experience that this is still somewhat foreign to students who are accustomed to simply ``getting the problem done and forgetting about it.''%
\par
The problems range from the fairly straightforward to the more challenging.  Some of them require the use of a computer algebra system (for example, to plot partial sums of a power series). These tend to occur earlier in the book where we encourage the students to use technology to explore the wonders of series.  A number of these problems can be done on a sufficiently advanced graphing calculator or even on Wolfram Alpha, so you should assure your students that they do not need to be super programmers to do this.  Of course, this is up to you.%
\par
A testing strategy we have used successfully is to assign more time consuming problems as collected homework and to assign other problems as possible test questions.  Students could then be given some subset of these (verbatim) as an in-class test.  Not only does this make test creation more straightforward, but it allows the opportunity to ask questions that could not reasonably be asked otherwise in a timed setting.  Our experience is that this does not make the tests ``too easy,'' and there are worse things than having students study by working together on possible test questions beforehand.  If you are shocked by the idea of giving students all of the possible test questions ahead of time, think of how much (re)learning you did studying the list of possible questions you \emph{knew} would be asked on a qualifying exam.%
\par
In the end, use this book as you see fit.  We believe your students will find it readable, as it is intended to be, and we are confident that it will help them to make sense out of the rigorous, non-intuitive definitions and theorems of Real Analysis and help them to develop their proof-writing skills.%
\par
If you have suggestions for improvement, comments or criticisms of our text please contact us at the email addresses below.  We appreciate any feedback you can give us on this.%
\par
Thank you.%
\begin{equation*}
\begin{array}{lcl} \text{ Robert R. Rogers } \amp \amp \text{
Eugene C. Boman } \\ \text{ robert.rogers@fredonia.edu } \amp
\amp \text{ ecb5@psu.edu } \end{array}
\end{equation*}
%
\end{preface}
%% begin: table of contents
%% Adjust Table of Contents
\setcounter{tocdepth}{0}
\renewcommand*\contentsname{Contents}
\tableofcontents
%% end:   table of contents
\mainmatter
%
%
\typeout{************************************************}
\typeout{Part I In Which We Raise A Number Of Questions}
\typeout{************************************************}
%
\begin{partptx}{Part}{In Which We Raise A Number Of Questions}{}{In Which We Raise A Number Of Questions}{}{}{AskingQuestions}
\renewcommand*{\partname}{Part}
%
%
\typeout{************************************************}
\typeout{Chapter 1 Prologue: Three Lessons Before We Begin}
\typeout{************************************************}
%
\begin{chapterptx}{Chapter}{Prologue: Three Lessons Before We Begin}{}{Prologue: Three Lessons Before We Begin}{}{}{Threelessons}
\renewcommand*{\chaptername}{Chapter}
%
%
\typeout{************************************************}
\typeout{Section 1.1 Lesson One}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Lesson One}{}{Lesson One}{}{}{ThreeLessons-lesson-one}
Get a pad of paper and write down the answer to this question: What is .~.~.~No, really.  We're serious. \emph{Get a writing pad.} We'll wait.%
\begin{aside}{Aside}{Comment.}{ThreeLessons-lesson-one-3}%
We \emph{really} are serious about this.  Get a pad of paper!%
\end{aside}
Got it?  Good. Now write down your answer to this question: What is a number?  Don't think about it.  Don't analyze it. Don't consider it.  Just write down the best answer you can without thinking.  You are the only person who ever needs to see what you've written.%
\par
Done?  Good.%
\par
Now consider this: All of the objects listed below are ``numbers'' in a sense we will not make explicit here.  How many of them does your definition include?%
\par
%
\begin{itemize}[label=\textbullet]
\item{}\(\displaystyle 1\)%
\item{}\(\displaystyle -1\)%
\item{}\(\displaystyle 0\)%
\item{}\(\displaystyle 3/5\)%
\item{}\(\displaystyle \sqrt{2}\)%
\item{}\(\displaystyle \sqrt{-1}\)%
\item{}\(\displaystyle i^i\)%
\item{}\(\displaystyle e^{5i}\)%
\item{}\(4+3i-2j+6k\) (this is called a quaternion)%
\item{}\(\dx{x}\) (this is the differential you learned all about in Calculus)%
\item{}\(\begin{pmatrix}
1\amp 2\\
-2\amp 1
\end{pmatrix}\) (yes, matrices can be considered numbers).%
\end{itemize}
%
\par
Surely you included \(1\).  Almost surely you included \(3/5\).  But what about \(0?\) \(-1?\) Does your definition include \(\sqrt{2}?\) Do you consider \(\dx{x}\) a number?  Leibniz\index{Leibniz, Gottfried Wilhelm} did.  Any of the others? (And, yes, they really are all ``numbers.'')%
\par
The lesson in this little demonstration is this: You don't really have a clear notion of what we mean when we use the word ``number.'' And this is fine.  Not knowing is acceptable.%
\begin{aside}{Aside}{Comment.}{ThreeLessons-lesson-one-10}%
Sometimes it is even encouraged.%
\end{aside}
A principal goal of this course of study is to rectify this, at least a little bit.  When the course is over you may or may not be able to give a better definition of the word ``number'' but you will have a deeper understanding of the real numbers at least.  That is enough for now.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 1.2 Lesson Two}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Lesson Two}{}{Lesson Two}{}{}{ThreeLessons_lesson-two}
Read and understand the following development of the Quadratic Formula.%
\par
Suppose \(a\neq0\).  If%
\begin{align}
ax^2+bx+c =0\label{eq_QForm1}\\
\intertext{then}
x^2+\frac{b}{a}x\amp =-\frac{c}{a}\label{eq_QForm2}\\
\intertext{Now let \(x=y-\frac{b}{2a}\) giving}
y^2\amp = -\frac{c}{a} +\frac{b^2}{4a^2}\label{eq_QForm3}\\
y\amp = \frac{\pm \sqrt{b^2-4ac}}{2a}\label{ThreeLessons_lesson-two-3-2-6}\\
x\amp = \frac{-b\pm\sqrt{b^2-4ac}}{2a}\label{eq_QForm4}
\end{align}
%
\par
Were you able to follow the argument?  Probably the step from \hyperref[eq_QForm1]{equation~({\xreffont\ref{eq_QForm1}})} to \hyperref[eq_QForm2]{equation~({\xreffont\ref{eq_QForm2}})} presented no difficulties.  But what about the next step?  Do you see where \hyperref[eq_QForm3]{equation~({\xreffont\ref{eq_QForm3}})} came from?  If so, good for you. Most students, in fact most mathematicians, cannot make that step in their heads.  But are you sure?  Is there, perhaps, some small detail you've overlooked?%
\par
Check to see.%
\par
That is, let \(x=y-\frac{b}{2a}\) in \hyperref[eq_QForm2]{equation~({\xreffont\ref{eq_QForm2}})} and see if you can get \hyperref[eq_QForm3]{equation~({\xreffont\ref{eq_QForm3}})}.  Do it on that handy pad of paper we told you to get out earlier.  Do it now.  We'll wait.%
\begin{aside}{Aside}{Comment.}{ThreeLessons_lesson-two-7}%
If you \emph{still} haven't gotten out a pad of paper, give up now.  You're going to fail this course.  Seriously. Do you think we would spend so much time on this, that we would repeat it so many times, if it weren't important. \emph{\emph{GET OUT A PAD OF PAPER NOW!}} Last chance. You've been warned.%
\end{aside}
Done?  Good.%
\par
Perhaps you haven't been able to fill in the details on your own.  That's ok.  Many people can't.  If not then get help: from a classmate, a friend, your instructor, whomever. Unfortunately most people won't get help in this situation. Instead they will perceive this as ``failure,'' hide it and berate themselves or the problem as ``stupid.'' In short they will let their personal insecurities and demons overwhelm them. \emph{Don't do this.  Get help.} You are neither dumb nor incapable.  There are a thousand reasons that on any given day you might not be able to solve this problem.  But don't let a bad day interfere with the education you are here for.  Get someone to help you over this hump.  Later you will be able to help your helper in the same way.  Really.%
\par
At this point we assume that you've successfully negotiated the transition from \hyperref[eq_QForm2]{equation~({\xreffont\ref{eq_QForm2}})} to \hyperref[eq_QForm4]{equation~({\xreffont\ref{eq_QForm4}})}.%
\par
See?  It really wasn't that bad after all.  Just a lot of elementary algebra.  Now that you've done it (or seen it done), it is easy to see that there really wasn't much there.%
\par
But this is the point!  We left those computations out precisely because we knew that they were routine and that you could fill in the details.  Moreover, filling in the details yourself gives you a little better insight into the computations.  If we'd filled them in for you we would have robbed you of that insight. And we would have made this book longer than it needs to be.  We don't want to do either of those things.  If we fill in all of the details of every computation for you, you won't learn to have confidence in your ability to do them yourself. And this book will easily double in length.%
\par
So the lesson here is this: Keep that pad of paper handy whenever you are reading this (or any other) mathematics text. You will need it.  Routine computations will often be skipped. But calling them ``routine'' and skipping them does not mean that they are unimportant.  If they were truly unimportant we would leave them out entirely.%
\par
Moreover ``routine'' does not mean ``obvious.'' Every step we took in the development of the Quadratic Formula was ``routine.'' But even routine computations need to be understood and the best way to understand them is to do them. This is the way to learn mathematics; it is the \emph{only} way that really works.  Don't deprive yourself of your mathematics education by skipping the most important parts.%
\begin{aside}{Aside}{Comment.}{ThreeLessons_lesson-two-15}%
If you didn't fill in those details then you are depriving yourself of the education you are here to obtain. This is sad.  There is a good reason for putting these three lessons first.  Stop wasting your time and intellect! Go do it now.%
\end{aside}
As you saw when you filled in the details of our development of the Quadratic Formula the substitution \(x=y-\frac{b}{2a}\) was crucial because it turned%
\begin{equation*}
x^2+\frac{b}{a}x +\frac{c}{a}=0
\end{equation*}
into%
\begin{equation*}
y^2=k
\end{equation*}
where \(k\) depends only on \(a\), \(b\), and \(c\). In the sixteenth century a similar technique was used by \href{https://mathshistory.st-andrews.ac.uk/Biographies/Ferrari/}{Ludovico Ferrari}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Ferrari/}\label{ThreeLessons_lesson-two-16-9}} (1522-1565) to reduce the general cubic equation%
\begin{equation}
ax^3+bx^2+cx+d=0\label{eq_GenCubic}
\end{equation}
into the so-called ``depressed cubic''%
\begin{equation*}
y^3 +py+q=0
\end{equation*}
where \(p\), and \(q\) depend only on \(a\), \(b\), \(c\), and \(d\).%
\par
The general depressed cubic had previously been solved by Tartaglia (the Stutterer, 1500-1557) so converting the general cubic into a depressed cubic provided a path for Ferrari to compute the ``Cubic Formula'' \textemdash{} like the Quadratic Formula but better.%
\begin{aside}{Aside}{Comment.}{ThreeLessons_lesson-two-18}%
It is not entirely clear why eliminating the quadratic term should be depressing, but there it is.%
\end{aside}
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Tartaglia/}{Tartaglia}\protect\footnotemark{}, ``The Stutterer''}{ThreeLessons_lesson-two-19}{}%
\begin{image}{0.36}{0.28}{0.36}{}%
\includegraphics[width=\linewidth]{external/images/Tartaglia.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[5]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Tartaglia/}\label{ThreeLessons_lesson-two-19-1-2}}%
Ferrari also knew how to compute the general solution of the ``depressed quartic'' so when he and his teacher Girolomo Cardano (1501-1576) figured out how to depress a general quartic they had a complete solution of the general quartic as well.%
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Cardan}{Girolomo Cardano}\protect\footnotemark{}}{ThreeLessons_lesson-two-21}{}%
\index{Cardano, Girolomo}%
\begin{image}{0.36}{0.28}{0.36}{}%
\includegraphics[width=\linewidth]{external/images/Cardan.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[6]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Cardan}\label{ThreeLessons_lesson-two-21-1-2}}%
Alas, their methods broke down entirely when they tried to solve the general quintic equation.  Unfortunately the rest of this story belongs in a course on Abstract Algebra, not Real Analysis.  But the lesson in this story applies to all of mathematics: Every problem solved is a new theorem which then becomes a tool for later use.  Depressing a cubic would have been utterly useless had not Tartaglia had a solution of the depressed cubic in hand.  The technique they used, with slight modifications, then allowed for a solution of the general quartic as well.%
\par
Keep this in mind as you proceed through this course and your mathematical education.  Every problem you solve is really a theorem, a potential tool that you can use later.  We have chosen the problems in this text deliberately with this in mind. Don't just solve the problems and move on.  Just because you have solved a problem does not mean you should stop thinking about it.  Keep thinking about the problems you've solved. Internalize them.  Make the ideas your own so that when you need them later you will have them at hand to use.%
\begin{problem}{Problem}{The Quadratic Formula.}{ThreeLessons_lesson-two-24}%
%
\begin{enumerate}[label={(\alph*)}]
\item{}Find \(M\) so that the substitution \(x=y-M\) depresses \hyperref[eq_GenCubic]{equation~({\xreffont\ref{eq_GenCubic}})}, the general cubic equation.  Then find \(p\) and \(q\) in terms of \(a\), \(b\), \(c\), and \(d\).%
\item{}Find \(K\) so that the substitution \(x=y-K\) depresses the general quartic equation.  Make sure you demonstrate how you obtained that value or why it works (if you guessed it).%
\item{}Find \(N\) so that the substitution \(x=y-N\) depresses a polynomial of degree \(n\).  Ditto on showing that this value works or showing how you obtained it.%
\end{enumerate}
%
\end{problem}
\begin{problem}{Problem}{Another Derivation of the Quadratic Formula.}{ThreeLessons_lesson-two-25}%
\index{Quadratic Formula!second proof} Here is yet another way to solve a quadratic equation.  Read the development below with pencil and paper handy.  Confirm all of the computations that are not completely transparent to you.  Then use your notes to present the solution with \emph{all} steps filled in.%
\begin{aside}{Aside}{Comment.}{ThreeLessons_lesson-two-25-2-2}%
Be sure you are clear on the purpose of this problem before you begin.  This is not about solving the Quadratic Equation.  You already know how to do that.  Our purpose here is to give you practice filling in the skipped details of mathematical exposition.  We've chosen this particular problem because it should be a comfortable setting for you, but this particular solution is probably outside of your previous experience.%
\end{aside}
Suppose that \(r_1\) and \(r_2\) are solutions of \(ax^2+bx+c=0\). Without loss of generality suppose that \(a>0\).  Suppose further that \(r_1\ge r_2\).  Then%
\begin{align*}
ax^2+bx+c \amp = a(x-r_1)(x-r_2)\\
\amp =
a\left[x^2-(r_1+r_2)x+(r_1+r_2)^2-(r_1-r_2)^2-3r_1r_2\right].
\end{align*}
%
\par
Therefore%
\begin{align}
r_1+r_2\amp = -\frac{b}{a}\label{eq_LagrangeQuadratic1}\\
\intertext{and}
r_1-r_2 \amp = \sqrt{\left(\frac{b}{a}\right)^2-\frac{4c}{a}}\text{.}\label{eq_LagrangeQuadratic2}
\end{align}
%
\par
Equations \hyperref[eq_LagrangeQuadratic1]{({\xreffont\ref{eq_LagrangeQuadratic1}})} and \hyperref[eq_LagrangeQuadratic2]{({\xreffont\ref{eq_LagrangeQuadratic2}})} can be solved simultaneously to yield%
\begin{align*}
r_1\amp =\frac{-b+\sqrt{b^2-4ac}}{2a}\\
r_2\amp =\frac{-b-\sqrt{b^2-4ac}}{2a}\text{.}
\end{align*}
%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 1.3 Lesson Three}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Lesson Three}{}{Lesson Three}{}{}{ThreeLessons_lesson-three}
In the hustle and bustle of a typical college semester, with a lot of demands on your time and very little time to think, it becomes very easy to see each problem you solve as a small, isolated victory and then move on to the next challenge.  This is understandable.  Each problem you solve \emph{is} a small victory and you've every right to be proud of it.  But it is not isolated and it is a mistake to think that it is.%
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Polya/}{George Polya}\protect\footnotemark{}}{ThreeLessons_lesson-three-3}{}%
\begin{image}{0.36}{0.28}{0.36}{}%
\includegraphics[width=\linewidth]{external/images/Polya.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[7]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Polya/}\label{ThreeLessons_lesson-three-3-1-2}}%
In his book \emph{How to Solve It} the mathematician and teacher George Polya gave four steps for problem solving.  The steps may be paraphrased as%
\begin{enumerate}
\item{}Understand the problem.%
\item{}Formulate a plan.%
\item{}Execute the plan.%
\item{}Reflect on what you've done.%
\end{enumerate}
%
\par
This process is iterative.  That is, once a plan is formulated and executed we often find that our plan was not up to the task. So we have to ask what went wrong, form a new plan and try again.  This is the fourth step: Reflect on what you've done.%
\par
Almost everyone remembers this fourth step when their plan \emph{doesn't} work.  After all, you've got to try again so you have to ask what went wrong.  But it is all too easy to neglect that crucial fourth step when the plan succeeds.  In that case, flush with success we usually move on to the next problem and start over from scratch.%
\par
This is a mistake.  Having solved a problem is no reason to stop thinking about it.%
\par
That fourth step is at least as important when we have succeeded as when we have failed.  Each time you solve a problem stop and ask yourself a few questions:%
\begin{itemize}[label=\textbullet]
\item{}Are there any easy consequences that follow from the result?%
\item{}How does it fit into the broader scheme of other problems you have solved?%
\item{}How might it be used in the future?%
\end{itemize}
%
\par
Also, notice the structure of the problem.  Some assumptions had to be made.  What were they?  Were they all necessary?  That is, did your solution use everything that was assumed?  If not, you may have something considerably more general than it at first appears.  What is that more general statement?  Even if you used all of the assumptions, was that really necessary?  Can you solve a similar problem with weaker assumptions?%
\par
Take a moment to pack all of these questions (and their answers) away in your mind so that when you see something similar in the future you will be reminded of it.  \emph{Don't} solve any problem and then forget it and move on.  The nature of mathematics is cumulative.  Remember, you are not here to accumulate grade points.  You are here to learn and understand the concepts and methods of mathematics, to gain ``mathematical maturity.'' Part of that maturation process is the accumulation of a body of facts (theorems), and techniques that can be used to prove new theorems (solve new problems).%
\par
This text has been written with the maturation process in mind. You will frequently find that the problems you solve today can be used to good effect in the ones you attempt tomorrow, but only if you remember them.  So take a moment after you've solved each problem to think about how it fits into the patterns you already know.  This is important enough to bear repeating: \emph{A problem, once solved, becomes a tool for solving subsequent problems!}%
\par
The purpose of the following sequence of problems is to help you become accustomed to this notion (if you aren't already).  It is a progression of results about prime numbers.  As you probably recall, a prime number is any integer greater than \(1\) whose only factors are itself and \(1\).  For example, \(2\), \(3\), \(5\), \(7\), \(11\) are prime, while \(4\), \(6\), \(9\) are not.  A major result about prime numbers is the following:%
\begin{theorem}{Theorem}{The Fundamental Theorem of Arithmetic.}{}{thm_FTA}%
Any integer greater than \(1\) is either prime or it is a product of prime numbers.  Furthermore, this prime decomposition is unique up to the order of the factors.%
\end{theorem}
We will not prove this, but we will use it as a starting point to examine the following problems.  As you do these problems, notice how subsequent problems make use of the previous results.%
\par
Notice that the notation \(p\,|a\) simply means that the integer \(p\) divides the integer \(a\) with no remainder.%
\begin{problem}{Problem}{Fermat's Little Theorem, step 1.}{ThreeLessons_lesson-three-16}%
\index{Fermat's Little Theorem!problems leading to!if a prime divides a product of two numbers then it divides one of the factors} Let \(p\) be a prime number and \(a, b\) positive integers such that \(p\, | (a\cdot b)\).  Show that \(p\,|a\) or \(p\,|b\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{ThreeLessons_lesson-three-16-3}{}\quad{}If \(p\,|a\) then we are done.  If not then notice that \(p\) is a prime factor of \(a\cdot b\).  What does the Fundamental Theorem of Arithmetic say about the prime factors of \(a\cdot b\) compared to the prime factors of \(a\) and \(b?\)%
\end{problem}
\begin{problem}{Problem}{Fermat's Little Theorem, step 2.}{ThreeLessons_lesson-three-17}%
\index{Fermat's Little Theorem!problems leading to!if a prime divides an arbitrary product then it divides one of the factors} Let \(p\) be a prime number and let \(a_1, a_2, \ldots,
a_n\) be positive integers such that \(p\,|\left(a_1\cdot
a_2\cdot a_3\cdot\ldots\cdot a_n\right)\).  Show that \(p\,|a_k\) for some \(k\in\{1, 2, 3, \ldots, n\}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{ThreeLessons_lesson-three-17-3}{}\quad{}Use induction on \(n\) and the result of the previous problem.%
\end{problem}
\begin{problem}{Problem}{Fermat's Little Theorem, step 3.}{ThreeLessons_lesson-three-18}%
\index{Fermat's Little Theorem!problems leading to!if \(p\) is prime then \(p\) divides \(p
\choose{}k\)} Let \(p\) be a prime number and let \(k\) be an integer with \(1\le k\le p-1\).  Prove that \(p\left|{p
\choose{}k}\right.\), where \({p \choose{}k}\) is the binomial coefficient \(\frac{p!}{k!(p-k)!}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{ThreeLessons_lesson-three-18-3}{}\quad{}We know \(p\,|p\,!\), so \(p\,|{p \choose{}k}k!(p-k)!\). How does the previous result apply?%
\end{problem}
We now have all the machinery in place to prove one of the really cool theorems from number theory.%
\begin{theorem}{Theorem}{Fermat's Little Theorem.}{}{thm_FermatsLittleTheorem}%
\index{Fermat's Little Theorem}%
Let \(p\) be any prime number.  Then \(p\,|(n^p-n)\) for all positive integers \(n\).%
\end{theorem}
\begin{problem}{Problem}{Fermat's Little Theorem, step 4.}{ThreeLessons_lesson-three-21}%
\index{Fermat's Little Theorem} Prove Fermat's Little Theorem.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{ThreeLessons_lesson-three-21-3}{}\quad{}Use induction on \(n\).  To get from \(n\) to \(n+1\), use the binomial theorem on \((n+1)^p\).%
\end{problem}
Fermat's Little Theorem is the foundational basis for a number of results in number theory and encryption.%
\begin{problem}{Problem}{}{ThreeLessons_lesson-three-23}%
Recall what we said above: Having solved a problem is no reason to stop thinking about it. Reflect upon your proof of \hyperref[thm_FermatsLittleTheorem]{Fermat's Little Theorem} by answering these questions.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Will the result of Fermat's Little Theorem hold if \(p\) is not a prime number?%
\item{}If not, then which of the steps in the proof breaks down?%
\end{enumerate}%
\end{problem}
\end{sectionptx}
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 2 Numbers, Real (\(\RR\)) and Rational (\(\QQ\))}
\typeout{************************************************}
%
\begin{chapterptx}{Chapter}{Numbers, Real (\(\RR\)) and Rational (\(\QQ\))}{}{Numbers, Real (\(\RR\)) and Rational (\(\QQ\))}{}{}{NumbersRealRational}
\renewcommand*{\chaptername}{Chapter}
\begin{introduction}{}%
The set of real numbers (denoted, \(\RR\)) is badly named. The real numbers are no more or less real \textemdash{} in the non-mathematical sense that they exist \textemdash{} than any other set of numbers, just like the set of rational numbers (\(\QQ\)), the set of integers (\(\ZZ\)), or the set of natural numbers (\(\NN\)). The name ``real numbers'' is (almost) an historical anomaly not unlike the name ``Pythagorean Theorem'' which was actually known and understood long before Pythagoras lived.%
\par
When Calculus was being invented in the \(17\)th century, numbers were thoroughly understood, or so it was believed.%
\begin{aside}{Aside}{Comment.}{NumbersRealRational-2-3}%
Some would say ``re-invented.'' See \hyperlink{russo96__forgot_revol}{[{\xreffont 13}]}, or \hyperlink{netz07__archim_codex}{[{\xreffont 9}]}.  %
\end{aside}
They were, after all, just numbers. Combine them. We call that addition. If you add them repeatedly we call it multiplication. Subtraction and division were similarly understood.%
\par
It was (and still is) useful to visualize these things in a more concrete way. If we take a stick of length 2 and another of length 3 and lay them end-to-end we get a length of 5. This is addition. If we lay them end-to-end but at right angles then our two sticks are the length and width of a rectangle whose area is 6. This is multiplication.%
\par
Of course measuring lengths with whole numbers has limitations, but these are not hard to fix. If we have a length (stick) of length 1 and another of length 2, then we can find another whose length when compared to 1 is the same (has the same proportion) as \(1\) is to \(2\). That number of course, is \(1/2\).%
\begin{figureptx}{Figure}{}{Fractions}{}%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/Fractions.png}
\end{image}%
\tcblower
\end{figureptx}%
Notice how fraction notation reflects the operation of comparing 1 to 2.  This comparison is usually referred to as the \emph{ratio} of \(1\) to \(2\) so numbers of this sort are called \emph{rational numbers.} The set of rational numbers is denoted \(\QQ\) for quotients.  In grade school they were introduced to you as fractions.  Once fractions are understood, this visualization using line segments (sticks) leads quite naturally to their representation with the rational number line.%
\begin{figureptx}{Figure}{The Rational Number Line}{RationalNumberLine}{}%
\begin{image}{0}{1}{0}{}%
\includegraphics[width=\linewidth]{external/images/RationalNumberLine.png}
\end{image}%
\tcblower
\end{figureptx}%
This seems to work as a visualization because the rational numbers and the points on a line seem to share certain properties.  Chief among these is that between any two points on the rational line there is another point, just as between any two rational numbers there is another rational number.%
\begin{problem}{Problem}{}{NumbersRealRational-2-11}%
\index{\(\QQ\)!rational numbers exist between rational numbers} Let \(a, b, c, d\in\NN\) and find a rational number between \(a/b\) and \(c/d\).%
\end{problem}
This is all very clean and satisfying until we examine it just a bit closer. Then it becomes quite mysterious. Consider again the rational numbers \(a/b\) and \(c/d\). If we think of these as lengths we can ask, "Is there a third length, say \(\alpha\), such that we can divide \(a/b\) into \(M\) pieces, each of length \(\alpha\) \emph{and also} divide \(c/d\) into \(N\) pieces each of length \(\alpha?\)" A few minutes thought should convince you that this is the same as the problem of finding a common denominator so \(\alpha=\frac{1}{bd}\) will work nicely. (Confirm this yourself.)%
\par
You may be wondering what we're making all of this fuss about. \emph{Obviously} this is \emph{always} true. In fact the previous paragraph gives an outline of a very nice little proof of this. Here are the theorem and its proof presented formally.%
\begin{theorem}{Theorem}{}{}{thm_CommonDenominatorsExist}%
\index{common denominators} Suppose \(a\), \(b\), \(c\), and \(d\) are integers, with \(b, d \neq 0\). There is a number \(\alpha\in\QQ\) such that \(M\alpha=a/b\) and \(N\alpha=c/d\) where \(M\) and \(N\) are also integers.%
\end{theorem}
\begin{proof}{Proof}{}{NumbersRealRational-2-15}
To prove this theorem we will display \(\alpha\), \(M\) and \(N\). It is your responsibility to confirm that these actually work. Here they are: \(\alpha=1/bd\), \(M=ad\), and \(N=cb\).%
\end{proof}
\begin{problem}{Problem}{}{NumbersRealRational-2-16}%
\index{common denominators} Confirm that \(\alpha, M, \text{ and } N\) as given in the proof of \hyperref[thm_CommonDenominatorsExist]{Theorem~{\xreffont\ref{thm_CommonDenominatorsExist}}} satisfy the requirements of the theorem.%
\end{problem}
\hyperref[thm_CommonDenominatorsExist]{Theorem~{\xreffont\ref{thm_CommonDenominatorsExist}}} suggests the following very deep and important question: Are there lengths which can \emph{not} be expressed as the ratio of two integer lengths? The answer, of course, is yes. Otherwise we wouldn't have asked the question.  %
\par
One of the best known examples of such a number is the circumference of a circle with diameter 1. This is the number usually denoted by \(\pi\). But circles are extremely complex objects \textemdash{} they only seem simple because they are so familiar. Arising as it does from a circle, you would expect the number \(\pi\) to be very complex as well and this is true. In fact \(\pi\) is an exceptionally weird number for a variety of reasons. Let's start with something a little easier to think about.%
\par
\index{\(\sqrt{2}\)!is irrational} Squares are simple. Two sets of parallel lines at right angles, all of the same length. What could be simpler? If we construct a square with sides having length 1 then its diagonal has length \(\sqrt{2}\).%
\begin{figureptx}{Figure}{A construction of \(\sqrt{2}\)}{Sqrt2}{}%
\begin{image}{0.25}{0.5}{0.25}{}%
\includegraphics[width=\linewidth]{external/images/Sqrt2.png}
\end{image}%
\tcblower
\end{figureptx}%
This is a number which cannot be expressed as the \emph{ratio} of two integers.  That is, it is \emph{irrational.} This has been known since ancient times, but it is still quite disconcerting when first encountered.  It seems so counter-intuitive that the intellect rebels.  ``This can't be right,'' it says. ``That's just crazy!''%
\par
Nevertheless it is true and we can prove it is true as follows.%
\par
What happens if we suppose that the square root of two \emph{can} be expressed as a ratio of integers? We will show that this leads irrevocably to a conclusion that is manifestly not true.%
\par
Suppose \(\sqrt{2}=a/b\) where \(a\) and \(b\) are integers. Suppose further that the fraction \(a/b\) is in lowest terms. \emph{This assumption is crucial because if \(a/b\) is in lowest terms we know that at most only one of them is even.}%
\par
So%
\begin{align*}
\frac{a}{b} \amp = \sqrt{2}.\\
\intertext{Squaring both sides gives:}
a^2 \amp = 2b^2.\\
\intertext{Therefore \(a^2\) is even. But if \(a^2\) is even then \(a\) must be even also (why?). If \(a\) is even then \(a=2k\) for some integer \(k\). Therefore}
4k^2\amp =2b^2\textit{ or}\\
2k^2\amp = b^2\text{.}
\end{align*}
%
\par
Therefore \(b^2\) is also even and so \(b\) must be even too. But this is impossible. We've just concluded that \(a\) and \(b\) are both even and this conclusion follows directly from our initial assumption that at most one of them could be even.%
\par
This is nonsense. Where is our error? It is not in any single step of our reasoning. That was all solid. Check it again to be sure.%
\par
Therefore our error must be in the initial assumption that \(\sqrt{2}\) could be expressed as a fraction. That assumption must therefore be false. In other words, \(\sqrt{2}\) cannot be so expressed.%
\begin{problem}{Problem}{Irrational Numbers.}{NumbersRealRational-2-29}%
Show that each of the following numbers is irrational:%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\sqrt{3}\)%
\item{}\(\sqrt{5}\)%
\item{}\(\sqrt[3]{2}\)%
\item{}\(i\) \((=\sqrt{-1})\)%
\item{}The square root of every positive integer which is not the square of an integer.%
\end{enumerate}%
\end{problem}
\index{\(\sqrt{2}\)!meaning of} The fact that \(\sqrt{2}\) is not rational is cute and interesting, but unless, like the Pythagoreans of ancient Greece, you have a strongly held religious conviction that all numbers are rational, it does not seem terribly important. On the other hand, the very existence of \(\sqrt{2}\) raises some interesting questions. For example what can the symbol \(4^{\sqrt{2}}\) possibly mean? If the exponent were a rational number, say \(m/n\), then clearly \(4^{m/n}=\sqrt[n]{4^m}\). But since \(\sqrt{2}\neq m/n\) for \emph{any} integers \(m\) and \(n\) how do we interpret \(4^{\sqrt{2}}?\) Does it have any meaning at all? The more you think about this, the more puzzling the existence of irrational numbers becomes. Suppose for example we reconsider the construction of a line segment of length \(\sqrt{2}\). It is clear that the construction works and that we really can build such a line segment. It exists.%
\par
Repeat the construction but this time let's put the base side on the rational line.%
\begin{figureptx}{Figure}{}{Sqrt2OnRatLine}{}%
\begin{image}{0.25}{0.5}{0.25}{}%
\includegraphics[width=\linewidth]{external/images/Sqrt2OnRatLine.png}
\end{image}%
\tcblower
\end{figureptx}%
We know that the diagonal of this square is \(\sqrt{2}\) as indicated. And we know that \(\sqrt{2}\) is not a rational number.%
\par
Now leave the diagonal pinned at \((0,0)\) but allow it to rotate down so that it coincides with the \(x-\)axis.%
\begin{figureptx}{Figure}{}{Sqrt2Irrational}{}%
\begin{image}{0.25}{0.5}{0.25}{}%
\includegraphics[width=\linewidth]{external/images/Sqrt2Irrational.png}
\end{image}%
\tcblower
\end{figureptx}%
The end of our diagonal will trace out an arc of the circle with radius \(\sqrt{2}\). When the diagonal coincides with the \(x-\)axis, its endpoint will obviously be the point \((\sqrt{2}, 0)\) as shown.%
\par
But wait! We're using the \emph{rational} number line for our \(x-\)axis. That means the only points on the \(x-\)axis are those that correspond to rational numbers (fractions). But we know that \(\sqrt{2}\) is not rational! Conclusion: There is no point \((\sqrt{2},0)\). It simply doesn't exist.%
\par
Put differently, there is a hole in the rational number line right where \(\sqrt{2}\) should be.%
\begin{figureptx}{Figure}{}{RationalLineWithHoles}{}%
\begin{image}{0}{1}{0}{}%
\includegraphics[width=\linewidth]{external/images/RationalLineWithHoles.png}
\end{image}%
\tcblower
\end{figureptx}%
This is weird!%
\par
Recall that between any two rational numbers there is always another. This fact is what led us to represent the rational numbers with a line in the first place.%
\par
But it's even worse than that. It's straightforward to show that \(\sqrt{3}\), \(\sqrt{5}\), etc. are all irrational too. So are \(\pi\) and \(e\), though they aren't as easy to show. It seems that the rational line has a bunch of holes in it. Infinitely many.%
\par
And yet, the following theorem is true%
\begin{theorem}{Theorem}{}{}{thm_IrrationalBetweenIrrationals}%
\index{\(\RR\)!real numbers exist between real numbers}%
\begin{enumerate}[label={(\alph*)}]
\item{}Between any two distinct real numbers there is a rational number.%
\item{}Between any two distinct real numbers there is an irrational number.%
\end{enumerate}
%
\end{theorem}
Both parts of this theorem rely on a judicious use of what is now called the Archimedean Property of the Real Number System, which can be formally stated as follows.%
\begin{principle}{Principle}{The Archimedean Property.}{}{NumbersRealRational-2-46}%
\index{Archimedean Property} Given any two positive real numbers, \(a\) and \(b\), there is a positive integer, \(n\) such that \(na>b\).%
\end{principle}
Physically this says that we can empty an ocean \(b\) with a teaspoon \(a\), provided we are willing to use the teaspoon a large number of times \(n\).%
\par
This is such an intuitively straightforward concept that it is easy to accept it without proof. Until the invention of Calculus, and even for some time after that, it was simply assumed. However as the foundational problems posed by the concepts of Calculus were understood and solved we were eventually led to a deeper understanding of the complexities of the real number system. The Archimedean Property is no longer taken as an unproved axiom, but rather it is now understood to be a consequence of other axioms. We will show this later, but for now we will accept it as obviously true just as Archimedes did.%
\par
With the invention of Calculus, mathematicians of the seventeenth century began to use objects which didn't satisfy the Archimedean Property (in fact, so did Archimedes). As we shall see in the next chapter, when Leibniz wrote the first paper on his version of the Calculus, he\index{Leibniz, Gottfried Wilhelm!and infinitesimals} followed this practice by explicitly laying out rules for manipulating infinitely small quantities (infinitesimals). These were taken to be actual numbers which are not zero and yet smaller than any real number. The notation he used was \(\dx{ x}\) (an infinitely small displacement in the \(x\) direction), and \(\dx{ y}\) (an infinitely small displacement in the \(y\) direction). These symbols should look familiar to you. They are the same \(\dx{ y}\) and \(\dx{ x}\) used to form the derivative symbol \(\dfdx{y}{x}\) that you learned about in Calculus.%
\par
Mathematicians of the seventeenth and eighteenth centuries made amazing scientific and mathematical progress exploiting these infinitesimals, even though they were foundationally suspect. No matter how many times you add the infinitesimal \(\dx{ x}\) to itself the result will not be bigger than, say \(10^{-1000}\), which is very bizarre.%
\par
When foundational issues came to the forefront, infinitesimals fell somewhat out of favor. You probably didn't use them very much in Calculus. Most of the time you probably used the prime notation, \(f^\prime(x)\) introduced by Lagrange \index{Lagrange, Joseph-Louis} in the eighteenth century. Some of the themes in this book are: Why differentials fell out of favor, what were they replaced with and how the modern notations you learned in Calculus evolved over time.%
\par
To conclude this aside on the Archimedean Property, the idea of infinitesimals was revisited in the twentieth century by the logician Abraham Robinson in~\hyperlink{robinson74__non_stand_analy}{[{\xreffont 12}]}. Robinson was able to put the idea of infinitesimals on a solid logical foundation. But in the 18th century, the existence of infinitesimal numbers was shaky to say the very least. However this did not prevent mathematicians from successfully exploiting these infinitely small quantities.%
\par
We will come back to this saga in later chapters, but for now we return to \hyperref[thm_IrrationalBetweenIrrationals]{Theorem~{\xreffont\ref{thm_IrrationalBetweenIrrationals}}}.%
\begin{proof}{Proof}{Sketch of Proof.}{NumbersRealRational-2-54}
We will outline the proof of part (a) of \hyperref[thm_IrrationalBetweenIrrationals]{Theorem~{\xreffont\ref{thm_IrrationalBetweenIrrationals}}} and indicate how it can be used to prove part b.%
\par
Let \(\alpha\) and \(\beta\) be real numbers with \(\alpha > \beta\). There are two cases.%
\begin{itemize}[label=\textbullet]
\item{}\lititle{Case 1:.}\par%
\(\alpha-\beta > 1\). In this case there is at least one integer between \(\alpha\) and \(\beta\). Since integers are rational we are done.%
\item{}\lititle{Case 2:.}\par%
\(\alpha-\beta \le 1\).  In this case, by the Archimedean Property there is a positive integer, say \(n\), such that \(n(\alpha-\beta) = n\alpha-n\beta
> 1\).  Now there will be an integer between \(n\alpha\) and \(n\beta\).  You should now be able to find a rational number between \(\alpha\) and \(\beta\).%
\end{itemize}
%
\end{proof}
For part (b), divide \(\alpha\) and \(\beta\) by any positive irrational number and apply part a.  There are a couple of details to keep in mind.  These are considered in the following problem.%
\begin{problem}{Problem}{}{NumbersRealRational-2-56}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Prove that the product of a nonzero rational number and an irrational number is irrational.%
\item{}Use the result of part (a) to prove \hyperref[thm_IrrationalBetweenIrrationals]{Theorem~{\xreffont\ref{thm_IrrationalBetweenIrrationals}}}.%
\end{enumerate}%
\end{problem}
As a practical matter, the existence of irrational numbers isn't really very important.  In light of \hyperref[thm_IrrationalBetweenIrrationals]{Theorem~{\xreffont\ref{thm_IrrationalBetweenIrrationals}}}, any irrational number can be approximated arbitrarily closely by a rational number.  So if we're designing a bridge and \(\sqrt{2}\) is needed we just use \(1.414\) instead.  The error introduced is less than \(0.001 =1/1000\) so it probably doesn't matter.%
\par
But from a theoretical point of view this is devastating.  When Calculus was invented, the rational numbers were suddenly not up to the task of justifying the concepts and operations we needed to work with.%
\par
Newton\index{Newton, Isaac!foundation of Calculus} explicitly founded his version of calculus on the assumption that we can think of variable quantities as being generated by a continuous motion.  If our number system has holes in it such continuous motion is impossible because we have no way to jump over the gaps.  So Newton simply postulated that there were no holes.  He filled in the hole where \(\sqrt{2}\) should be.  He simply said, yes there is a number there called \(\sqrt{2}\) and he did the same with all of the other holes.%
\par
To be sure there is no record of Newton explicitly saying, ``Here's how I'm going to fill in the holes in the rational number line.'' Along with everyone else at the time, he simply assumed there were no holes and moved on.  It took about \(200\) years of puzzling and arguing over the contradictions, anomalies and paradoxes to work out the consequences of that apparently simple assumption.  The task may not yet be fully accomplished, but by the 20th century the properties of the real number system (\(\RR\)) as an extension of the rational number system (\(\QQ\)) were well understood.  Here are both systems visualized as lines:%
\begin{figureptx}{Figure}{\(\RR\) and \(\QQ\)}{RandQ}{}%
\begin{image}{0}{1}{0}{}%
\includegraphics[width=\linewidth]{external/images/RandQ.png}
\end{image}%
\tcblower
\end{figureptx}%
Impressive, no?%
\par
The reason they look alike, except for the labels \(\RR\) and \(\QQ\) of course, is that our ability to draw sketches of the objects we're studying utterly fails when we try to sketch \(\RR\), as different from \(\QQ\).  All of the holes in \(\QQ\) really are there, but the non-holes are packed together so closely that we can't separate them in a drawing. This inability to sketch the objects we study will be a frequent source of frustration.%
\par
Of course, this will not stop us from drawing sketches.  When we do, our imaginations will save us because it \emph{is} possible to \emph{imagine} \(\QQ\) as distinct from \(\RR\).  But put away the idea that a sketch is an accurate representation of anything.  At best our sketches will only be aids to the imagination.%
\par
So, at this point we will simply assume the existence of the real numbers.  We will assume also that they have all of the properties that we are used to.  This is perfectly acceptable as long as we make our assumptions explicit.  However we need to be aware that, so far, the existence and properties of the real numbers is an \emph{assumption} that has not been logically derived.  Any time we make an assumption we need to be prepared to either abandon it completely if we find that it leads to nonsensical results, or to re-examine the assumption in the light of these results to see if we can find another assumption that subsumes the first and explains the (apparently) nonsensical results.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Section 2.1 Additional Problems}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Additional Problems}{}{Additional Problems}{}{}{NumbersRealRational-AddProbs}
\begin{problem}{Problem}{}{NumbersRealRational-AddProbs-2}%
Determine if each of the following is always rational or always irrational. Justify your answers.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}The sum of two rational numbers.%
\item{}The sum of two irrational numbers.%
\item{}The sum of a rational and an irrational number.%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{NumbersRealRational-AddProbs-3}%
\index{\(\QQ\)!creating irrationals from rationals}\index{\(\RR\)!irrational numbers}\index{\(\QQ\)!is it possible to have two rational numbers, \(a\) and \(b\), such that \(a^b\) is irrational} Is it possible to have two rational numbers, \(a\) and \(b\), such that \(a^b\) is irrational? If so, display an example of such \(a\) and \(b\). If not, prove that it is not possible.%
\end{problem}
\begin{problem}{Problem}{}{NumbersRealRational-AddProbs-4}%
Decide if it is possible to have two irrational numbers, \(a\) and \(b\), such that \(a^b\) is rational. Prove it in either case.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{NumbersRealRational-AddProbs-4-5}{}\quad{}The number \(\left( \sqrt{2}\right)^{\sqrt{2}}\) must be either rational or irrational (why?). What can you conclude in each case?%
\end{problem}
\end{sectionptx}
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 3 Calculus in the 17th and 18th Centuries}
\typeout{************************************************}
%
\begin{chapterptx}{Chapter}{Calculus in the 17th and 18th Centuries}{}{Calculus in the 17th and 18th Centuries}{}{}{CalcIn17th18thCentury}
\renewcommand*{\chaptername}{Chapter}
%
%
\typeout{************************************************}
\typeout{Section 3.1 Newton and Leibniz Get Started}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Newton and Leibniz Get Started}{}{Newton and Leibniz Get Started}{}{}{CalcIn17th18thCentury-NewtLeibStart}
%
%
\typeout{************************************************}
\typeout{Subsection  Leibniz's Calculus Rules}
\typeout{************************************************}
%
\begin{subsectionptx}{Subsection}{Leibniz's Calculus Rules}{}{Leibniz's Calculus Rules}{}{}{sec_leibn-calc-rules}
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Leibniz/}{Gottfried Wilhelm Leibniz}\protect\footnotemark{}}{sec_leibn-calc-rules-2}{}%
\index{Leibniz, Gottfried Wilhelm!portrait of}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Leibniz.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[8]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Leibniz/}\label{sec_leibn-calc-rules-2-1-2}}%
The rules for Calculus were first laid out in Gottfried Wilhelm Leibniz's 1684 paper\index{Leibniz, Gottfried Wilhelm!first Calculus publication} \textit{Nova methodus pro maximis et minimis, itemque tangentibus, quae nec fractas nec irrationales, quantitates moratur, et singulare pro illi calculi genus} (A New Method for Maxima and Minima as Well as Tangents, Which is Impeded Neither by Fractional Nor by Irrational Quantities, and a Remarkable Type of Calculus for This). Leibniz started with subtraction.  That is, if \(x_1\) and \(x_2\) are very close together then their difference, \(\Delta
x=x_2-x_1\), is very small.  He expanded this idea to say that if \(x_1\) and \(x_2\) are \emph{infinitely} close together (but still distinct) then their difference, \(\dx{ x}\), is infinitesimally small (but not zero).%
\begin{aside}{Aside}{\textit{Calculus Differentialis}.}{sec_leibn-calc-rules-4}%
This translates, loosely, as the calculus of differences.%
\end{aside}
This idea is logically very suspect and Leibniz knew it.  But he also knew that when he used his \textit{calculus differentialis} he was getting correct answers to some very hard problems.  So he persevered.%
\par
Leibniz called both \(\Delta x\) and \(\dx{ x}\) ``differentials'' (Latin for difference) because he thought of them as, essentially, the same thing.  Over time it has become customary to refer to the infinitesimal \(\dx{ x}\) as a differential, reserving ``difference'' for the finite case, \(\Delta x\).  This is why Calculus is often called ``Differential Calculus.''%
\par
In his paper Leibniz gave rules for dealing with these infinitely small differentials.  Specifically, given a variable quantity \(x\), \(dx\) represented an infinitesimal change in \(x\).  Differentials are related via the slope of the tangent line to a curve.  That is, if \(y=f(x)\), then \(\dx{ y}\) and \(\dx{ x}\) are related by%
\begin{equation*}
\dx{ y}=\text{ (slope of the tangent line) } \cdot \dx{ x}\text{.}
\end{equation*}
%
\par
Leibniz then divided by \(\dx{ x}\) giving%
\begin{equation*}
\dfdx{y}{x}= \text{ (slope of the tangent line). }
\end{equation*}
%
\par
The elegant and expressive notation Leibniz invented was so useful that it has been retained through the years despite some profound changes in the underlying concepts.  For example, Leibniz and his contemporaries would have viewed the symbol \(\dfdx{y}{x}\) as an actual quotient of infinitesimals, whereas today we define it via the limit concept first suggested by Newton.  \index{Newton, Isaac}%
\par
As a result the rules governing these differentials are very modern in appearance: \index{Leibniz, Gottfried Wilhelm!differentiation rules}%
\begin{align*}
\dx{(\text{ constant } )}\amp =0\\
\dx{(z-y+w+x)}\amp =\dx{ z}-\dx{ y}+\dx{ w}+\dx{ x}\\
\dx{(xv)}\amp =x\dx{ v}+v\dx{ x}\\
\dx{\left(\frac{v}{y}\right)}\amp =\frac{y\dx{ v}-v\dx{ y}}{yy}\\
\intertext{and, when \(a\) is an integer:}
\dx{(x^a)}\amp =ax^{a-1}\dx{ x}\text{.}
\end{align*}
%
\par
Leibniz states these rules without proof: ``. . . the demonstration of all this will be easy to one who is experienced in such matters . . ..'' As an example, mathematicians in Leibniz's day would be expected to understand intuitively that if \(c\) is a constant, then \(d(c)=c-c=0\).  Likewise, \(d(x+y)=dx+dy\) is really an extension of \((x_2+y_2)-(x_1+y_1)=(x_2-x_1)+(y_2-y_1)\).%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Leibniz's Approach to the Product Rule}
\typeout{************************************************}
%
\begin{subsectionptx}{Subsection}{Leibniz's Approach to the Product Rule}{}{Leibniz's Approach to the Product Rule}{}{}{LeibnizProductRule}
\index{Leibniz, Gottfried Wilhelm} The explanation of the product rule using differentials is a bit more involved, but Leibniz expected that mathematicans would be fluent enough to derive it.  The product \(p=xv\) can be thought of as the area of the following rectangle%
\begin{figureptx}{Figure}{}{fig1}{}%
\begin{image}{0.25}{0.5}{0.25}{}%
\includegraphics[width=\linewidth]{external/images/fig1.png}
\end{image}%
\tcblower
\end{figureptx}%
With this in mind, \(\dx{ p}=\dx{(xv)}\) can be thought of as the change in area when \(x\) is changed by \(\dx{ x}\) and \(v\) is changed by \(\dx{ v}\).  This can be seen as the L shaped region in the following drawing.%
\begin{figureptx}{Figure}{}{fig2}{}%
\begin{image}{0.05}{0.9}{0.05}{}%
\includegraphics[width=\linewidth]{external/images/fig2.png}
\end{image}%
\tcblower
\end{figureptx}%
By dividing the L shaped region into 3 rectangles we obtain%
\begin{equation}
\dx{(xv)}=x\dx{ v}+v\dx{ x}+\dx{ x}\,\dx{ v}\text{.}\label{eq_LeibnizProductRule}
\end{equation}
%
\par
Even though \(\dx{ x}\) and \(\dx{ v}\) are infinitely small, Leibniz reasoned that \(\dx{ x}\,\dx{ v}\) is \emph{even more} infinitely small (quadratically infinitely small?)  compared to \(x\dx{ v}\) and \(v\dx{ x}\) and can thus be ignored leaving%
\begin{equation*}
\dx{ (xv)}=x\dx{ v}+v\dx{ x}\text{.}
\end{equation*}
%
\par
You should feel some discomfort at the idea of simply tossing the product \(\dx{ x}\,\dx{ v}\) aside because it is ``comparatively small.'' This means you have been well trained, and have thoroughly internalized Newton's \index{Newton, Isaac} dictum~\hyperlink{newton45__sir_isaac_two_treat_quadr}{[{\xreffont 10}]}: ``The smallest errors may not, in mathematical matters, be scorned.'' It is logically untenable to toss aside an expression just because it is small.  Even less so should we be willing to ignore an expression on the grounds that it is ``infinitely smaller'' than another quantity which is itself ``infinitely small.''%
\par
Newton and Leibniz both knew this as well as we do.  But they also knew that their methods \emph{worked}.  They gave verifiably correct answers to problems which had, heretofore, been completely intractable.  It is the mark of their genius that both men persevered in spite of the very evident difficulties their methods entailed.%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Newton's Approach to the Product Rule}
\typeout{************************************************}
%
\begin{subsectionptx}{Subsection}{Newton's Approach to the Product Rule}{}{Newton's Approach to the Product Rule}{}{}{NewtonsApproach}
In the Principia, Newton ``proved'' the Product Rule as follows: Let \(x\) and \(v\) be ``flowing quantites'' and consider the rectangle, \(R\), whose sides are \(x\) and \(v\).  \(R\) is also a flowing quantity and we wish to find its fluxion (derivative) at any time.%
\begin{historical}{Historical Aside}{Newton's `Method of Fluxions'.}{NewtonsApproach-3}%
Newton's approach to Calculus \textemdash{} his `Method of Fluxions' \textemdash{} depended fundamentally on motion. That is, he viewed his variables (fluents) as changing (flowing or fluxing) in time.  The rate of change of a fluent he called a fluxion.  As a foundation both Leibniz's and Newton's approaches have fallen out of favor, although both are still universally used as a conceptual approach, a ``way of thinking,'' about the ideas of Calculus.%
\end{historical}
First increment \(x\) and \(v\) by \(\frac{\Delta x}{2}\) and \(\frac{\Delta v}{2}\) respectively. Then the corresponding increment of \(R\) is%
\begin{equation}
\left(x+\frac{\Delta x}{2}\right)\left(v+\frac{\Delta v}{2}\right) = xv + x\frac{\Delta v}{2} + v\frac{\Delta x}{2} +\frac{\Delta x\Delta v}{4}\text{.}\label{eq_prodruleinc}
\end{equation}
%
\par
Now decrement \(x\) and \(v\) by the same amounts:%
\begin{equation}
\left(x-\frac{\Delta x}{2}\right)\left(v-\frac{\Delta v}{2}\right) = xv - x\frac{\Delta v}{2} - v\frac{\Delta x}{2} + \frac{\Delta x\Delta v}{4}\text{.}\label{eq_prodruledec}
\end{equation}
%
\par
Subtracting the right side of \hyperref[eq_prodruledec]{equation~({\xreffont\ref{eq_prodruledec}})} from the right side of \hyperref[eq_prodruleinc]{equation~({\xreffont\ref{eq_prodruleinc}})} gives%
\begin{equation*}
\Delta R = x\Delta v + v\Delta x
\end{equation*}
which is the total change of \(R = xv\) over the intervals \(\Delta x\) and \(\Delta v\) and also recognizably the Product Rule.%
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Newton/}{Isaac Newton}\protect\footnotemark{}}{Newton}{}%
\index{Newton, Isaac!portrait of}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Newton.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[9]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Newton/}\label{Newton-2-2}}%
This argument is no better than Leibniz's as it relies heavily on the number \(1/2\) to make it work.  If we take any other increments in \(x\) and \(v\) whose total lengths are \(\Delta x\) and \(\Delta v\) it will simply not work.  Try it and see.%
\par
In Newton's defense, he wasn't really trying to justify his mathematical methods in the Principia.  His attention there was on physics, not math, so he was really just trying to give a convincing demonstration of his methods.  You may decide for yourself how convincing his demonstration is.%
\par
\index{Lagrange, Joseph-Louis} Notice that there is no mention of limits of difference quotients or derivatives.  In fact, the term derivative was not coined until 1797, by Lagrange.  In a sense, these topics were not necessary at the time, as Leibniz and Newton both assumed that the curves they dealt with had tangent lines and, in fact, Leibniz explicitly used the tangent line to relate two differential quantities.  This was consistent with the thinking of the time and for the duration of this chapter we will also assume that all quantities are differentiable.  As we will see later this assumption leads to difficulties.%
\par
Both Newton and Leibniz were satisfied that their Calculus provided answers that agreed with what was known at the time.  For example%
\begin{equation*}
\dx{ \left(x^2\right)}=\dx{\left(xx\right)}=x\dx{ x}+x\dx{
x}=2x\dx{ x} 
\end{equation*}
and%
\begin{equation*}
\dx{\left(x^3\right)}=\dx{\left(x^2x\right)}=x^2\dx{
x}+x\dx{\left(x^2\right)}=x^2+x\left(2x\dx{
x}\right)=3x^2\dx{ x}\text{,}
\end{equation*}
results that were essentially derived by others in different ways.%
\begin{problem}{Problem}{}{NewtonsApproach-12}%
Assume \(n\) is a positive integer.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Use Leibniz' product rule \(\dx{\left(xv\right)}=x\dx{v}+v\dx{x}\) to show that%
\begin{equation*}
\dx{\left(x^n\right)}=nx^{n-1}\dx{x}
\end{equation*}
%
\item{}Suppose \(y=x^{-1}=\frac{1}{x}\), Use Leibniz' product rule to show%
\begin{equation*}
dy=-1x^{-2}\dx{x}
\end{equation*}
%
\item{}Use the product rule and the result of part (b) to derive the quotient rule%
\begin{equation*}
d\left(\frac{v}{x}\right)=\frac{x\dx{v}-v\dx{x}}{x^2}
\end{equation*}
%
\item{}Use the quotient rule to show that%
\begin{equation*}
d\left(x^{-n}\right)=-nx^{-n-1}\dx{x}
\end{equation*}
%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{NewtonsApproach-13}%
\index{differentiation!power rule with fractional exponents} Suppose \(y=x^{\frac{p}{q}}\) with \(q\neq 0\), where \(p\) and \(q\) are integers. Show that%
\begin{equation*}
\dx{y}=\dx{
\left(x^{\frac{p}{q}}\right)}=\frac{p}{q}x^{\frac{p}{q}-1}\dx{
x}\text{.}
\end{equation*}
%
\end{problem}
To prove the worth of his Calculus Leibniz also provided applications.  As an example he derived Snell's Law of Refraction from his Calculus rules as follows.%
\par
Given that light travels through air at a speed of \(v_a\) and travels through water at a speed of \(v_w\) the problem is to find the fastest path from point \(A\) to point \(B\).%
\begin{figureptx}{Figure}{}{snellfig}{}%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/snellfig.png}
\end{image}%
\tcblower
\end{figureptx}%
According to Fermat's Principle of Least Time, this fastest path is the one that light will travel.%
\par
Using the fact that \(\text{ Time } =\text{ Distance }
/\text{ Velocity }\) and the labeling in the picture below we can obtain a formula for the time \(T\) it takes for light to travel from \(A\) to \(B\).%
\begin{figureptx}{Figure}{}{snellfig2}{}%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/snellfig2.png}
\end{image}%
\tcblower
\end{figureptx}%
%
\begin{equation*}
T=\frac{\sqrt{x^2+a^2}}{v_a}+\frac{\sqrt{(c-x)^2+b^2}}{v_w}
\end{equation*}
%
\par
Using the rules of Leibniz's Calculus, we obtain%
\begin{align*}
\dx{ T}\amp = \left(\frac{1}{v_a}\frac{1}{2}\left(x^2+a^2\right)^{-\frac{1}{2}} (2x)+\frac{1}{v_w}\frac{1}{2}((c-x)^2+b^2)^{-\frac{1}{2}}(2(c-x)(-1))\right) \dx{ x}\\
\amp =\left(\frac{1}{v_a}\frac{x}{\sqrt{x^2+a^2}}-\frac{1}{v_w}\frac{c-x}{\sqrt{(c-x)^2+b^2}}\right)\dx{ x}\text{.}
\end{align*}
%
\par
Using the fact that at the minimum value for \(T\), \(\dx{ T}=0\), we have that the fastest path from \(A\)to \(B\) must satisfy \(\frac{1}{v_a}\frac{x}{\sqrt{x^2+a^2}}=\frac{1}{v_w}\frac{c-x}{\sqrt{(c-x)^2+b^2}}\). Inserting the following angles%
\begin{figureptx}{Figure}{}{snellfig3}{}%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/snellfig3.png}
\end{image}%
\tcblower
\end{figureptx}%
we get that the path that light travels must satisfy \(\frac{\sin\theta_a}{v_a}=\frac{\sin\theta_w}{v_w}\) which is Snell's Law.%
\par
\index{Bernoulli, Johann}\index{Brachistochrone problem, the} To compare 18th century and modern techniques we will consider Johann Bernoulli's solution of the Brachistochrone problem. In 1696, Bernoulli posed, and solved, the Brachistochrone problem; that is, to find the shape of a frictionless wire joining points A and B so that the time it takes for a bead to slide down under the force of gravity is as small as possible.%
\begin{figureptx}{Figure}{}{brachfig1}{}%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/brachfig1.png}
\end{image}%
\tcblower
\end{figureptx}%
Bernoulli posed this ``path of fastest descent'' problem to challenge the mathematicians of Europe and used his solution to demonstrate the power of Leibniz's Calculus as well as his own ingenuity.  \index{Bernoulli, Johann!Bernoulli's challenge}%
\begin{quote}%
I, Johann Bernoulli, address the most brilliant mathematicians in the world. Nothing is more attractive to intelligent people than an honest, challenging problem, whose possible solution will bestow fame and remain as a lasting monument. Following the example set by Pascal, Fermat, etc., I hope to gain the gratitude of the whole scientific community by placing before the finest mathematicians of our time a problem which will test their methods and the strength of their intellect. If someone communicates to me the solution of the proposed problem, I shall publicly declare him worthy of praise. \hyperlink{Bernoulli_bio_mactutor}{[{\xreffont 11}]}%
\end{quote}
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Johann/}{Johann Bernoulli}\protect\footnotemark{}}{NewtonsApproach-29}{}%
\index{Bernoulli, Johann!portrait of}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/BernoulliJohann.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[10]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Johann/}\label{NewtonsApproach-29-1-2}}%
In addition to Johann's, solutions were obtained from Newton, \index{Newton, Isaac} Leibniz, Johann's brother Jacob Bernoulli, \index{Bernoulli Jacob} and the Marquis de l'Hopital \hyperlink{struik69__sourc_book_mathem}{[{\xreffont 15}]}.  At the time there was an ongoing and very vitriolic controversy raging over whether Newton or Leibniz had been the first to invent Calculus.  An advocate of the methods of Leibniz, Bernoulli did not believe Newton would be able to solve the problem using his methods. Bernoulli attempted to embarrass Newton by sending him the problem.  However Newton did solve it.%
\par
At this point in his life Newton had all but quit science and mathematics and was fully focused on his administrative duties as Master of the Mint.  In part due to rampant counterfeiting, England's money had become severely devalued and the nation was on the verge of economic collapse.  The solution was to recall all of the existing coins, melt them down, and strike new ones.  As Master of the Mint this job fell to Newton \hyperlink{levenson09__newton_count}{[{\xreffont 8}]}.  As you might imagine this was a rather Herculean task.  Nevertheless, according to his niece:%
\begin{quote}%
When the problem in 1696 was sent by Bernoulli\textendash{}Sir I.N. was in the midst of the hurry of the great recoinage and did not come home till four from the Tower very much tired, but did not sleep till he had solved it, which was by four in the morning.%
\end{quote}
He is later reported to have complained, ``I do not love . . . to be . . . teezed by forreigners about Mathematical things'' \hyperlink{dunham90__journ_throug_genius}{[{\xreffont 2}]}.%
\par
\index{Bernoulli, Johann!\textit{"Tanquam ex ungue leonem"}} Newton submitted his solution anonymously, presumably to avoid more controversy.  Nevertheless the methods used were so distinctively Newton's that Bernoulli is said to have exclaimed ``\textit{Tanquam ex ungue leonem}.''%
\begin{aside}{Aside}{\textit{Tanquam ex ungue leonem}.}{NewtonsApproach-35}%
``I know the lion by his claw.''%
\end{aside}
\index{Brachistochrone problem, the!Bernoulli's solution} Bernoulli's ingenious solution starts, interestingly enough, with Snell's Law of Refraction. He begins by considering the stratified medium in the following figure, where an object travels with velocities \(v_1,\,v_2,\,v_3,\,\ldots\) in the various layers.%
\begin{figureptx}{Figure}{}{brachfig2}{}%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/brachfig2.png}
\end{image}%
\tcblower
\end{figureptx}%
By repeatedly applying Snell's Law he concluded that the fastest path must satisfy%
\begin{equation*}
\frac{\sin \theta_1}{v_1}=\frac{\sin \theta_2}{v_2}=\frac{\sin\theta_3}{v_3}=\cdots\text{.}
\end{equation*}
%
\par
In other words, the ratio of the sine of the angle that the curve makes with the vertical and the speed remains constant along this fastest path.%
\par
If we think of a continuously changing medium as stratified into infinitesimal layers and extend Snell's law to an object whose speed is constantly changing, then along the fastest path, the ratio of the sine of the angle that the curve's tangent makes with the vertical, \(\alpha\), and the speed, \(v\), must remain constant.%
\begin{equation*}
\frac{\sin\alpha}{v}=c\text{.}
\end{equation*}
%
\begin{figureptx}{Figure}{}{snellfig4}{}%
\begin{image}{0.25}{0.5}{0.25}{}%
\includegraphics[width=\linewidth]{external/images/snellfig4.png}
\end{image}%
\tcblower
\end{figureptx}%
If we include axes and let \(P\) denote the position of the bead at a particular time then we have the following picture.%
\begin{figureptx}{Figure}{}{snellfig5}{}%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/snellfig5.png}
\end{image}%
\tcblower
\end{figureptx}%
In the above figure, \(s\) denotes the length that the bead has traveled down to point \(P\) (that is, the arc length of the curve from the origin to that point) and \(a\) denotes the tangential component of the acceleration due to gravity \(g\).  Since the bead travels only under the influence of gravity then \(\dfdx{v}{t}=a\).%
\par
To get a sense of how physical problems were approached using Leibniz's Calculus we will use the above equation to show that \(v=\sqrt{2gy}\).%
\par
By similar triangles we have \(\frac{a}{g}=\frac{\dx{ y}}{\dx{ s}}\). As a student of Leibniz, Bernoulli would have regarded \(\frac{\dx{ y}}{\dx{ s}}\) as a fraction so%
\begin{align*}
a\dx{ s} \amp = g\dx{ y}\\
\intertext{and since acceleration is the rate of change of velocity we have}
\frac{\dx{ v}}{\dx{ t}}\dx{ s} \amp = g\dx{ y}.\\
\intertext{Again, 18th century European mathematicians regarded \(\dx{ v}, \dx{ t}\), and \(\dx{ s}\) as infinitesimally small numbers which nevertheless obey all of the usual rules of algebra. Thus we can rearrange the above to get}
\frac{\dx{ s}}{\dx{ t}}\dx{ v} \amp = g\dx{ y}.\\
\intertext{Since \(\frac{\dx{ s}}{\dx{ t}}\) is the rate of change of position with respect to time it is, in fact, the velocity of the bead. That is}
v\dx{ v} \amp = g\dx{ y}.\\
\intertext{Bernoulli would have interpreted this as a statement that two rectangles of height \(v\) and \(g\), with respective widths \(\dx{ v}\) and \(\dx{ y}\) have equal area. Summing (integrating) all such rectangles we get:}
\int{}v\dx{ v} \amp = \int{}g\dx{ y}\\
\frac{v^2}{2} \amp = gy
\end{align*}
or%
\begin{equation}
v=\sqrt{2gy}\text{.}\label{eq_brach_vel}
\end{equation}
%
\par
You are undoubtedly uncomfortable with the cavalier manipulation of infinitesimal quantities you've just witnessed, so we'll pause for a moment now to compare a modern development of \hyperref[eq_brach_vel]{equation~({\xreffont\ref{eq_brach_vel}})} to Bernoulli's.  As before we begin with the equation:%
\begin{align*}
\frac{a}{g}\amp = \dfdx{y}{s}\\
a \amp = g\dfdx{y}{s}.\\
\intertext{Moreover, since acceleration is the derivative of velocity this is the same as:}
\dfdx{v}{t} \amp = g\dfdx{y}{s}.\\
\intertext{Now observe that by the Chain Rule \(\dfdx{v}{t} =
\dfdx{v}{s}\dfdx{s}{t}\). The physical interpretation of this formula is that velocity will depend on \(s\), how far down the wire the bead has moved, but that the distance traveled will depend on how much time has elapsed. Therefore}
\dfdx{v}{s}\dfdx{s}{t} \amp = g\dfdx{y}{s}\\
\intertext{or}
\dfdx{s}{t}\dfdx{v}{s} \amp = g\dfdx{y}{s}\\
\intertext{and since \(\dfdx{s}{t} = v\) we have}
v\dfdx{v}{s} \amp = g\dfdx{y}{s}\\
\intertext{Integrating both sides with respect to \(s\) gives:}
\int{}v\dfdx{v}{s}d s \amp = g\int{}\dfdx{y}{s}d s\\
\int{}vd v \amp = g\int{}d y\\
\intertext{and integrating gives}
\frac{v^2}{2} \amp = gy
\end{align*}
as before.%
\par
In effect, in the modern formulation we have traded the simplicity and elegance of differentials for a comparatively cumbersome repeated use of the Chain Rule.  No doubt you noticed when taking Calculus that in the differential notation of Leibniz,\index{Leibniz, Gottfried Wilhelm} the Chain Rule looks like ``canceling'' an expression in the top and bottom of a fraction: \(\dfdx{y}{u}\dfdx{u}{x} = \dfdx{y}{x}\).  This is because for 18th century mathematicians, this is exactly what it was.%
\par
To put it another way, 18th century mathematicians wouldn't have recognized a need for what we call the Chain Rule because this operation was a triviality for them.  Just reduce the fraction.  This begs the question: Why did we abandon such a clear and simple interpretation of our symbols in favor of the comparatively more cumbersome modern interpretation?  This is one of the questions we will try to answer in this course.%
\par
Returning to the Brachistochrone problem we observe that%
\begin{align}
\frac{\sin\alpha}{v} \amp = c\notag\\
\intertext{and since \(\sin\alpha = \frac{d x}{d s}\)   we see that}
\frac{\frac{d x}{d s}}{\sqrt{2gy}} \amp = c\notag\\
\frac{d x}{\sqrt{2gy(ds)^2}} \amp = c\notag\\
\frac{d x}{\sqrt{2gy\left[(d x)^2+(d y)^2\right]}} \amp = c\text{.}\label{eq_Brachistochrone}
\end{align}
%
\par
Bernoulli was then able to solve this differential equation.%
\begin{problem}{Problem}{}{NewtonsApproach-52}%
Show that the equations \(x=\frac{\phi-\sin
\phi}{4gc^2},\,y=\frac{1-\cos \phi}{4gc^2}\) satisfy equation \hyperref[eq_Brachistochrone]{equation~({\xreffont\ref{eq_Brachistochrone}})}. Bernoulli recognized this solution to be an inverted cycloid, the curve traced by a fixed point on a circle as the circle rolls along a horizontal surface.%
\end{problem}
This illustrates the state of Calculus in the late 1600s and early 1700s; the foundations of the subject were a bit shaky but there was no denying its power.%
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 3.2 Power Series as Infinite Polynomials}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Power Series as Infinite Polynomials}{}{Power Series as Infinite Polynomials}{}{}{ExponentAdditionProperty}
\index{polynomials!infinite} Applied to polynomials, the rules of differential and integral Calculus are straightforward.  Indeed, differentiating and integrating polynomials represent some of the easiest tasks in a Calculus course.  For example, computing \(\int(7-x+x^2)\dx{
x}\) is relatively easy compared to computing \(\int\sqrt[3]{1+x^3}\dx{ x}\).  Unfortunately, not all functions can be expressed as a polynomial.  For example, \(f(x)=\sin x\) cannot be since a polynomial has only finitely many roots and the sine function has infinitely many roots, namely \(\{n\pi|\,n\in\ZZ\}\).  A standard technique in the 18th century was to write such functions as an ``infinite polynomial,'' or what today we refer to as a power series.  Unfortunately an ``infinite polynomial'' is a much more subtle object than a mere polynomial, which by definition is finite.  For now we will not concern ourselves with these subtleties.  We will follow the example of our forebears and manipulate all ``polynomial\textendash{}like'' objects (finite or infinite) as if they are polynomials.%
\begin{definition}{Definition}{}{def_PowerSeries}%
\index{power series!definition of}%
A \terminology{power series centered at \(\boldsymbol{a}\)} is a series of the form%
\begin{equation*}
\sum_{n=0}^\infty a_n(x-a)^n=a_0+a_1(x-a)+a_2(x-a)^2+\cdots\text{.}
\end{equation*}
%
\par
Often we will focus on the behavior of power series \(\sum_{n=0}^\infty a_nx^n\), centered around \(0\), as the series centered around other values of \(a\) are obtained by shifting a series centered at \(0\).%
\end{definition}
Before we continue, we will make the following notational comment.  The most advantageous way to represent a series is using summation notation since there can be no doubt about the pattern to the terms.  After all, this notation contains a formula for the general term.  This being said, there are instances where writing this formula is not practical.  In these cases, it is acceptable to write the sum by supplying the first few terms and using ellipses (the three dots).  If this is done, then enough terms must be included to make the pattern clear to the reader.%
\par
\index{series!Geometric series!naive derivation} Returning to our definition of a power series, consider, for example, the geometric series \(\sum_{n=0}^\infty
x^n=1+x+x^2+\cdots\).  If we multiply this series by \((1-x)\), we obtain%
\begin{equation*}
(1-x)(1+x+x^2+\cdots)=(1+x+x^2+\cdots)-(x+x^2+x^3+\cdots)=1\text{.}
\end{equation*}
%
\par
This leads us to the power series representation%
\begin{equation*}
\frac{1}{1-x}=1+x+x^2+\cdots=\sum_{n=0}^\infty x^n \text{.}
\end{equation*}
%
\par
If we substitute \(x=\frac{1}{10}\) into the above, we obtain%
\begin{equation*}
1+\frac{1}{10}+\left(\frac{1}{10}\right)^2+\left(\frac{1}{10}\right)^3+
\cdots=\frac{1}{1-\frac{1}{10}}=\frac{10}{9} \text{.}
\end{equation*}
%
\par
This agrees with the fact that \(.333\ldots=\frac{1}{3}\), and so \(.111\ldots=\frac{1}{9}\), and \(1.111\ldots=\frac{10}{9}\).%
\par
There are limitations to these formal manipulations however. Substituting \(x=1\) or \(x=2\) yields the questionable results%
\begin{equation*}
\frac{1}{0}=1+1+1+\cdots\,\text{  and  }  \,\frac{1}{-1}=1+2+2^2+\cdots\text{.}
\end{equation*}
%
\par
We are missing something important here, though it may not be clear exactly what.  A series representation of a function works \emph{sometimes,} but there are some problems.  For now, we will continue to follow the example of our 18th century predecessors and ignore them.  That is, for the rest of this section we will focus on the formal manipulations to obtain and use power series representations of various functions.  Keep in mind that this is all highly suspect until we can resolve problems like those just given.%
\par
Power series became an important tool in analysis in the 1700s.  By representing various functions as power series they could be dealt with as if they were (infinite) polynomials.  The following is an example.%
\begin{example}{Example}{}{ExponentAdditionProperty-12}%
Solve the following Initial Value problem: Find \(y(x)\) given that \(\frac{\dx{ y}}{\dx{
x}}=y,\,y(0)=1\).%
\begin{aside}{Aside}{Comment.}{ExponentAdditionProperty-12-2-2}%
A few seconds of thought should convince you that the solution of this problem is \(y(x) = e^x\).  We will ignore this for now in favor of emphasising the technique.%
\end{aside}
Assuming the solution can be expressed as a power series we have%
\begin{equation*}
y=\sum_{n=0}^\infty
a_nx^n=a_0+a_1x+a_2x^2+\cdots \text{.}
\end{equation*}
%
\par
Differentiating gives us%
\begin{equation*}
\frac{\dx{ y}}{\dx{
x}}=a_1+2a_2x+3a_3x^2+4a_4x^3+\ldots \text{.}
\end{equation*}
%
\par
Since \(\frac{\dx{ y}}{\dx{ x}}=y\) we see that%
\begin{equation*}
a_1=a_0\,,\,2a_2=a_1\,,\,3a_3=a_2\,,\,\ldots,\,na_n=a_{n-1}\,,\ldots\text{.}
\end{equation*}
%
\par
This leads to the relationship%
\begin{equation*}
a_n=\frac{1}{n}a_{n-1}=\frac{1}{n(n-1)}a_{n-2}=\cdots=\frac{1}{n!}a_0\text{.}
\end{equation*}
%
\par
Thus the series solution of the differential equation is%
\begin{equation*}
y=\sum_{n=0}^\infty\frac{a_0}{n!}x^n
=a_0\sum_{n=0}^\infty\frac{1}{n!}x^n \text{.}
\end{equation*}
%
\par
Using the initial condition \(y(0)=1\), we get \(1=a_0(1+0+\frac{1}{2!}0^2+\cdots)=a_0\).  Thus the solution to the initial problem is \(y=\sum_{n=0}^\infty\frac{1}{n!}x^n\).  Let's call this function \(E(x)\).  Then by definition%
\begin{equation*}
E(x)=\sum_{n=0}^\infty\frac{1}{n!}x^n=1+\frac{x^1}{1!}+\frac{x^2}{2!}+\frac{x^3}{3!}+\,\ldots\text{.}
\end{equation*}
%
\end{example}
Let's examine some properties of this function.  The first property is clear from the definition.%
\par
\terminology{Property 1}. \(E(0)=1\)%
\par
\terminology{Property 2}. \(E(x+y)=E(x)E(y)\).%
\par
To see this we multiply the two series together, so we have%
\begin{align}
E(x)E(y) =\amp \left(\sum_{n=0}^\infty\frac{1}{n!}x^n\right)\left(\sum_{n=0}^\infty\frac{1}{n!}y^n\right)\notag\\
=\amp \left(\frac{x^0}{0!}+\frac{x^1}{1!}+\frac{x^2}{2!}+\frac{x^3}{3!}+\,\ldots\right)\left(\frac{y^0}{0!}+\frac{y^1}{1!}+\frac{y^2}{2!}+\frac{y^3}{3!}+\,\ldots\right)\notag\\
=\amp \frac{x^0}{0!}\frac{y^0}{0!}+\frac{x^0}{0!}\frac{y^1}{1!}+\frac{x^1}{1!}\frac{y^0}{0!}+\frac{x^0}{0!}\frac{y^2}{2!}+\frac{x^1}{1!}\frac{y^1}{1!}+\frac{x^2}{2!}\frac{y^0}{0!}\notag\\
\amp        +\frac{x^0}{0!}\frac{y^3}{3!}+\frac{x^1}{1!}\frac{y^2}{2!}+\frac{x^2}{2!}\frac{y^1}{1!}+\frac{x^3}{3!}\frac{y^0}{0!}+\,\ldots\notag\\
=\amp\frac{x^0}{0!}\frac{y^0}{0!}+\left(\frac{x^0}{0!}\frac{y^1}{1!}+ \frac{x^1}{1!}\frac{y^0}{0!}\right)\notag\\
\amp        +\left(\frac{x^0}{0!}\frac{y^2}{2!}+\frac{x^1}{1!}\frac{y^1}{1!}+\frac{x^2}{2!}\frac{y^0}{0!}\right)\notag\\
\amp       +\left(\frac{x^0}{0!}\frac{y^3}{3!}+\frac{x^1}{1!}\frac{y^2}{2!}+\frac{x^2}{2!}\frac{y^1}{1!}+\frac{x^3}{3!}\frac{y^0}{0!}\right)+\,\ldots\notag\\
=\amp\frac{1}{0!}+\frac{1}{1!}\left(\frac{1!}{0!1!}x^0y^1+\frac{1!}{1!0!}x^1y^0\right)\notag\\
\amp       +\frac{1}{2!}\left(\frac{2!}{0!2!}x^0y^2+\frac{2!}{1!1!}x^1y^1+\frac{2!}{2!0!}x^2y^0\right)\notag\\
\amp       +\frac{1}{3!}\left(\frac{3!}{0!3!}x^0y^3+\frac{3!}{1!2!}x^1y^2+\frac{3!}{2!1!}x^2y^1+\frac{3!}{3!0!}x^3y^0\right)+\ldots\notag\\
=\amp \frac{1}{0!}+\frac{1}{1!}\left(\binom{1}{0}x^0y^1+\binom{1}{1}x^1y^0\right)\notag\\
\amp       +\frac{1}{2!}\left(\binom{2}{0}x^0y^2+\binom{2}{1}x^1y^1+\binom{2}{2}x^2y^0\right)\notag\\
\amp       +\frac{1}{3!}\left(\binom{3}{0}x^0y^3+\binom{3}{1}x^1y^2+\binom{3}{2}x^2y^1+\binom{3}{3}x^3y^0\right)+\ldots\notag\\
=\amp \frac{1}{0!}+\frac{1}{1!}\left(x+y\right)^1+\frac{1}{2!}\left(x+y\right)^2+\frac{1}{3!}\left(x+y\right)^3+\ldots\notag\\
= \amp E(x+y)\text{.}\label{eq_ExponentAdditionProperty}
\end{align}
%
\par
\terminology{Property 3}. If \(m\) is a positive integer then \(E(mx)=\left(E(x\right))^m\). In particular, \(E(m)=\left(E(1)\right)^m\).%
\begin{problem}{Problem}{}{ExponentAdditionProperty-18}%
Prove Property 3.%
\end{problem}
\terminology{Property 4}. \(E(-x)=\frac{1}{E(x)}=\left(E(x)\right)^{-1}\).%
\begin{problem}{Problem}{}{ExponentAdditionProperty-20}%
Prove Property 4.%
\end{problem}
\terminology{Property 5}. If \(n\) is an integer with \(n\neq 0\), then \(E(\frac{1}{n})=\sqrt[n]{E(1)}=\left(E(1)\right)^{1/n}\).%
\begin{problem}{Problem}{}{ExponentAdditionProperty-22}%
Prove Property 5.%
\end{problem}
\terminology{Property 6}. If \(m\) and \(n\) are integers with \(n\neq 0\), then \(E\left(\frac{m}{n}\right)=\left(E(1)\right)^{m/n}\).%
\begin{problem}{Problem}{}{ExponentAdditionProperty-24}%
Prove Property 6.%
\end{problem}
\begin{definition}{Definition}{}{def_e}%
\index{\(e^x\)!definition of \(e\)}%
Let \(E(1)\) be denoted by the number \(e\). Using the series \(e=E(1)=\sum_{n=0}^\infty\frac{1}{n!}\), we can approximate \(e\) to any degree of accuracy. In particular \(e\approx 2.71828\).%
\end{definition}
In light of Property 6, we see that for any rational number \(r\), \(E(r)=e^r\).  Not only does this give us the series representation \(e^r=\sum_{n=0}^\infty\frac{1}{n!}r^n\) for any rational number \(r\), but it gives us a way to define \(e^x\) for irrational values of \(x\) as well.  That is, we can define%
\begin{equation*}
e^x=E(x)=\sum_{n=0}^\infty\frac{1}{n!}x^n
\end{equation*}
for any real number \(x\).%
\par
As an illustration, we now have \(e^{\sqrt{2}}=\sum_{n=0}^\infty\frac{1}{n!}\left(\sqrt{2}\right)^n\). The expression \(e^{\sqrt{2}}\) is meaningless if we try to interpret it as one irrational number raised to another. What does it mean to raise anything to the \(\sqrt{2}\) power?  However the series \(\sum_{n=0}^\infty\frac{1}{n!}\left(\sqrt{2}\right)^n\) does seem to have meaning and it can be used to extend the exponential function to irrational exponents.  In fact, defining the exponential function via this series answers the question we raised in \hyperref[NumbersRealRational]{Chapter~{\xreffont\ref{NumbersRealRational}}}: What does \(4^{\sqrt{2}}\) mean?%
\par
It means \(\displaystyle 4^{\sqrt{2}} = e^{\sqrt{2}\log 4}
= \sum_{n=0}^\infty\frac{(\sqrt{2}\log 4)^n}{n!}\).%
\par
This may seem to be the long way around just to define something as simple as exponentiation.  But that is a fundamentally misguided attitude.  Exponentiation only \emph{seems} simple because we've always thought of it as repeated multiplication (in \(\ZZ\)) or root-taking (in \(\QQ\)).  When we expand the operation to the real numbers this simply can't be the way we interpret something like \(4^{\sqrt{2}}\).  How do you take the product of \(\sqrt{2}\) copies of \(4?\) The concept is meaningless.  What we need is an interpretation of \(4^{\sqrt{2}}\) which is consistent with, say \(4^{3/2}
= \left(\sqrt{4}\right)^3=8\).  This is exactly what the series representation of \(e^x\) provides.%
\par
We also have a means of computing integrals as series.  For example, the famous ``bell shaped'' curve given by the function \(f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\) is of vital importance in statistics and must be integrated to calculate probabilities.  The power series we developed gives us a method of integrating this function.  For example, we have%
\begin{align*}
\int_{x=0}^b\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}d x \amp  =\frac{1}{\sqrt{2\pi}}\int_{x=0}^b\left(\sum_{n=0}^\infty\frac{1}{n!}\left(\frac{-x^2}{2}\right)^n\right)d x\\
\amp =\frac{1}{\sqrt{2\pi}}\,\sum_{n=0}^\infty\left(\frac{\left(-1\right)^n}{n!2^n}\int_{x=0}^bx^{2n}d x\right)\\
\amp =\frac{1}{\sqrt{2\pi}}\,\sum_{n=0}^\infty\left(\frac{\left(-1\right)^nb^{2n+1}}{n!2^n\left(2n+1\right)}\right)\text{.}
\end{align*}
%
\par
This series can be used to approximate the integral to any degree of accuracy.  The ability to provide such calculations made power series of paramount importance in the 1700s.%
\begin{problem}{Problem}{}{ExponentAdditionProperty-32}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Show that if \(y=\sum_{n=0}^\infty a_nx^n\) satisfies the differential equation \(\frac{\dx^2y}{\dx{ x}^2}=-y\), then%
\begin{equation*}
a_{n+2}=\frac{-1}{\left(n+2\right)\left(n+1\right)}\,a_n
\end{equation*}
and conclude that%
\begin{align*}
y=a_0+a_1x-\amp{}\frac{1}{2!}a_0x^2-\frac{1}{3!}a_1x^3+\frac{1}{4!}a_0x^4+\frac{1}{5!}a_1x^5\\
\amp{}-\frac{1}{6!}a_0x^6-\frac{1}{7!}a_1x^7+\cdots.
\end{align*}
%
\item{}Since \(y=\sin x\) satisfies \(\frac{\dx^2y}{\dx{
x}^2}=-y\), we see that%
\begin{align*}
\sin x=a_0+a_1x-\frac{1}{2!}a_0x^2-\amp{}\frac{1}{3!}a_1x^3+\frac{1}{4!}a_0x^4+\frac{1}{5!}a_1x^5\\
\amp{}-\frac{1}{6!}\,a_0x^6-\frac{1}{7!}\,a_1x^7+\cdots
\end{align*}
for some constants \(a_0\) and \(a_1\).  Show that in this case \(a_0=0\) and \(a_1=1\) and obtain%
\begin{align*}
\sin
x\amp{}=x-\frac{1}{3!}\,x^3+\frac{1}{5!}x^5-\frac{1}{7!}x^7+\cdots\\
\amp{}=\sum_{n=0}^\infty\frac{\left(-1\right)^n}{\left(2n+1\right)!}x^{2n+1}.
\end{align*}
%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{ExponentAdditionProperty-33}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Use the series%
\begin{align*}
\sin x\amp{}=x-\frac{1}{3!}\,x^3+\frac{1}{5!}x^5-\frac{1}{7!}x^7+\cdots\\
\amp{}=\sum_{n=0}^\infty\frac{\left(-1\right)^n}{\left(2n+1\right)!}x^{2n+1}
\end{align*}
to obtain the series%
\begin{align*}
\cos
x\amp{}=1-\frac{1}{2!}\,x^2+\frac{1}{4!}x^4-\frac{1}{6!}x^6+\cdots\\
\amp{}=\sum_{n=0}^\infty\frac{\left(-1\right)^n}{\left(2n\right)!}x^{2n}.
\end{align*}
%
\item{}Let \(s(x,N)=\sum_{n=0}^N\frac{\left(-1\right)^n}{\left(2n+1\right)!}x^{2n+1}\) and \(c(x,N)=\sum_{n=0}^N\frac{\left(-1\right)^n}{\left(2n\right)!}x^{2n}\) and use a computer algebra system to plot these for\(\,-4\pi\leq x\leq
4\pi,\,\,N=1,2,5,\,10,\,15\).  Describe what is happening to the series as N becomes larger.%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{prob_alternating_harmonic_series}%
Use the geometric series, \(\frac{1}{1-x}=1+x+x^2+x^3+\cdots=\sum_{n=0}^\infty x^n\), to obtain a series for \(\frac{1}{1+x^2}\) and use this to obtain the series%
\begin{align*}
\arctan
x\amp{}=x-\frac{1}{3}x^3+\frac{1}{5}x^5-\cdots\\
\amp{}=\sum_{n=0}^\infty(-1)^n
\frac{1}{2n+1}x^{2n+1}.
\end{align*}
%
\par
Use the series above to obtain the series \(\frac{\pi}{4}=\sum_{n=0}^\infty(-1)^n\frac{1}{2n+1}\).%
\end{problem}
The series for arctangent was known by James Gregory (1638-1675) and it is sometimes referred to as ``Gregory's series.'' Leibniz\index{Leibniz, Gottfried Wilhelm} independently discovered \(\frac{\pi}{4}=1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\cdots\) by examining the area of a circle.  Though it gives us a means for approximating \(\pi\) to any desired accuracy, the series converges too slowly to be of any practical use. For example, if we compute the sum of the first \(1000\) terms we get%
\begin{equation*}
4\left(\sum_{n=0}^{1000}(-1)^n\frac{1}{2n+1}\right)\approx 3.142591654
\end{equation*}
which only approximates \(\pi\) to two decimal places.%
\par
\index{Newton, Isaac} Newton knew of these results and the general scheme of using series to compute areas under curves.  These results motivated Newton to provide a series approximation for \(\pi\) as well, which, hopefully, would converge faster. We will use modern terminology to streamline Newton's ideas. First notice that \(\frac{\pi}{4}=\int_{x=0}^1\sqrt{1-x^2}\dx{ x}\) as this integral gives the area of one quarter of the unit circle. The trick now is to find series that represents \(\sqrt{1-x^2}\).%
\par
To this end we start with the binomial theorem%
\begin{equation*}
\left(a+b\right)^N=\sum_{n=0}^N\binom{N}{n}a^{N-n}b^n\text{,}
\end{equation*}
where%
\begin{align*}
\binom{N}{n}\amp =\frac{N!}{n!\left(N-n\right)!}\\
\amp =\frac{N\left(N-1\right)\left(N-2\right)\cdots\left(N-n+1\,\right)}{n!}\\
\amp =\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\text{.}
\end{align*}
%
\par
Unfortunately, we now have a small problem with our notation which will be a source of confusion later if we don't fix it.  So we will pause to address this matter.  We will come back to the binomial expansion afterward.%
\par
This last expression is becoming awkward in much the same way that an expression like%
\begin{equation*}
1+\frac{1}{2}+\left(\frac{1}{2}\right)^2+\left(\frac{1}{2}\right)^3+\ldots+\left(\frac{1}{2}\right)^k
\end{equation*}
is awkward.  Just as this sum is less cumbersome when written as \(\sum_{n=0}^k\left(\frac{1}{2}\right)^n\) the \emph{product}%
\begin{equation*}
N\left(N-1\right)\left(N-2\right)\cdots\left(N-n+1\,\right)
\end{equation*}
is less cumbersom when we write it as \(\prod_{j=0}^{n-1}\left(N-j\right)\).%
\par
A capital pi (\(\Pi\)) is used to denote a product in the same way that a capital sigma (\(\Sigma\)) is used to denote a sum.  The most familiar example would be writing%
\begin{equation*}
n!=\prod_{j=1}^{n}j\text{.}
\end{equation*}
%
\par
Just as it is convenient to define \(0!=1\), we will find it convenient to define \(\prod_{j=1}^{0}=1\). Similarly, the fact that \(\binom{N}{0}=1\) leads to the convention \(\prod_{j=0}^{-1}\left(N-j\right)=1\). Strange as this may look, it is convenient and \emph{is} consistent with the convention \(\sum_{j=0}^{-1}s_j=0\).%
\par
Returning to the binomial expansion and recalling our convention%
\begin{equation*}
\prod_{j=0}^{-1}\left(N-j\right)=1\text{,}
\end{equation*}
we can write,%
\begin{equation*}
\left(1+x\right)^N=1+\sum_{n=1}^N\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\right)x^n = \sum_{n=0}^N\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\right)x^n\text{.}
\end{equation*}
%
\par
These two representations probably look the same at first. Take a moment and be sure you see where they differ.%
\par
There is an advantage to using this convention (especially when programing a product into a computer), but this is not a deep mathematical insight.  It is just a notational convenience and we don't want you to fret over it, so we will use both formulations (at least initially).%
\par
Notice that we can extend the above definition of \(\binom{N}{n}\) to values \(n>N\).  In this case, \(\prod_{j=0}^{n-1}\left(N-j\right)\) will equal 0 as one of the factors in the product will be \(0\) (the one where \(j=N\)).  This gives us that \(\binom{N}{n}=0\) when \(n>N\) and so%
\begin{equation*}
\left(1+x\right)^N=1+\sum_{n=1}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\text{ } \right)x^n= \sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\text{ } \right)x^n
\end{equation*}
holds true for any nonnegative integer \(N\). Essentially Newton asked if it could be possible that the above equation could hold values of \(N\) which are not nonnegative integers.  For example, if the equation held true for \(N=\frac{1}{2}\) , we would obtain%
\begin{equation*}
\left(1+x\right)^{\frac{1}{2}}=1+\sum_{n=1}^\infty\left(\frac{ \prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\right)x^n=\sum_{n=0}^\infty\left(\frac{ \prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\right)x^n
\end{equation*}
or%
\begin{equation}
\left(1+x\right)^{\frac{1}{2}}=1+\frac{1}{2}x+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)}{2!}x^2+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right)}{3!}x^3+\cdots\text{.}\label{eq_BinomialSeries}
\end{equation}
%
\par
Notice that since \(1/2\) is not an integer the series no longer terminates.  Although Newton did not prove that this series was correct (nor did we), he tested it by multiplying the series by itself.  When he saw that by squaring the series he started to obtain \(1+x+0\,x^2+0\,x^3+\cdots\), he was convinced that the series was exactly equal to \(\sqrt{1+x}\).%
\begin{problem}{Problem}{}{ExponentAdditionProperty-47}%
Consider the series representation%
\begin{align*}
\left(1+x\right)^{\frac{1}{2}}\amp =1+\sum_{n=1}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n\\
\amp  =\sum_{n=0}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n\text{.}
\end{align*}
%
\par
Multiply this series by itself and compute the coefficients for \(x^0,\,x^1,\,x^2,\,x^3,\,x^4\) in the resulting series.%
\end{problem}
\begin{problem}{Problem}{}{prob_SqrtSeriesProb}%
Let%
\begin{equation*}
S(x,M)=\sum_{n=0}^M\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j \right)}{n!}x^n\text{.}
\end{equation*}
%
\par
Use a computer algebra system to plot \(S(x,M)\) for \(M=5,\,10,\,15,\,95,\,100\) and compare these to the graph for \(\sqrt{1+x}\).  What seems to be happening?  For what values of \(x\) does the series appear to converge to \(\sqrt{1+x}?\)%
\end{problem}
Convinced that he had the correct series, Newton used it to find a series representation of \(\int_{x=0}^1\sqrt{1-x^2}
\dx{ x}\).%
\begin{problem}{Problem}{}{ExponentAdditionProperty-50}%
Use the series \(\displaystyle
\left(1+x\right)^{\frac{1}{2}}=\sum_{n=0}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n\) to obtain the series%
\begin{align*}
\frac{\pi}{4}\amp =\int_{x=0}^1\sqrt{1-x^2} \dx{ x}\\
\amp =\sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\text{ } \right)\left(\frac{\left(-1\right)^n}{2n+1}\right)\\
\amp =1-\frac{1}{6}-\frac{1}{40}-\frac{1}{112}-\frac{5}{1152}-\cdots\text{.}
\end{align*}
%
\par
Use a computer algebra system to sum the first 100 terms of this series and compare the answer to \(\frac{\pi}{4}\).%
\end{problem}
Again, Newton had a series which could be verified (somewhat) computationally.  This convinced him even further that he had the correct series.%
\begin{problem}{Problem}{}{ExponentAdditionProperty-52}%
%
\begin{enumerate}[label={(\alph*)}]
\item{}Show that%
\begin{equation*}
\int_{x=0}^{1/2}\sqrt{x-x^2}\dx{ x}=\sum_{n=0}^\infty\frac{(-1)^n\,\,\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{\sqrt{2\,}n!\left(2n+3\right)2^n}
\end{equation*}
and use this to show that%
\begin{equation*}
\pi=16\left(\sum_{n=0}^\infty\frac{(-1)^n\,\,\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{\sqrt{2\,}n!\left(2n+3\right)2^n}\right)\text{.}
\end{equation*}
%
\item{}We now have two series for calculating \(\pi:\) the one from part (a) and the one derived earlier, namely%
\begin{equation*}
\pi=4\left(\sum_{n=0}^\infty\frac{(-1)^n\,\,}{2n+1}\right)\text{.}
\end{equation*}
We will explore which one converges to \(\pi\) faster.  With this in mind, define \(S1(N)=16\left(\sum_{n=0}^N\frac{(-1)^n\,\,\prod_{j=0}^{n-1}\left(
\frac{1}{2}-j\right)}{\sqrt{2\,}n!\left(2n+3\right)2^n}\right)\) and \(S2(N)=4\left(\sum_{n=0}^N\frac{(-1)^n\,\,}{2n+1}\right)\). Use a computer algebra system to compute \(S1(N)\)and \(S2(N)\) for \(N=5,10,15,20\).  Which one appears to converge to \(\pi\) faster?%
\end{enumerate}
%
\end{problem}
In general the series representation%
\begin{align*}
\left(1+x\right)^\alpha \amp  =\sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(\alpha-j\right)}{n!}\text{ } \right)x^n\\
\amp =1+\alpha x+\frac{\alpha\left(\alpha-1\right)}{2!}x^2+\frac{\alpha\left(\alpha-1\right)\left(\alpha-2\right)}{3!}x^3+\cdots
\end{align*}
is called the \terminology{binomial series} (or Newton's binomial series).  This series is correct when \(\alpha\) is a non-negative integer (after all, that is how we got the series).  We can also see that it is correct when \(\alpha=-1\) as we obtain%
\begin{align*}
\left(1+x\right)^{-1}\amp =\sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(-1-j\right)}{n!}\text{ } \right)x^n\\
\amp =1+(-1)x+\frac{-1\left(-1-1\right)}{2!}x^2+\frac{-1\left(-1-1\right)\left(-1-2\right)}{3!}x^3+\cdots\\
\amp =1-x+x^2-x^3+\cdots
\end{align*}
which can be obtained from the geometric series \(\frac{1}{1-x}=1+x+x^2+\cdots\) .%
\par
In fact, the binomial series is the correct series representation for all values of the exponent \(\alpha\) (though we haven't proved this yet).%
\begin{problem}{Problem}{}{ExponentAdditionProperty-55}%
Let \(k\) be a positive integer.  Find the power series, centered at zero, for \(f(x) =
\left(1-x\right)^{-k}\) by%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Differentiating the \index{series!Geometric series!differentiating} geometric series \(\left(k-1\right)\) times.%
\item{}Applying the binomial series.%
\item{}Compare these two results.%
\end{enumerate}%
\end{problem}
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Euler/}{Leonhard Euler}\protect\footnotemark{}}{ExponentAdditionProperty-56}{}%
\index{Euler, Leonhard!portrait of}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Euler.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[11]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Euler/}\label{ExponentAdditionProperty-56-1-2}}%
Leonhard Euler was a master at exploiting power series.  In 1735, the 28 year-old Euler won acclaim for what is now called the Basel problem: to find a closed form for \(\sum_{n=1}^\infty\frac{1}{n^2}\).  Other mathematicans knew that the series converged, but Euler was the first to find its exact value.  The following problem essentially provides Euler's solution.%
\begin{problem}{Problem}{The Basel Problem.}{ExponentAdditionProperty-58}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Show that the power series for \(\frac{\sin x}{x}\) is given by \(1-\frac{1}{3!}x^2+\frac{1}{5!}x^4-\cdots\)%
\item{}Use (a) to infer that the roots of \(1-\frac{1}{3!}x^2+\frac{1}{5!}x^4-\cdots\) are given by%
\begin{equation*}
x=\pm\pi,\,\pm 2\pi,\,\pm 3\pi,\,\ldots
\end{equation*}
%
\item{}Suppose \(p(x)=a_0+a_1x+\cdots+a_nx^n\) is a polynomial with roots \(r_1,\,r_2,\,\ldots,r_n\). Show that if \(a_0\neq\) \(0\), then all the roots are non-zero and%
\begin{equation*}
p(x)=a_0\left(1-\frac{x}{r_1}\right)\left(1-\frac{x}{r_2}\right)\cdots\left(1-\frac{x}{r_n}\right)\text{.}
\end{equation*}
%
\item{}Assuming that the result in part (c) holds for an infinite polynomial (power series), deduce that%
\begin{align*}
1-\frac{1}{3!}x^2+\frac{1}{5!}x^4-\cdots\amp =\left(1-\left(\frac{x}{\pi}\right)^2\right)\left(1-\left(\frac{x}{2\pi}\right)^2\right)\left(1-\left(\frac{x}{3\pi}\right)^2\right)\cdots
\end{align*}
%
\item{}Expand this product to deduce%
\begin{equation*}
\sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6}.{}
\end{equation*}
%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{Euler's Formula.}{ExponentAdditionProperty-59}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Use the power series expansion of \(e^x\), \(\sin
x,\) and \(\cos x\) to derive \terminology{Euler's Formula}:%
\begin{equation*}
e^{i\theta} = \cos\theta+i\sin\theta.
\end{equation*}
%
\item{}Use Euler's formula to derive the Addition\slash{}Subtraction formulas from Trigonometry:%
\begin{equation*}
\sin(\alpha\pm\beta) = \sin\alpha\cos\beta\pm\sin\beta\cos\alpha
\end{equation*}
%
\begin{equation*}
\cos(\alpha\pm\beta) = \cos\alpha\cos\beta\mp\sin\alpha\sin\beta
\end{equation*}
%
\item{}Use Euler's formula to show that%
\begin{equation*}
\sin 2\theta = 2\cos\theta\sin\theta
\end{equation*}
%
\begin{equation*}
\cos 2\theta =\cos^2\theta-\sin^2\theta
\end{equation*}
%
\item{}Use Euler's formula to show that%
\begin{equation*}
\sin 3\theta = 3\cos^2\theta\sin\theta-\sin^3\theta
\end{equation*}
%
\begin{equation*}
\cos 3\theta=\cos^3\theta-3\cos\theta\sin^2\theta
\end{equation*}
%
\item{}Find a formula \(\sin(n\theta)\) and \(\cos(n\theta)\) for any positive integer \(n\).%
\end{enumerate}%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 3.3 Additional Problems}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Additional Problems}{}{Additional Problems}{}{}{CalcIn17th18thCentury-AddProb}
\begin{problem}{Problem}{}{CalcIn17th18thCentury-AddProb-2}%
Use the geometric series to obtain the series%
\begin{align*}
\ln \left(1+x\right)\amp =x-\frac{1}{2}x^2+\frac{1}{3}x^3-\cdots\\
\amp =\sum_{n=0}^\infty\frac{(-1)^n}{n+1}x^{n+1}.{}
\end{align*}
%
\end{problem}
\begin{problem}{Problem}{}{CalcIn17th18thCentury-AddProb-3}%
\emph{Without} using Taylor's Theorem, represent the following functions as power series expanded about 0 (i.e., in the form \(\sum_{n=0}^\infty a_nx^n\)).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\ln\left(1-x^2\right)\)%
\item{}\(\frac{x}{1+x^2}\)%
\item{}\(\arctan \left(x^3\right)\)%
\item{}\(\ln\left(2+x\right)\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{CalcIn17th18thCentury-AddProb-3-6-2}{}\quad{}\(2+x=2\left(1+\frac{x}{2}\right)\)%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{CalcIn17th18thCentury-AddProb-4}%
Let \(a\) be a positive real number.  Find a power series for \(a^x\) expanded about 0.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{CalcIn17th18thCentury-AddProb-4-3}{}\quad{}\(a^x=e^{\ln\,\left(a^x\right)}\)%
\end{problem}
\begin{problem}{Problem}{}{CalcIn17th18thCentury-AddProb-5}%
Represent the function \(\)sin \(x\) as a power series expanded about \(a\) (i.e., in the form \(\sum_{n=0}^\infty a_n\left(x-a\right)^n\)).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{CalcIn17th18thCentury-AddProb-5-4}{}\quad{}\(\sin x=\sin \left(a+x-a\right)\).%
\end{problem}
\begin{problem}{Problem}{}{CalcIn17th18thCentury-AddProb-6}%
\emph{Without} using Taylor's Theorem, represent the following functions as a power series expanded about \(a\) for the given value of \(a\) (i.e., in the form \(\sum_{n=0}^\infty a_n\left(x-a\right)^n\)).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\ln x\), \(a=1\)%
\item{}\(e^x\), \(a=3\)%
\item{}\(x^3+2x^2+3\) , \(a=1\)%
\item{}\(\frac{1}{x}\) , \(a=5\)%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{CalcIn17th18thCentury-AddProb-7}%
Evaluate the following integrals as series.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\displaystyle\int_{x=0}^1e^{x^2}\dx{ x}\)%
\item{}\(\displaystyle\int_{x=0}^1\frac{1}{1+x^4}\dx{ x}\)%
\item{}\(\displaystyle\int_{x=0}^1\sqrt[3]{1-x^3}\dx{ x}\)%
\end{enumerate}%
\end{problem}
\end{sectionptx}
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 4 Questions Concerning Power Series}
\typeout{************************************************}
%
\begin{chapterptx}{Chapter}{Questions Concerning Power Series}{}{Questions Concerning Power Series}{}{}{PowerSeriesQuestions}
\renewcommand*{\chaptername}{Chapter}
%
%
\typeout{************************************************}
\typeout{Section 4.1 Taylor's Formula}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Taylor's Formula}{}{Taylor's Formula}{}{}{PowerSeriesQuestions-TaylorsFormula}
As we saw in the previous chapter, representing functions as power series was a fruitful strategy for mathematicans in the eighteenth century (as it still is).  Differentiating and integrating power series term by term was relatively easy, \emph{seemed} to work, and led to many applications.  Furthermore, power series representations for all of the elementary functions could be obtained if one was clever enough.%
\par
However, cleverness is an unreliable tool. Is there some systematic way to find a power series for a given function? To be sure, there were nagging questions: If we can find a power series, how do we know that the series we've created represents the function we started with? Even worse, is it possible for a function to have more than one power series representation centered at a given value \(a?\) This uniqueness issue is addressed by the following theorem.%
\begin{theorem}{Theorem}{}{}{TaylorSeriesThm}%
\index{Taylor's Formula} If \(f(x)=\sum_{n=0}^\infty a_n(x-a)^n\), then \(a_n=\frac{f^{(n)}(a)}{n!}\), where \(f^{(n)}(a)\) represents the \(n^{th}\) derivative of \(f\) evaluated at \(a\).%
\end{theorem}
A few comments about \hyperref[TaylorSeriesThm]{Theorem~{\xreffont\ref{TaylorSeriesThm}}} are in order. Notice that we did \emph{not} start with a function and derive its series representation. Instead we \emph{defined} \(f(x)\) to be the series we wrote down. This assumes that the expression \(\sum_{n=0}^\infty a_n(x-a)^n\) actually has meaning (that it converges). At this point we have every reason to expect that it does, however expectation is not proof so we note that this is an assumption, not an established truth. Similarly, the idea that we can differentiate an infinite polynomial term-by-term as we would a finite polynomial is also assumed. As before, we follow in the footsteps of our 18th century forebears in making these assumptions. For now.%
\begin{problem}{Problem}{}{PowerSeriesQuestions-TaylorsFormula-6}%
\index{Taylor's Formula} Prove \hyperref[TaylorSeriesThm]{Theorem~{\xreffont\ref{TaylorSeriesThm}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesQuestions-TaylorsFormula-6-2}{}\quad{}\(f(a)=a_0+a_1(a-a)+a_2(a-a)^2+\cdots=a_0\). Differentiate to obtain the other terms.%
\end{problem}
From \hyperref[TaylorSeriesThm]{Theorem~{\xreffont\ref{TaylorSeriesThm}}} we see that if we do start with the function \(f(x)\) then no matter how we obtain its power series, the result will always be the same. The series%
\begin{align*}
\sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n=f(a)+\amp{}f^\prime(a)(x-a)+\frac{f^{\prime\prime}(a)}{2!}(x-a)^2\\
\amp{}+\frac{f^{\prime\prime\prime}(a)}{3!}(x-a)^3+\cdots
\end{align*}
%
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Taylor/}{Brook Taylor}\protect\footnotemark{}}{PowerSeriesQuestions-TaylorsFormula-8}{}%
\index{Taylor, Brook!portrait of}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Taylor.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[12]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Taylor/}\label{PowerSeriesQuestions-TaylorsFormula-8-1-2}}%
is called the \emph{\alert{Taylor series} for \(f\) expanded about (centered at) \(a\)}.  Although this systematic ``machine'' for obtaining power series for a function seems to have been known to a number of mathematicians in the early 1700s, Brook Taylor was the first to publish this result in his \textit{Methodus Incrementorum} (1715). The special case when \(a=0\) was included by Colin Maclaurin in his \emph{Treatise of Fluxions} (1742).  Thus when \(a=0\), the series \(\sum_{n=0}^\infty\frac{f^{(n)}(0)}{n!}x^n\) is often called the \emph{Maclaurin Series for \(f\)}.%
\par
The ``prime notation'' for the derivative was not used by Taylor, Maclaurin or their contemporaries. It was introduced by Joseph Louis Lagrange in his 1779 work \textit{Thorie des Fonctions Analytiques}. In that work, Lagrange sought to get rid of Leibniz' infinitesimals and base Calculus on the power series idea. His idea was that by representing every function as a power series, Calculus could be done ``algebraically'' by manipulating power series and examining various aspects of the series representation instead of appealing to the ``controversial'' notion of infinitesimals. He implicitly assumed that every continuous function could be replaced with its power series representation.%
\par
That is, he wanted to think of the Taylor series as a ``great big polynomial,'' because polynomials are easy to work with. It was a very simple, yet exceedingly clever and far-reaching idea. Since \(e^x = 1 +x +x^2/2 +\ldots\), for example, why not just define the exponential to be the series and work with the series. After all, the series is just a very long polynomial.%
\par
This idea did not come out of nowhere.  Leonhard Euler had put exactly that idea to work to solve many problems throughout the 18th century.  Some of his solutions are still quite breath-taking when you first see them~\hyperlink{sandifer07__early_mathem_leonar_euler}{[{\xreffont 14}]}.%
\par
Taking his cue from the Taylor series%
\begin{equation*}
f(x) = \sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n
\end{equation*}
%
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Lagrange/}{Joseph-Louis Lagrange}\protect\footnotemark{}}{PowerSeriesQuestions-TaylorsFormula-14}{}%
\index{Lagrange, Joseph-Louis!portrait of}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Lagrange.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[13]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Lagrange/}\label{PowerSeriesQuestions-TaylorsFormula-14-1-2}}%
Lagrange observed that the coefficient of \((x-a)^n\) provides the derivative of \(f\) at \(a\) (divided by \(n!\)). Modifying the formula above to suit his purpose, Lagrange supposed that every differentiable function could be represented as%
\begin{equation*}
f(x) = \sum_{n=0}^\infty g_n(a)(x-a)^n\text{.}
\end{equation*}
%
\par
If we regard the parameter \(a\) as a variable then \(g_1\) is the derivative of \(f\), \(g_2=2f^{\prime\prime}\) and generally%
\begin{equation*}
g_n=n!f^{(n)}\text{.}
\end{equation*}
%
\par
Lagrange dubbed his function \(g_1\) the \textit{``fonction drive''} from which we get the modern name ``derivative.''%
\par
All in all, this was a very clever and insightful idea whose only real flaw is that its fundamental assumption is not true. It turns out that not every differentiable function can be represented as a Taylor series. This was demonstrated very dramatically by Augustin Cauchy's famous counter-example%
\begin{equation}
f(x) = \begin{cases} e^{-\frac{1}{x^2}}\amp  x\ne0\\ 0 \amp x=0 \end{cases}\text{.}\label{eq_CauchyCounterEx}
\end{equation}
%
\par
This function is actually infinitely differentiable everywhere but its Maclaurin series (that is, a Taylor series with \(a=0\)) does not converge to \(f\) because all of its derivatives at the origin are equal to zero: \(f^{(n)}(0) = 0, \forall n\in\NN\).%
\par
Computing these derivatives using the definition you learned in Calculus is not conceptually difficult but the formulas involved do become complicated rather quickly. Some care must be taken to avoid error.%
\par
To begin with, let's compute a few derivatives when \(x \neq 0\).%
\begin{align*}
f^{(0)}(x) \amp = e^{x^{-2}}\\
f^{(1)}(x) \amp = 2x^{-3}e^{-x^{-2}}\\
f^{(2)}(x) \amp = \left(4x^{-6}-6x^{-4}\right)e^{-x^{-2}}\text{.}
\end{align*}
%
\par
As you can see the calculations are already getting a little complicated and we've only taken the second derivative. To streamline things a bit we take \(y= x^{-1}\), and define \(p_2(x) = 4x^6-6x^4\) so that%
\begin{equation*}
f^{(2)}(x) = p_2(x^{-1})e^{-x^{-2}} = p_2(y)e^{-y^2}\text{.}
\end{equation*}
%
\begin{problem}{Problem}{Cauchy's Counterexample, Part 1.}{PowerSeriesQuestions-TaylorsFormula-23}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Adopting the notation \(y=x^{-1}\) and \(f^{(n)}(x) =p_n(y)e^{-y^2}\), find \(p_{n+1}(y)\) in terms of \(p_{n}(y)\). [Note: Don't forget that you are differentiating with respect to \(x\), not \(y\).]%
\item{}Use induction on \(n\) to show that \(p_n(y)\) is a polynomial for all \(n\in\NN\).%
\end{enumerate}%
\end{problem}
Unfortunately everything we've done so far only gives us the derivatives we need when \(x\) is \emph{not} zero, and we need the derivatives when \(x\) \emph{is} zero. To find these we need to get back to very basic ideas.%
\par
Let's assume for the moment that we know that \(f^{(n)}(0)=0\) and recall that%
\begin{align*}
f^{(n+1)}(0) \amp = \limit{x}{0}{\frac{f^{(n)}(x)-f^{(n)}(0)}{x-0}}\\
f^{(n+1)}(0) \amp = \limit{x}{0}{x^{-1}p_n(x^{-1})e^{-x^{-2}}}\\
f^{(n+1)}(0) \amp = \limit{y}{\pm\infty}{\frac{yp_n(y)}{e^{y^2}}}\text{.}
\end{align*}
%
\par
We can close the deal with the following problem.%
\begin{problem}{Problem}{Cauchy's Counterexample, Part 2.}{PowerSeriesQuestions-TaylorsFormula-27}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Let \(m\) be a nonnegative integer. Show that \(\limit{y}{\pm\infty}{\frac{y^m}{e^{y^2}}}=0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesQuestions-TaylorsFormula-27-3-2}{}\quad{}Induction and a dash of L'Hpital's rule should do the trick.%
\item{}Prove that \(\limit{y}{\pm\infty}{\frac{q(y)}{e^{y^2}}}=0\) for any polynomial \(q\).%
\item{}Let \(f(x)\) be as in \hyperref[eq_CauchyCounterEx]{equation~({\xreffont\ref{eq_CauchyCounterEx}})} and show that for every nonnegative integer \(n\), \(f^{(n)}(0)=0\).%
\end{enumerate}%
\end{problem}
This example showed that while it was fruitful to exploit Taylor series representations of various functions, basing the foundations of Calculus on power series was not a sound idea.%
\par
While Lagrange's approach wasn't totally successful, it was a major step away from infinitesimals and toward the modern approach.  We still use aspects of it today.  For instance we still use his prime notation (\(f^\prime\)) to denote the derivative.%
\par
Turning Lagrange's idea on its head it is clear that if we know how to compute derivatives, we can use this machine to obtain a power series when we are not ``clever enough'' to obtain the series in other (typically shorter) ways. For example, consider Newton's binomial series when \(\alpha=\frac{1}{2}\). Originally, we obtained this series by extending the binomial theorem to non-integer exponents. Taylor's formula provides a more systematic way to obtain this series:%
\begin{align*}
f(x)\amp =(1+x)^{\frac{1}{2}};\amp f(0)\amp =1\\
f^\prime(x)\amp =\frac{1}{2}(1+x)^{\frac{1}{2}-1};\amp  f^\prime(0)\amp =\frac{1}{2}\\
f^{\prime\prime}(x)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)(1+x)^{\frac{1}{2}-2}\amp f^{\prime\prime}(0)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)
\end{align*}
and in general since%
\begin{align*}
f^{(n)}(x)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-(n-1)\right)(1+x)^{\frac{1}{2}-n}\\
\intertext{we have}
f^{(n)}(0)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-(n-1)\right)\text{.}
\end{align*}
%
\par
Using Taylor's formula we obtain the series%
\begin{align*}
\sum_{n=0}^\infty\frac{f^{(n)}(0)}{n!}x^n \amp{}= 1+\sum_{n=1}^\infty\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\cdots\left( \frac{1}{2}-(n-1)\right)}{n!}x^n\\
\amp{}= 1+\sum_{n=1}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n
\end{align*}
which agrees with \hyperref[eq_BinomialSeries]{equation~({\xreffont\ref{eq_BinomialSeries}})} in the previous chapter.%
\begin{problem}{Problem}{}{PowerSeriesQuestions-TaylorsFormula-32}%
\index{Taylor's Formula!use to obtain the general binomial series} Use Taylor's formula to obtain the general binomial series%
\begin{equation*}
(1+x)^\alpha=1+\sum_{n=1}^\infty\frac{\prod_{j=0}^{n-1}\left(\alpha-j\right)}{n!}x^n.{}
\end{equation*}
%
\end{problem}
\begin{problem}{Problem}{}{PowerSeriesQuestions-TaylorsFormula-33}%
\index{\(e^x\)!Taylor's series for}\index{series!Taylor's series!expansion of \(e^x, \sin x\), and \(\cos x\)}\index{\(\sin x\)!Taylor's series for}\index{\(\cos x\)!Taylor's series for} Use Taylor's formula to obtain the Taylor series for the functions \(e^x\), sin \(x\), and cos \(x\) expanded about \(a\).%
\end{problem}
As you can see, Taylor's ``machine'' will produce the power series for a function (if it has one), but is tedious to perform. We will find, generally, that this tediousness can be an obstacle to understanding. In many cases it will be better to be clever if we can. This is usually shorter. However, it is comforting to have Taylor's formula available as a last resort.%
\par
The existence of a Taylor series is addressed (to some degree) by the following.%
\begin{theorem}{Theorem}{}{}{TaylorsTheorem}%
\index{Taylor's Theorem} If \(f^\prime, f^{\prime\prime}, \ldots, f^{(n+1)}\) are all continuous on an interval containing \(a\) and \(x\), then%
\begin{align*}
f(x)=f(a)+\amp{}\frac{f^{\prime}(a)}{1!}(x-a)+\frac{f^{\prime \prime}(a)}{2!}(x-a)^2 +\\
\amp{}\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n + \frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{t}\text{.}
\end{align*}
The last term in this expression,%
\begin{equation*}
\frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{t}\text{,}
\end{equation*}
is called the \terminology{Integral Form of the Remainder} of the Taylor series.%
\end{theorem}
Before we address the proof, notice that the \(n\)-th degree polynomial%
\begin{equation*}
f(a)+\frac{f^{\,\prime}(a)}{1!}(x-a)+\frac{f^{\,\prime\prime}(a)}{2!}(x-a)^2+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n
\end{equation*}
resembles the Taylor series and, in fact, is called the \emph{\(n\)-th degree Taylor polynomial of \(f\) about \(a\).} \hyperref[TaylorsTheorem]{Theorem~{\xreffont\ref{TaylorsTheorem}}} says that a function can be written as the sum of this polynomial and a specific integral which we will analyze in the next chapter. We will get the proof started and leave the formal induction proof as an exercise.%
\par
Notice that the case when \(n=0\) is really a restatement of the Fundamental Theorem of Calculus. Specifically, the FTC says \(\int_{t=a}^xf^\prime(t)\dx{ t}=f(x)-f(a)\) which we can rewrite as%
\begin{equation*}
f(x)=f(a)+\frac{1}{0!}\int_{t=a}^xf^\prime(t)(x-t)^0\dx{ t}
\end{equation*}
to provide the anchor step for our induction.%
\par
To derive the case where \(n=1\), we use integration by parts. If we let%
\begin{align*}
u\amp =f^\prime(t)\amp  d v\amp =(x-t)^0d t\\
d u\amp =f^{\prime\prime}(t)d t\amp  v\amp =-\frac{1}{1}(x-t)^1
\end{align*}
we obtain%
\begin{align*}
f(x)\amp =f(a)+\frac{1}{0!}\left(-\frac{1}{1}f^\prime(t)(x-t)^1|_{t=a}^{^x}+\frac{1}{1} \int_{t=a}^xf^{\prime\prime}(t)(x-t)^1\dx{ t}\right)\\
\amp =f(a)+\frac{1}{0!}\left(-\frac{1}{1}f^\prime(x)(x-x)^1+ \frac{1}{1}f^\prime(a)(x-a)^1+\frac{1}{1}\int_{t=a}^xf^{\prime\prime}(t)(x-t)^1\dx{ t}\right)\\
\amp =f(a)+\frac{1}{1!}f^\prime(a)\left(x-a\right)^1 + \frac{1}{1!}\int_{t=a}^xf^{\prime\prime}(t)(x-t)^1\dx{ t}\text{.}
\end{align*}
%
\begin{problem}{Problem}{}{PowerSeriesQuestions-TaylorsFormula-40}%
\index{Taylor's Theorem} Provide a formal induction proof for \hyperref[TaylorsTheorem]{Theorem~{\xreffont\ref{TaylorsTheorem}}}.%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 4.2 Series Anomalies}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Series Anomalies}{}{Series Anomalies}{}{}{PowerSeriesQuestions-SeriesAnomalies}
Up to this point, we have been somewhat frivolous in our approach to series.  This approach mirrors eighteenth century mathematicians who ingeniously exploited Calculus and series to provide mathematical and physical results which were virtually unobtainable before.  Mathematicans were eager to push these techniques as far as they could to obtain their results and they often showed good intuition regarding what was mathematically acceptable and what was not.  However, as the envelope was pushed, questions about the validity of the methods surfaced.%
\par
As an illustration consider the series expansion%
\begin{equation*}
\frac{1}{1+x}=1-x+x^2-x^3+\cdots\text{.}
\end{equation*}
%
\par
If we substitute \(x=1\) into this equation, we obtain%
\begin{equation*}
\frac{1}{2}=1-1+1-1+\cdots\text{.}
\end{equation*}
%
\par
If we group the terms as follows \((1-1)+(1-1)+\cdots\), the series would equal \(0\). A regrouping of \(1+(-1+1)+(-1+1)+\cdots\) provides an answer of \(1\). This violation of the associative law of addition did not escape the mathematicians of the 1700s. In his 1760 paper \pubtitle{On Divergent Series} Euler said:%
\begin{quote}%
Notable enough, however are the controversies over the series \(1-1+1-1+etc\), \index{Leibniz, Gottfried Wilhelm} whose sum was given by Leibniz as \(\frac{1}{2}\), although others disagree .~.~. Understanding of this question is to be sought in the word ``sum;'' this idea, if thus conceived - namely, the sum of a series is said to be that quantity to which it is brought closer as more terms of a series are taken - has relevance only for the convergent series, and we should in general give up this idea of sum for divergent series. On the other hand, as series in analysis arise from the expansion of fractions or irrational quantities or even of transcendentals, it will, in turn, be permissible in calculation to substitute in place of such series that quantity out of whose development it is produced.%
\end{quote}
Even with this formal approach to series, an interesting question arises. The series for the antiderivative of \(\frac{1}{1+x}\) \emph{does} converge for \(x=1\) while this one does not. Specifically, taking the antiderivative of the above series, we obtain%
\begin{equation*}
\ln(1+x)=x-\frac{1}{2}x^2+\frac{1}{3}x^3-\cdots\text{.}
\end{equation*}
%
\par
If we substitute \(x=1\) into this series, we obtain \(\ln 2=1-\frac{1}{2}+\frac{1}{3}-\cdots\). It is not hard to see that such an alternating series converges. The following picture shows why. In this diagram, \(S_n\) denotes the partial sum \(1-\frac{1}{2}+\frac{1}{3}-\cdots+\frac{(-1)^{n+1}}{n}\).%
\begin{image}{0.05}{0.9}{0.05}{}%
\includegraphics[width=\linewidth]{external/images/AltHarmonic.png}
\end{image}%
From the diagram we can see \(S_2\leq S_4\leq S_6\leq\cdots\leq\cdots\leq S_5\leq S_3\leq S_1\) and \(S_{2k+1}-S_{2k}=\frac{1}{2k+1}\). It seems that the sequence of partial sums will converge to whatever is in the ``middle.'' Our diagram indicates that it is ln \(2\) in the middle but actually this is not obvious. Nonetheless it is interesting that one series converges for \(x=1\) but the other does not.%
\begin{problem}{Problem}{}{PowerSeriesQuestions-SeriesAnomalies-11}%
\index{series!Taylor's series!used to approximate \(\ln 2\)} Use the fact that%
\begin{equation*}
1-\frac{1}{2}+\frac{1}{3}-\cdots+\frac{(-1)^{2k+1}}{2k}\leq\ln 2\leq 1-\frac{1}{2}+\frac{1}{3}-\cdots+\frac{(-1)^{2k+2}}{2k+1}
\end{equation*}
to determine how many terms of the series \(\sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\) should be added together to approximate \(\ln 2\) to within \(.0001\) without actually computing what \(\ln 2\) is.%
\end{problem}
There is an even more perplexing situation brought about by these examples. An infinite sum such as \(1-1+1-1+\cdots\) appears to not satisfy the associative law for addition. While a convergent series such as \(1-\frac{1}{2}+\frac{1}{3}-\cdots\) does satisfy the associative law, it does not satisfy the commutative law. In fact, it does not satisfy it rather spectacularly.%
\par
A generalization of the following result was stated and proved by Bernhard Riemann \index{Riemann, Bernhard} in 1854.%
\begin{theorem}{Theorem}{}{}{thm_rearrangements}%
\index{series!Alternating Harmonic Series!rearrangements of}%
Let \(a\) be any real number. There exists a rearrangement of the series \(1-\frac{1}{2}+\frac{1}{3}-\cdots\) which converges to \(a\).%
\end{theorem}
This theorem shows that a series is most decidedly \emph{not} a great big sum. It follows that a power series is \emph{not} a great big polynomial.%
\par
To set the stage, consider the \emph{harmonic series} \index{series!Harmonic Series}%
\begin{equation*}
\sum_{n=1}^\infty\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\cdots\text{.}
\end{equation*}
%
\par
Even though the individual terms in this series converge to \(0\), the series still diverges (to infinity) as evidenced by the inequality%
\begin{align*}
\left(1+\frac{1}{2}\right)\amp +\left(\frac{1}{3}+\frac{1}{4}\right)+\left(\frac{1}{5}+\frac{1}{6}+ \frac{1}{7}+\frac{1}{8}\right)+\left(\frac{1}{9}+\cdots+\frac{1}{16}\right)+\cdots\\
\amp >\frac{1}{2}+\left(\frac{1}{4}+\frac{1}{4}\right)+\left(\frac{1}{8}+ \frac{1}{8}+\frac{1}{8}+\frac{1}{8}\right)+\left(\frac{1}{16}+\cdots+\frac{1}{16}\right)+\cdots\\
\amp =\frac{1}{2}+\frac{1}{2}+\frac{1}{2}+\frac{1}{2}+\cdots\\
\amp =   \infty\text{.}
\end{align*}
%
\par
Armed with this fact, we can see why \hyperref[thm_rearrangements]{Theorem~{\xreffont\ref{thm_rearrangements}}} is true. First note that%
\begin{equation*}
-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\cdots=-\frac{1}{2}(1+\frac{1}{2}+ \frac{1}{3}+\cdots)=-\infty
\end{equation*}
and%
\begin{equation*}
1+\frac{1}{3}+\frac{1}{5}+\cdots\geq\frac{1}{2}+\frac{1}{4}+\frac{1}{6}+\ldots= \infty\text{.}
\end{equation*}
%
\par
This says that if we add enough terms of \(-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\cdots\) we can make such a sum as small as we wish and if we add enough terms of \(1+\frac{1}{3}+\frac{1}{5}+\cdots\) we can make such a sum as large as we wish. This provides us with the general outline of the proof. The trick is to add just enough positive terms until the sum is just greater than \(a\). Then we start to add on negative terms until the sum is just less than \(a\). Picking up where we left off with the positive terms, we add on just enough positive terms until we are just above \(a\) again. We then add on negative terms until we are below \(a\). In essence, we are bouncing back and forth around \(a\). If we do this carefully, then we can get this rearrangement to converge to \(a\). The notation in the proof below gets a bit hairy, but keep this general idea in mind as you read through it.%
\par
Let \(O_1\) be the first odd integer such that \(1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}>a\). Now choose \(E_1\) to be the first even integer such that%
\begin{equation*}
-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\cdots-\frac{1}{E_1} \lt a-\left(1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}\right)\text{.}
\end{equation*}
%
\par
Thus%
\begin{equation*}
1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4} - \frac{1}{6}-\cdots-\frac{1}{E_1}\lt a\text{.}
\end{equation*}
%
\par
Notice that we still have \(\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots=\infty\). With this in mind, choose \(O_2\) to be the first odd integer with%
\begin{align*}
\frac{1}{O_1+2}\amp{}+\frac{1}{O_1+4}+\cdots\frac{1}{O_2}\\
\amp{}\gt          a-\left(1+\frac{1}{3}+
\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\cdots-
\frac{1}{E_1}\right).
\end{align*}
%
\par
Thus we have%
\begin{align*}
a\lt
1+\frac{1}{3}+\frac{1}{5}+\amp{}\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}-
\frac{1}{6}-\\
\amp{}\cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+
\frac{1}{O_2}.
\end{align*}
%
\par
Furthermore, since%
\begin{align*}
1+\frac{1}{3}+\frac{1}{5}+\amp{}\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}-
\frac{1}{6}-\\
\amp{}\cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\\
\amp{}\cdots+ \frac{1}{O_2-2}\lt a
\end{align*}
we have%
\begin{align*}
\amp{}\left|1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}- \frac{1}{6}-\cdots\right.\\
\amp \left.-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+ \frac{1}{O_2}-a\right|\lt \frac{1}{O_2}
\end{align*}
%
\par
In a similar fashion choose \(E_2\) to be the first even integer such that%
\begin{align*}
1+\frac{1}{3}+\frac{1}{5}+\cdots\amp +\frac{1}{O_1}-\frac{1}{2}- \frac{1}{4}-\frac{1}{6}-\\
\amp \cdots -\frac{1}{E_1}+ \frac{1}{O_1+2}+\frac{1}{O_1+4}+\\
\amp \cdots+\frac{1}{O_2}-\frac{1}{E_1+2}-\frac{1}{E_1+4}-\\
\amp \cdots-\frac{1}{E_2}\lt a\text{.}
\end{align*}
%
\par
Since%
\begin{align*}
1+\frac{1}{3}+\frac{1}{5} +\cdots\amp{}+\frac{1}{O_1}-\frac{1}{2}- \frac{1}{4}-\frac{1}{6}-\\
\cdots\amp{}-\frac{1}{E_1}+ \frac{1}{O_1+2}+\frac{1}{O_1+4}+\\
\cdots\amp{}+\frac{1}{O_2}- \frac{1}{E_1+2}-\frac{1}{E_1+4}-\cdots-\frac{1}{E_2-2}>a
\end{align*}
then%
\begin{align*}
\left|1+\frac{1}{3}\right.+\frac{1}{5}+
\amp \cdots+\frac{1}{O_1}-\frac{1}{2}-
\frac{1}{4}-\frac{1}{6}-\\
\amp{}\cdots-\frac{1}{E_1} +\frac{1}{O_1+2}+\frac{1}{O_1+4}+\\
\amp{}\cdots+\frac{1}{O_2}-
\frac{1}{E_1+2}-\frac{1}{E_1+4}-\\
\amp{}\cdots-\left.\frac{1}{E_2}-a\right|
\lt \frac{1}{E_2}.
\end{align*}
%
\par
Again choose \(O_3\) to be the first odd integer such that%
\begin{align*}
a\lt 1+\frac{1}{3} +\frac{1}{5}+
\amp{}\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}- \frac{1}{6}-\\
\amp{}\cdots
-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\\
\amp{}\cdots+\frac{1}{E_1+2}-\frac{1}{E_1+4}-\cdots -\frac{1}{E_2} +\\
\amp{} \cdots \frac{1}{O_2+2} + \frac{1}{O_2+4}\cdots+\frac{1}{O_3}
\end{align*}
and notice that%
\begin{align*}
\left|1+\frac{1}{3}\right.
+\frac{1}{5}+\amp{}\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\\
\amp
\cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\\
\amp \cdots+ \frac{1}{O_2}+
-\frac{1}{E_1+2}-\frac{1}{E_1+4}-\\
\amp{}\cdots-\frac{1}{E_2}+\frac{1}{O_2+2}+
\frac{1}{O_2+4}+\cdots+\left.\frac{1}{O_3}-a\right|\\
\amp \lt \frac{1}{O_3}\text{.}
\end{align*}
%
\par
Continue defining \(O_k\) and \(E_k\) in this fashion. Since \(\lim_{k\rightarrow\infty}\frac{1}{O_k}=\,\lim_{k\rightarrow\infty} \frac{1}{E_k}=0\), it is evident that the partial sums%
\begin{align*}
1+\frac{1}{3} +\frac{1}{5}+\amp{}\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\\
\amp \cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\\
\amp \cdots \frac{1}{O_2}+\cdots -\frac{1}{E_{k-2}+2}-\frac{1}{E_{k-2}+4}-\\
\amp \cdots-\frac{1}{E_{k-1}}+  \frac{1}{O_{k-1}+2}+\frac{1}{O_{k-1}+4}+\cdots+\frac{1}{O_k}
\end{align*}
and%
\begin{align*}
1+\frac{1}{3}\amp
+\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}-
\frac{1}{6}-\\
\amp \cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+
\cdots{} \frac{1}{O_2}+\\
\amp
\cdots-\frac{1}{E_{k-2}+2}-\frac{1}{E_{k-2}+4}-\cdots-\frac{1}{E_{k-1}}
\end{align*}
must converge to \(a\). Furthermore, it is evident that every partial sum of the rearrangement%
\begin{align*}
1+\frac{1}{3} +\frac{1}{5}+\amp\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}- \frac{1}{6}-\\
\amp{}-\cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+ \frac{1}{O_2}+\cdots
\end{align*}
is trapped between two such extreme partial sums. This forces the entire rearranged series to converge to \(a\).%
\par
The next two problems are similar to the above, but notationally are easier since we don't need to worry about converging to an actual number. We only need to make the rearrangement grow (or shrink in the case of \hyperref[prob_RearrangeDivToNegInf]{Problem~{\xreffont\ref{prob_RearrangeDivToNegInf}}}) without bound.%
\begin{problem}{Problem}{}{PowerSeriesQuestions-SeriesAnomalies-30}%
Show that there is a rearrangement of \(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots\) which diverges to \(\infty\).%
\end{problem}
\begin{problem}{Problem}{}{prob_RearrangeDivToNegInf}%
Show that there is a rearrangement of \(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots\) which diverges to \(-\infty\).%
\end{problem}
It is fun to know that we can rearrange some series to make them add up to anything you like but there is a more fundamental idea at play here. That the negative terms of the alternating Harmonic Series \emph{diverge} to negative infinity and the positive terms \emph{diverge} to positive infinity make the \emph{convergence} of the alternating series very special.%
\par
Consider, first we add \(1\). This is one of the positive terms so our sum is starting to increase without bound. Next we add \(-1/2\) which is one of the negative terms so our sum has turned around and is now starting to decrease without bound. Then another positive term is added: increasing without bound. Then another negative term: decreasing. And so on. The convergence of the alternating Harmonic Series is the result of a delicate balance between a tendency to run off to positive infinity and back to negative infinity. When viewed in this light it is not really too surprising that rearranging the terms can destroy this delicate balance.%
\par
Naturally, the alternating Harmonic Series is not the only such series. Any such series is said to converge ``conditionally'' \textemdash{} the condition being the specific arrangement of the terms.%
\par
To stir the pot a bit more, some series do satisfy the commutative property. More specifically, one can show that any rearrangement of the series \(1-\frac{1}{2^2}+\frac{1}{3^2}-\cdots\) must converge to the same value as the original series (which happens to be \(\int_{x=0}^1\frac{\text{ ln } (1+x)}{x}dx\approx.8224670334\)). Why does one series behave so nicely whereas the other does not?%
\par
Issues such as these and, more generally, the validity of using the infinitely small and infinitely large certainly existed in the 1700s, but they were overshadowed by the utility of the Calculus. Indeed, foundational questions raised by the above examples, while certainly interesting and of importance, did not significantly deter the exploitation of Calculus in studying physical phenomena. However, the envelope eventually was pushed to the point that not even the most practically oriented mathematician could avoid the foundational issues.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 4.3 Additional Problems}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Additional Problems}{}{Additional Problems}{}{}{PowerSeriesQuestions-AddProb}
\begin{problem}{Problem}{}{PowerSeriesQuestions-AddProb-2}%
Use Taylor's formula to find the Taylor series of the given function expanded about the given point \(a\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(f(x)=\ln\left(1+x\right)\), \(a=0\)%
\item{}\(f(x)=e^x\), \(a=-1\)%
\item{}\(f(x)=x^3+x^2+x+1\), \(a=0\)%
\item{}\(f(x)=x^3+x^2+x+1\), \(a=1\)%
\end{enumerate}%
\end{problem}
\end{sectionptx}
\end{chapterptx}
\end{partptx}
%
%
\typeout{************************************************}
\typeout{Part II Interregnum}
\typeout{************************************************}
%
\begin{partptx}{Part}{Interregnum}{}{Interregnum}{}{}{Interregnum}
\renewcommand*{\partname}{Part}
%
%
\typeout{************************************************}
\typeout{Chapter 5 Joseph Fourier: The Man Who Broke Calculus}
\typeout{************************************************}
%
\begin{chapterptx}{Chapter}{Joseph Fourier: The Man Who Broke Calculus}{}{Joseph Fourier: The Man Who Broke Calculus}{}{}{Interegnum}
\renewcommand*{\chaptername}{Chapter}
%
%
\typeout{************************************************}
\typeout{Section 5.1 Joseph Fourier and His Series}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Joseph Fourier and His Series}{}{Joseph Fourier and His Series}{}{}{Interegnum-2}
Applying mathematics to physical problems such as heat flow in a solid body drew much attention in the latter part of the 1700's and the early part of the 1800's. One of the people to attack the heat flow problem was%
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Fourier/}{Jean Baptiste Joseph Fourier}\protect\footnotemark{}}{Interegnum-2-3}{}%
\index{Fourier, Jean Baptiste Joseph!portrait of}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Fourier.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[14]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Fourier/}\label{Interegnum-2-3-1-2}}%
\index{Fourier, Jean Baptiste Joseph} Jean Baptiste Joseph Fourier.  Fourier submitted a manuscript on the subject, \textit{Sur la propagation de la chaleur} (\emph{On the Propagation of Heat}), to the \textit{Institut National des Sciences et des Arts} in 1807.  These ideas were subsequently published in \textit{La theorie analytique de la chaleur} (\emph{The Analytic Theory of Heat (1822)}).%
\par
To examine Fourier's ideas, consider the example of a thin wire of length one, which is perfectly insulated and whose endpoints are held at a fixed temperature of zero. Given an initial temperature distribution in the wire, the problem is to monitor the temperature of the wire at any point \(x\) and at any time \(t\). Specifically, if we let \(u(x,t)\) denote the temperature of the wire at point \(x\in[0,1]\) at time \(t\geq 0\), then it can be shown that \(u\) must satisfy the one-dimensional heat equation \(\rho^2\frac{\partial^2u}{\partial x^2}=\frac{\partial u}{\partial t}\), where \(\rho^2\) is a positive constant known as the thermal diffusivity. If the initial temperature distribution is given by the function \(f(x)\), then the \(u\) we are seeking must satisfy all of the following%
\begin{align}
\rho^2\frac{\partial^2u}{\partial x^2}\amp{}=\frac{\partial u}{\partial t},\amp{}\label{EQUATIONHeatEquation}\\
u(0,t)\amp{} =u(1,t)=0,  
\amp{}
\forall t\amp{}\geq 0,\notag\\
u(x,0)\amp{} =f(x), 
\amp{}
\forall x\amp{}\in[\,0,1].\notag
\end{align}
%
\par
To do this, Fourier employed what is now referred to as the method of separation of variables.  Specifically, Fourier looked for solutions of the form \(u(x,t)=X(x)T(t)\); that is, solutions where the \(x\)-part can be separated from the \(t\)-part.  Assuming that \(u\) has this form, we get \(\frac{\partial^2u}{\partial x^2}=X^{\prime\prime}T\) and \(\frac{\partial u}{\partial t}=X\,T^{\prime}\).  Substituting these into \hyperref[EQUATIONHeatEquation]{equation~({\xreffont\ref{EQUATIONHeatEquation}})} \(\rho^2\frac{\partial^2u}{\partial x^2}=\frac{\partial
u}{\partial t}\), we obtain%
\begin{align*}
\rho^2X^{\prime\prime}T=X
T^\prime\amp{}\amp{}\text{or}\amp{}\amp{}
\frac{X^{\prime\prime}}{X}=\frac{T^\prime}{\rho^2T}\text{.}
\end{align*}
%
\par
Since the left-hand side involves no \(t\)'s and the right-hand side involves no \(x\)'s, both sides must equal a constant \(k\). Thus we have%
\begin{align*}
X^{\prime\prime}=k X\amp{}\amp{}\text{and}\amp{}\amp{}
T^\prime=\rho^2k T.
\end{align*}
%
\begin{problem}{Problem}{}{prob_HarmonicMotion}%
\index{Heat Equation, the}\index{Heat Equation, the!parameter \(k\) must be less than zero} Show that \(T=Ce^{\rho^2kt}\) satisfies the equation \(T^\prime=\rho^2k T\), where \(C\), and \(\rho\) are arbitrary constants.  Use the physics of the problem to show that if \(u\) is not constantly zero, then \(k\lt 0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{prob_HarmonicMotion-2}{}\quad{}Consider \(\limit{t}{\infty}{u(x,t)}\).%
\end{problem}
Using the result from \hyperref[prob_HarmonicMotion]{Problem~{\xreffont\ref{prob_HarmonicMotion}}} that \(k\lt 0\), we will let \(k=-p^2\).%
\begin{problem}{Problem}{}{Interegnum-2-10}%
Show that \(X=A\sin\left(px\right)+B\cos\left(px\right)\) satisfies the equation \(X^{\prime\prime}=-p^2X\), where \(A\) and \(B\) are arbitrary constants.  Use the boundary conditions \(u(0,t)=u(1,t)=0\), \(\forall\) \(t\geq 0\) to show that \(B=0\) and \(A\sin p=0\).  Conclude that if \(u\) is not constantly zero, then \(p=n\pi\), where \(n\) is any integer.%
\end{problem}
\begin{problem}{Problem}{}{Interegnum-2-11}%
Show that if \(u_1\) and \(u_2\) satisfy the equations%
\begin{equation*}
\rho^2\frac{\partial^2u}{\partial x^2}=\frac{\partial
u}{\partial t}
\end{equation*}
and%
\begin{align*}
u(0,t)=u(1,t)=0,\amp{}\amp{} \forall t\geq   0
\end{align*}
then \(u=A_1u_1+A_2u_2\) satisfy them as well, where \(A_1\) and \(A_2\) are arbitrary constants.%
\end{problem}
\index{Fourier, Jean Baptiste Joseph} Putting all of these results together, Fourier surmised that the general solution to%
\begin{align*}
\rho^2\frac{\partial^2u}{\partial x^2}=\frac{\partial u}{\partial t} u(0,t)=u(1,t)=0,\amp{}\amp{}\forall t\geq 0
\end{align*}
could be expressed as the series%
\begin{equation*}
u(x,t)=\sum_{n=1}^\infty A_ne^{-(\rho n\pi)^2t}\sin\left(n\pi x\right)\text{.}
\end{equation*}
%
\par
All that is left is to have \(u\) satisfy the initial condition \(u(x,0)=f(x)\), \(\forall\,x\in[\,0,1]\). That is, we need to find coefficients \(A_n\), such that%
\begin{equation*}
f(x)=u(x,0)=\sum_{n=1}^\infty A_n\sin\left(n\pi x\right)\text{.}
\end{equation*}
%
\par
The idea of representing a function as a series of sine waves was proposed by Daniel Bernoulli in 1753 while examining the problem of modeling a vibrating string. Unfortunately for Bernoulli, he didn't know how to compute the coefficients in such a series representation. What distinguished Fourier was that he developed a technique to compute these coefficients. The key is the result of the following problem.%
\begin{problem}{Problem}{}{prob_SinOrthogonality}%
\index{\(\sin x\)!orthogonality of}\index{orthogonality!of \(\sin nx\)} Let \(n\) and \(m\) be positive integers. Show%
\begin{equation*}
\int_{x=0}^1\sin\left(n\pi x\right)\sin\left(m\pi x\right)\dx{ x}= \left\{\begin{matrix}0\amp \text{ if } n\neq m\\ \frac{1}{2}\amp \text{ if } n=m \end{matrix} \right.. {}
\end{equation*}
%
\end{problem}
Armed with the result from \hyperref[prob_SinOrthogonality]{Problem~{\xreffont\ref{prob_SinOrthogonality}}}, \index{Fourier, Jean Baptiste Joseph} Fourier could compute the coefficients \(A_n\) in the series representation \(f(x)=\sum_{n=1}^\infty A_n \sin\left(n\pi x\right)\) in the following manner. Since we are trying to find \(A_n\) for a particular (albeit general) \(n\), we will temporarily change the index in the summation from \(n\) to \(j\). With this in mind, consider%
\begin{align*}
\int_{x=0}^1f(x)\sin\left(n\pi x\right)\dx{ x} \amp =\int_{x=0}^1\left(\sum_{j=1}^\infty A_j\text{ sin } \left(j\pi x\right)\right)\sin\left(n\pi x\right)\dx{ x}\\
\amp =\sum_{j=1}^\infty A_j\int_{x=0}^1\sin\left(j\pi x\right)\sin\left(n\pi x\right)\dx{ x}\\
\amp =\frac{A_n}{2}
\end{align*}
which leads to the formula \(A_n=2\int_{x=0}^1f(x)\sin\left(n\pi x\right)d x\).%
\par
The series \(f(x)=\sum_{n=1}^\infty A_n\sin\left(n\pi
x\right)\) with%
\begin{equation*}
A_n=2\int_{x=0}^1f(x)\sin\left(n\pi
x\right)\dx{ x}
\end{equation*}
is called the \emph{Fourier (sine) series of \(\boldsymbol{f}\)}.%
\begin{example}{Example}{}{Interegnum-2-18}%
Let's apply this to the function, \(f(x)=\frac{1}{2}-\abs{x-\frac{1}{2}}\), whose graph of this function is seen below.%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/FourierEx1.png}
\end{image}%
\end{example}
\begin{problem}{Problem}{}{Interegnum-2-19}%
\index{Fourier Series!sine series of an odd function} Let \(n\) be a positive integer. Show that if%
\begin{equation*}
f(x)=\frac{1}{2}-\abs{x-\frac{1}{2}}
\end{equation*}
then%
\begin{equation*}
\int_{x=0}^1f(x)\sin\left(n\pi x\right)d x = \frac{2}{\left(n\pi\right)^2}\sin\left(\frac{n\pi}{2}\right)
\end{equation*}
and show that the Fourier sine series of \(f\) is given by%
\begin{align*}
f(x)\amp{}=\sum_{n=1}^\infty\frac{4}{\left(n\pi\right)^2}\sin\left(\frac{n\pi}{2}
\right)\sin\left(n\pi x\right)\\
\amp{}=\frac{4}{\pi^2}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)^2}\sin\left(\left(2k+1\right)\pi x\right).{}
\end{align*}
%
\end{problem}
To check if this series really does, in fact, represent \(f\) on \([0,1]\), let%
\begin{equation*}
S_N(x)=\frac{4}{\pi^2}\sum_{k=0}^N\frac{\left(-1\right)^k}{\left(2k+1\right)^2} \sin\left(\left(2k+1\right)\pi x\right)
\end{equation*}
be the \(N^{th}\) partial sum of the series and use the graphing tool below to view the graph of \(S_N(x)\) for several values of \(N\).%
\par
That is, \(S_N\) denotes the \(N^{th}\) partial sum of the series. We will graph \(S_N\) for \(N=1,\,2,\,5,\,50\).%
\par
                           %
\begin{image}{0}{1}{0}{}%
\includegraphics[width=\linewidth]{external/images/FourierEx4.png}
\end{image}%
As you can see, it appears that as we add more terms to the partial sum, \(S_N\), it looks more and more like the original function \(f(x)=\frac{1}{2}-\abs{x-\frac{1}{2}}\). This would lead us to believe that the series converges to the function and that%
\begin{equation}
f(x)=\frac{4}{\pi^2}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)^2}\sin\left(\left(2k+1\right)\pi x\right)\text{.}\label{PDE_sol}
\end{equation}
is a valid representation of \(f\) as a Fourier series.%
\par
Recall, that when we represented a function as a power series, we freely differentiated and integrated the series term by term as though it was a polynomial. Let's do the same with this Fourier series.%
\par
To start, notice that the derivative of%
\begin{equation*}
f(x)=\frac{1}{2}-\abs{x-\frac{1}{2}}
\end{equation*}
is given by%
\begin{equation*}
f^\prime(x) = \begin{cases}1\amp \text{ if } \,\text{ } 0\leq x\lt \frac{1}{2}\\ -1\amp \text{ if } \,\frac{1}{2}\lt x\leq 1 \end{cases} \text{.}
\end{equation*}
%
\par
This derivative does not exist at \(x=\frac{1}{2}\) and its graph is given by%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/Ch2fig1.png}
\end{image}%
If we differentiate the Fourier series term-by-term, we obtain%
\begin{equation*}
\frac{4}{\pi}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)} \cos\left(\left(2k+1\right)\pi x\right)\text{.}
\end{equation*}
%
\par
Again, if we let \(C_N(x)=\frac{4}{\pi}\sum_{k=0}^N\frac{\left(-1\right)^k}{\left(2k+1\right)} \cos\left(\left(2k+1\right)\pi x\right)\) be the \(N^{th}\) partial sum of this Fourier cosine series and plot \(C_N(x)\) for \(N=5\), we obtain%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/FourierEx8.png}
\end{image}%
In fact, if we were to graph the series \(\frac{4}{\pi}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)}\) cos\(\left(\left(2k+1\right)\pi x\right)\), we would obtain%
\begin{sidebyside}{1}{0.05}{0.05}{0}%
\begin{sbspanel}{0.9}[center]%
\includegraphics[width=\linewidth]{external/images/FourierEx10.png}
\end{sbspanel}%
\end{sidebyside}%
\par
Notice that this agrees with the graph of \(f^\prime\), except that \(f^\prime\) didn't exist at \(x=\frac{1}{2}\), and this series takes on the value \(0\) at \(x=\frac{1}{2}\). Notice also, that every partial sum of this series is continuous, since it is a finite combination of continuous cosine functions. This agrees with what you learned in Calculus, the (finite) sum of continuous functions is always continuous.  In the 1700s it was assumed (falsely) that this could be extended to infinite series, because every time a power series converged to a function, that function happened to be continuous.  This never failed for power series, so this example was a bit disconcerting as it is an example of the sum of \emph{infinitely many} continuous functions which is, in this case, discontinuous.  Was it possible that there was some power series which converged to a function which was not continuous?  Even if there wasn't, what was the difference between power series and this Fourier series?%
\par
Even more disconcerting is what happens if we try differentiating the series%
\begin{equation*}
\frac{4}{\pi}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)} \cos\left(\left(2k+1\right)\pi x\right)
\end{equation*}
term-by-term. Given the above graph of this series, the derivative of it should be constantly 0, except at \(x=\frac{1}{2}\), where the derivative wouldn't exist. Using the old adage that the derivative of a sum is the sum of the derivatives, we differentiate this series term-by-term to obtain the series%
\begin{equation*}
4\sum_{k=0}^\infty\left(-1\right)^{k+1}\sin\left(\left(2k+1\right)\pi x\right)\text{.}
\end{equation*}
%
\par
If we sum the first forty terms of this series, we get         %
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/FourierEx11.png}
\end{image}%
We knew that there might be a problem at \(x=\frac{1}{2}\) but this is crazy! The series seems to not be converging to zero at all!%
\begin{problem}{Problem}{}{prob_FourierDiverge}%
\index{Fourier Series}\index{Fourier Series!divergent Fourier series example} Show that when \(x=\frac{1}{4}\)%
\begin{align*}
4\sum_{k=0}^\infty\left(-1\right)^{k+1}\amp{} \sin\left(\left(2k+1\right)\pi x\right)\\
\amp{}=4\left(-\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{2}}- \frac{1}{\sqrt{2}}-\frac{1}{\sqrt{2}}+\cdots\right).
\end{align*}
%
\end{problem}
\hyperref[prob_FourierDiverge]{Problem~{\xreffont\ref{prob_FourierDiverge}}} shows that when we differentiate the series%
\begin{equation*}
\frac{4}{\pi}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)} \cos\left(\left(2k+1\right)\pi x\right)
\end{equation*}
term by term, this differentiated series doesn't converge to anything at \(x=\frac{1}{4}\), let alone converge to zero. In this case, the old Calculus rule that the derivative of a sum is the sum of the derivatives does not apply for this infinite sum, though it did apply before. As if the continuity issue wasn't bad enough before, this was even worse. Power series were routinely differentiated and integrated term-by-term. This was part of their appeal. They were treated like ``infinite polynomials.'' Either there is some power series lurking that refuses to behave nicely, or there is some property that power series have that not all Fourier series have.%
\par
Could it be that everything we did in \hyperref[PowerSeriesQuestions]{Chapter~{\xreffont\ref{PowerSeriesQuestions}}} was bogus?%
\par
Fortunately, the answer to that question is no.  Power series are generally much more well\textendash{}behaved than Fourier series. Whenever a power series converges, the function it converges to will be continuous.  And, as long as one stays inside the interval of convergence, power series can be differentiated and integrated term\textendash{}by\textendash{}term.  Power series have something going for them that your average Fourier series does not, but none of this is any more obvious to us than it was to mathematicians at the beginning of the nineteenth century.  What they (and we) did know was that relying on intuition was perilous and that rigorous formulations were needed to either justify or dismiss these intuitions.  In some sense, the nineteenth century was the ``morning after'' the mathematical party that went on throughout the eighteenth century.%
\begin{problem}{Problem}{}{prob_Fourier_Series-orthogonality}%
Let \(n\) and \(m\) be positive integers. Show%
\begin{equation*}
\int_{x=0}^1\cos\left(n\pi x\right)\cos\left(m\pi x\right)\dx{ x}=\left\{ \begin{matrix}0\amp \text{ if } n\neq m\\ \frac{1}{2}\amp \text{ if } n=m \end{matrix} \right.\text{.}
\end{equation*}
%
\end{problem}
\begin{problem}{Problem}{}{prob_fouriercoef}%
\index{Fourier Series!computing the coefficients} Use the result of \hyperref[prob_Fourier_Series-orthogonality]{Problem~{\xreffont\ref{prob_Fourier_Series-orthogonality}}} to show that if%
\begin{equation*}
f(x)=\sum_{n=1}^\infty B_n\cos\left(n\pi x\right)
\end{equation*}
on \([0,1]\), then%
\begin{equation*}
B_m=2\int_{x=0}^1f(x)\cos\left(m\pi x\right)\dx{ x}.{}
\end{equation*}
%
\end{problem}
\begin{problem}{Problem}{}{Interegnum-2-45}%
Apply the result of \hyperref[prob_fouriercoef]{Problem~{\xreffont\ref{prob_fouriercoef}}} to show that the Fourier cosine series of \(f(x)=x-\frac{1}{2}\) on \([0,1]\) is given by%
\begin{equation*}
\frac{-4}{\pi^2}\sum_{k=0}^\infty\frac{1}{\left(2k+1\right)^2}\cos \left((2k+1)\pi x\right)\text{.}
\end{equation*}
%
\par
Let%
\begin{equation*}
C(x,N)=\frac{-4}{\pi^2}\sum_{k=0}^N\frac{1}{\left(2k+1\right)^2}\cos
\left((2k+1)\pi x\right)
\end{equation*}
and plot \(C(x,N)\) for \(N=1,2,5,50\) \(x\in[\,0,1]\). How does this compare to the function \(f(x)=x-\frac{1}{2}\) on \([\,0,1]\)? What if you plot it for \(x\in[\,0,2]?\)%
\end{problem}
\begin{problem}{Problem}{}{Interegnum-2-46}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Differentiate the series%
\begin{equation*}
\frac{-4}{\pi^2}\sum_{k=0}^\infty\frac{1}{\left(2k+1\right)^2}\cos \left((2k+1)\pi x\right)
\end{equation*}
term by term and plot various partial sums for that series on \([\,0,1]\).  How does this compare to the derivative of \(f(x)=x-\frac{1}{2}\) on that interval?%
\item{}Differentiate the series you obtained in part (a) and plot various partial sums of that on \([\,0,1]\).  How does this compare to the second derivative of \(f(x)=x-\frac{1}{2}\) on that interval?%
\end{enumerate}%
\end{problem}
\end{sectionptx}
\end{chapterptx}
\end{partptx}
%
%
\typeout{************************************************}
\typeout{Part III In Which We Find (Some) Answers}
\typeout{************************************************}
%
\begin{partptx}{Part}{In Which We Find (Some) Answers}{}{In Which We Find (Some) Answers}{}{}{FindingAnswers}
\renewcommand*{\partname}{Part}
%
%
\typeout{************************************************}
\typeout{Chapter 6 Convergence of Sequences and Series}
\typeout{************************************************}
%
\begin{chapterptx}{Chapter}{Convergence of Sequences and Series}{}{Convergence of Sequences and Series}{}{}{Convergence}
\renewcommand*{\chaptername}{Chapter}
%
%
\typeout{************************************************}
\typeout{Section 6.1 Sequences of Real Numbers}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Sequences of Real Numbers}{}{Sequences of Real Numbers}{}{}{SeqRealNum}
In \hyperref[CalcIn17th18thCentury]{Chapter~{\xreffont\ref{CalcIn17th18thCentury}}}, we developed the equation \(1+x+x^2+x^3+\cdots=\frac{1}{1-x}\), and we mentioned that there are  limitations to this power series representation.  For example, substituting \(x=1\) and \(x=-1\) into this expression leads to%
\begin{align*}
1+1+1+\cdots=\frac{1}{0}\amp{}\amp{} \text{and}\amp{}\amp{}  1-1+1-1+\cdots=\frac{1}{2}
\end{align*}
which are rather hard to accept.  On the other hand, if we substitute \(x=\frac{1}{2}\) into the expression we get%
\begin{equation*}
1+\frac{1}{2}+\left(\frac{1}{2}\right)^2+\left(\frac{1}{2}\right)^3+\cdots=2
\end{equation*}
which seems more palatable. Until we think about it.%
\par
We can add two numbers together by the method we all learned in elementary school.  Or three numbers.  Or any finite set of numbers. At least in principle.%
\par
But infinitely many?  What does that even mean?  Before we can add infinitely many numbers together we must find a way to give meaning to the idea of adding infinitely many numbers.%
\par
To do this, we examine an infinite sum by thinking of it as a sequence of finite partial sums. In our example, we would have the following sequence of partial sums.%
\begin{equation*}
\left(1,1+\frac{1}{2},1+\frac{1}{2}+\left(\frac{1}{2}\right)^2,1+ \frac{1}{2}+\left(\frac{1}{2}\right)^3,\ldots,\sum_{j=0}^n\left(\frac{1}{2} \right)^j,\ldots\right)\text{.}
\end{equation*}
%
\par
We can plot these sums on a number line to see what they tend toward as \(n\) gets large.%
\begin{image}{0.08}{0.84}{0.08}{}%
\includegraphics[width=\linewidth]{external/images/NumberLine.png}
\end{image}%
Since each partial sum is located at the midpoint between the previous partial sum and \(2\), it is reasonable to suppose that these sums tend to the number 2. Indeed, you probably have seen an expression such as%
\begin{equation*}
\limit{n}{\infty}{\left(\sum_{j=0}^n\left(\frac{1}{2}\right)^j\right)}=2
\end{equation*}
justified by a similar argument. Of course, the reliance on such pictures and words is fine if we are satisfied with intuition. However, to establish rigor we cannot rely on pictures or such nebulous words as ``approaches.''%
\par
No doubt you are wondering ``What's wrong with the word `approaches'?  It seems clear enough to me.'' This is often a sticking point.  But if we think carefully about what we mean by the word ``approach'' we see that there is an implicit assumption that will cause us some difficulties later if we don't expose it.%
\par
To see this consider the sequence \(\left(1,\frac12,\frac13,\frac14,\ldots\right)\).  Clearly it ``approaches'' zero, right?  But, doesn't it also ``approach'' \(-1?\) It does, in the sense that each term gets closer to \(-1\) than the one previous.  But it also ``approaches'' \(-2\), \(-3\), or even \(-1000\) in the same sense.  That's the problem with the word ``approaches.'' It just says that we're getting closer to something than we were in the previous step.  It does \emph{not} tell us that we are actually getting close.  Since the moon moves in an elliptical orbit about the earth for part of each month it is ``approaching'' the earth.  The moon gets clos\emph{er} to the earth but, thankfully, it does not get \emph{close} to the earth.%
\par
The implicit assumption we alluded to earlier is this: When we say that the sequence \(\left(\frac1n\right)_{n=1}^\infty\) ``approaches'' zero we mean that it is getting \emph{close} not clos\emph{er}. Ordinarily this kind of vagueness in our language is pretty innocuous.  When we say ``approaches'' in casual conversation we can usually tell from the context of the conversation whether we mean ``getting close to'' or ``getting closer to.'' But when speaking mathematically we need to be more careful, more explicit, in the language we use.%
\par
So how can we change the language we use so that this ambiguity is eliminated?  We start by recognizing, rigorously, what we mean when we say that a sequence converges to zero.  For example, you would probably want to say that the sequence \(\left(1,\frac{1}{2},\frac{1}{3},\frac{1}{4},\,\ldots\right)=\left(
\frac{1}{n}\right)_{n=1}^\infty\) converges to zero.  Is there a way to give this meaning without relying on pictures or intuition?%
\par
One way would be to say that we can make \(\frac{1}{n}\) as close to zero as we wish, provided we make \(n\) large enough. But even this needs to be made more specific. For example, we can get \(\frac{1}{n}\) to within a distance of \(0.1\) of \(0\) provided we make \(n>10\), we can get \(\frac{1}{n}\) to within a distance of \(0.01\) of \(0\) provided we make \(n>100\), etc. After a few such examples it is apparent that given any arbitrary distance \(\eps>0\), we can get \(\frac{1}{n}\) to within \(\eps\) of \(0\) provided we make \(n>\frac{1}{\eps}\). This leads to the following definition.%
\begin{definition}{Definition}{}{def_ConvergeToZero}%
\index{sequences!convergence to zero} Let \(\left(s_n\right)=\left(s_1,s_2,s_3,\ldots\right)\) be a sequence of real numbers. We say that \(\left(\boldsymbol{s}_{\boldsymbol{n}}\right)\) \terminology{converges to 0} and write%
\begin{equation*}
\limit{n}{\infty}{s_n}=0
\end{equation*}
provided for any \(\eps>0\), there is a real number \(N\) such that if \(n>N\), then \(|s_n|\lt \eps\).%
\end{definition}
\terminology{Notes on \hyperref[def_ConvergeToZero]{Definition~{\xreffont\ref{def_ConvergeToZero}}}}:%
\begin{enumerate}
\item{}This definition is the formal version of the idea we just talked about. Given an arbitrary distance \(\eps\), we must be able to find a specific number \(N\) such that \(s_n\) is within \(\eps\) of \(0\), whenever \(n>N\).  The \(N\) is the answer to the question, how large is ``large enough'' to put \(s_n\) this close to \(0\).%
\item{}Even though we didn't need it in the example \(\left(\frac{1}{n}\right)\), the absolute value appears in the definition because we need to make the \emph{distance} from \(s_n\) to 0 smaller than \(\eps\). Without the absolute value in the definition, we would be able to ``prove'' such outrageous statements as \(\limitt{n}{\infty}{-n}=0\), which we obviously don't want.%
\item{}The statement \(|s_n|\lt \eps\) can also be written as \(-\eps\lt s_n\lt \eps\) or \(s_n\in\left(-\eps,\eps\right)\). (See the \hyperref[prob_absolute_value]{Problem~{\xreffont\ref{prob_absolute_value}}} below.) Any one of these equivalent formulations can be used to prove convergence. Depending on the application, one of these may be more advantageous to use than the others.%
\item{}Any time an \(N\) can be found that works for a particular \(\eps\), any number \(M>N\) will work for that \(\eps\) as well, since if \(n>M\) then \(n>N\).%
\end{enumerate}
%
\begin{problem}{Problem}{}{prob_absolute_value}%
\index{absolute value} Let \(a\) and \(b\) be real numbers with \(b>0\). Prove \(|a|\lt b\) if and only if \(-b\lt a\lt b\).%
\par
Notice that this can be extended to \(\abs{a}\leq b\) if and only if \(-b\leq a\leq b\).%
\end{problem}
To illustrate how this definition makes the above ideas rigorous, let's use it to prove that \(\limit{n}{\infty}{\textstyle\frac{1}{n}}=0\).%
\begin{proof}{Proof}{}{SeqRealNum-18}
Let \(\eps>0\) be given. Let \(N=\frac{1}{\eps}\). If \(n>N\), then \(n>\frac{1}{\eps}\) and so \(\abs{\frac{1}{n}}=\frac{1}{n}\lt \eps\). Hence by definition, \(\limitt{n}{\infty}{\frac{1}{n}}=0\).%
\end{proof}
Notice that this proof is rigorous and makes no reference to vague notions such as ``getting smaller'' or ``approaching infinity.'' It has three components:%
\begin{enumerate}
\item{}provide the challenge of a distance \(\eps>0\),%
\item{}identify a real number \(N\), and%
\item{}show that this \(N\) works for this given \(\eps\).%
\end{enumerate}
There is also no explanation about where \(N\) came from. While it is true that this choice of \(N\) is not surprising in light of the ``scrapwork'' we did before the definition, the motivation for how we got it is not in the formal proof nor is it required.  In fact, such scrapwork is typically not included in a formal proof.  For example, consider the following.%
\begin{example}{Example}{}{SeqRealNum-20}%
Use the definition of convergence to zero to prove%
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{\sin n}{n}=0\text{.}
\end{equation*}
%
\end{example}
\begin{proof}{Proof}{}{SeqRealNum-21}
Let \(\eps>0\). Let \(N=\frac{1}{\eps}\). If \(n>N\), then \(n>\frac{1}{\eps}\) and \(\frac{1}{n}\lt \eps\). Thus \(\abs{\frac{\sin(n)}{n}}\leq\frac{1}{n}\lt \eps\). Hence by definition, \(\limitt{n}{\infty}{\frac{\sin n}{n}}=0\).%
\end{proof}
Notice that the \(N\) came out of nowhere, but you can probably see the thought process that went into this choice: We needed to use the inequality \(\abs{\sin n}\leq 1\). Again this scrapwork is not part of the formal proof, but it is typically necessary for finding what \(N\) should be. You might be able to do the next problem without doing any scrapwork first, but don't hesitate to do scrapwork if you need it.%
\begin{problem}{Problem}{}{SeqRealNum-23}%
\index{convergence!of a sequence!convergence to zero drill} Use the definition of convergence to zero to prove the following.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\limit{n}{\infty}{\frac{1}{n^2}}=0\)%
\item{}\(\limit{n}{\infty}{\frac{1}{\sqrt{n}}}=0\)%
\end{enumerate}%
\end{problem}
As the sequences get more complicated, doing scrapwork ahead of time will become more necessary.%
\begin{example}{Example}{}{sec_defin-conv-sequ}%
Use the definition of convergence to zero to prove%
\begin{equation*}
\limit{n}{\infty}{\frac{n+4}{n^2+1}}=0\text{.}
\end{equation*}
%
\par
\terminology{SCRAPWORK}%
\par
Given an \(\eps>0\), we need to see how large to make \(n\) in order to guarantee that \(|\frac{n+4}{n^2+1}|\lt \eps\). First notice that \(\frac{n+4}{n^2+1}\lt \frac{n+4}{n^2}\). Also, notice that if \(n>4\), then \(n+4\lt n+n=2n\). So as long as \(n>4\), we have \(\frac{n+4}{n^2+1}\lt \frac{n+4}{n^2}\lt \frac{2n}{n^2}=\frac{2}{n}\). We can make this less than \(\eps\) if we make \(n>\frac{2}{\eps}\). This means we need to make \(n>4\) and \(n>\frac{2}{\eps}\), simultaneously. These can be done if we let \(N\) be the maximum of these two numbers. This sort of thing comes up regularly, so the notation \(N=\max\left(4,\frac{2}{\eps}\right)\) was developed to mean the maximum of these two numbers. Notice that if \(N=\max\left(4, \frac{2}{\eps}\right)\) then \(N\geq 4\) and \(N\geq\frac{2}{\eps}\). We're now ready for the formal proof.%
\end{example}
\begin{proof}{Proof}{}{SeqRealNum-26}
Let \(\eps>0\). Let \(N=\max\left(4,\frac{2}{\eps}\right)\). If \(n>N\), then \(n>4\) and \(n>\frac{2}{\eps}\). Thus we have \(n>4\) and \(\frac{2}{n}\lt \eps\). Therefore%
\begin{equation*}
\abs{\frac{n+4}{n^2+1}}=\frac{n+4}{n^2+1}\lt \frac{n+4}{n^2}\lt \frac{2n}{n^2}= \frac{2}{n}\lt \eps\text{.}
\end{equation*}
%
\par
Hence by definition, \(\displaystyle\lim_{n\rightarrow\infty}\frac{n+4}{n^2+1}=0\).%
\end{proof}
Again we emphasize that the scrapwork is not \alert{explicitly} a part of the formal proof.  However, if you look carefully, you can always find the scrapwork in the formal proof.%
\begin{problem}{Problem}{}{SeqRealNum-28}%
\index{convergence!of a sequence!convergence to zero drill} Use the definition of convergence to zero to prove%
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{n^2+4n+1}{n^3}=0.{}
\end{equation*}
%
\end{problem}
\begin{problem}{Problem}{}{prob_sequences3}%
Let \(b\) be a nonzero real number with \(|b|\lt 1\) and let \(\eps>0\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Solve the inequality \(|b|^n\lt \eps\) for \(n\)%
\item{}Use part (a) to prove \(\lim_{n\rightarrow\infty}b^n=0\).%
\end{enumerate}%
\end{problem}
We can negate this definition to prove that a particular sequence does not converge to zero.%
\begin{example}{Example}{}{ex_zero-two-not-converge}%
Use the definition to prove that the sequence%
\begin{equation*}
\left(1+(-1)^n\right)_{n=0}^\infty=(2,0,2,0,2,\ldots)
\end{equation*}
does not converge to zero.%
\end{example}
Before we provide this proof, let's analyze what it means for a sequence \(\left(s_n\right)\) to \emph{not} converge to zero.  Converging to zero means that any time a distance \(\eps>0\) is given, we must be able to respond with a number \(N\) such that \(|s_n|\lt \eps\) for every \(n>N\).  To have this not happen, we must be able to find some \(\eps>0\) such that no choice of \(N\) will work.  Of course, if we find such an \(\eps\), then any smaller one will fail to have such an \(N\), but we only need one to mess us up.  If you stare at the example long enough, you see that any \(\eps\) with \(0\lt
\eps\leq 2\) will cause problems.  For our purposes, we will let \(\eps=2\).%
\begin{proof}{Proof}{}{SeqRealNum-33}
Let \(\eps=2\) and let \(N\in\NN\) be any integer. If we let \(k\) be any non-negative integer with \(k>\frac{N}{2}\), then \(n=2k>N\), but \(|1+(-1)^n|=2\). Thus no choice of \(N\) will satisfy the conditions of the definition for this \(\eps\), (namely that \(|1+(-1)^n|\lt 2\) for all \(n>N\)) and so \(\lim_{n\rightarrow\infty}\left(1+(-1)^n\right)\neq 0\).%
\end{proof}
\begin{problem}{Problem}{}{prob_sequences-not_converge_to_zero}%
\index{limit!definition of non-existence} Negate the definition of \(\lim_{n\rightarrow\infty}s_n=0\) to provide a formal definition for \(\limitt{n}{\infty}{s_n}\neq 0\).%
\end{problem}
\begin{problem}{Problem}{}{prob_sequences4}%
Use the definition to prove \(\limitt{n}{\infty}{\frac{n}{n+100}}\neq 0\).%
\end{problem}
Now that we have a handle on how to rigorously prove that a sequence converges to zero, let's generalize this to a formal definition for a sequence converging to something else. Basically, we want to say that a sequence \(\left(s_n\right)\) converges to a real number \(s\), provided the difference \(\left(s_n-s\right)\) converges to zero. This leads to the following definition:%
\begin{definition}{Definition}{}{def_ConvergenceOfASequence}%
\index{sequences!convergence}\index{convergence!of a sequence} Let \(\left(s_n\right)=\left(s_1,s_2,s_3,\ldots\right)\) be a sequence of real numbers and let \(s\) be a real number. We say that \(\left(\boldsymbol{s}_{\boldsymbol{n}}\right)\) \textbraceleft{}converges to\textbraceright{} \(\boldsymbol{s}\) and write \(\limitt{n}{\infty}{s_n}=s\) provided for any \(\eps>0\), there is a real number \(N\) such that if \(n>N\), then \(|s_n-s|\lt \eps\).%
\end{definition}
Notes on \hyperref[def_ConvergenceOfASequence]{Definition~{\xreffont\ref{def_ConvergenceOfASequence}}}%
\begin{enumerate}
\item{}Clearly%
\begin{align*}
\limit{n}{\infty}{s_n}=s\amp{}\amp{} \text{if and only
if}\amp{}\amp{}
\limit{n}{\infty}{\left(s_n-s\right)}=0.
\end{align*}
%
\item{}Again notice that this says that we can make \(s_n\) as close to \(s\) as we wish (within \(\eps\)) by making \(n\) large enough (\(>N)\). As before, this definition makes these notions very specific.%
\item{}Notice that \(\abs{s_n-s}\lt \eps\) can be written in the following equivalent forms%
\par
%
\begin{itemize}[label=\textbullet]
\item{}\(\displaystyle \abs{s_n-s}\lt \eps\)%
\item{}\(\displaystyle -\eps\lt s_n-s\lt \eps\)%
\item{}\(\displaystyle s-\eps\lt s_n\lt s+\eps\)%
\item{}\(\displaystyle s_n\in\left(s-\eps,s+\eps\right)\)%
\end{itemize}
%
\par
and we are free to use any one of these which is convenient at the time.%
\end{enumerate}
%
\par
As an example, let's use this definition to prove that the sequence in \hyperref[prob_sequences4]{Problem~{\xreffont\ref{prob_sequences4}}}, in fact, converges to 1.%
\begin{example}{Example}{}{example_SeriesConverge}%
Prove \(\limitt{n}{\infty}{\frac{n}{n+100}}=1\).%
\par
\terminology{SCRAPWORK}%
\par
Given an \(\eps>0\), we need to get \(\abs{\frac{n}{n+100}-1}\lt \eps\). This prompts us to do some algebra.%
\begin{equation*}
\left|\frac{n}{n+100}-1\right|=\left|\frac{n-(n+100)}{n+100}\right|\leq\frac{100}{n}\text{.}
\end{equation*}
%
\par
This in turn, seems to suggest that \(N=\frac{100}{\eps}\) should work.%
\end{example}
\begin{proof}{Proof}{}{SeqRealNum-41}
Let \(\eps>0\). Let \(N=\frac{100}{\eps}\). If \(n>N\), then \(n>\frac{100}{\eps}\) and so \(\frac{100}{n}\lt \eps\). Hence%
\begin{equation*}
\left|\frac{n}{n+100}-1\right|=\left|\frac{n-(n+100)}{n+100}\right|= \frac{100}{n+100}\lt \frac{100}{n}\lt \eps\text{.}
\end{equation*}
%
\par
Thus by definition \(\limitt{n}{\infty}{\frac{n}{n+100}} =1\).%
\end{proof}
Notice again that the scrapwork is not part of the formal proof and the author of a proof is not obligated to explain where the choice of \(N\) came from (although the thought process can usually be seen in the formal proof).  The formal proof contains only the requisite three parts:%
\begin{itemize}[label=\textbullet]
\item{}provide the challenge of an arbitrary \(\eps>0\),%
\item{}provide a specific \(N\), and%
\item{}show that this \(N\) works for the given \(\eps\).%
\end{itemize}
%
\par
Also notice that given a specific sequence such as \(\left(\frac{n}{n+100}\right)\), the definition does not indicate what the limit would be if, in fact, it exists.  Once an educated guess is made the definition only verifies that this intuition is correct.%
\par
This leads to the following question: If intuition is needed to determine what a limit of a sequence should be, then what is the purpose of this relatively non-intuitive, complicated definition?%
\par
Remember that when these rigorous formulations were developed, intuitive notions of convergence were already in place and had been used with great success.  But the arguments based on them could not be fully justified. This definition was developed to address these foundational issues.%
\par
Could our intuitions be verified in a concrete fashion that was above reproach? This was the purpose of this non-intuitive definition. It was to be used to verify that our intuition was, in fact, correct and do so in a very prescribed manner. For example, if \(b>0\) is a fixed number, then you would probably say as \(n\) approaches infinity, \(b^{\left(\frac{1}{n}\right)}\) approaches \(b^0=1\). After all, we did already prove that \(\lim_{n\rightarrow\infty}\frac{1}{n}=0\). We should be able to back up this intuition with our rigorous definition.%
\begin{problem}{Problem}{}{SeqRealNum-47}%
\index{limit!\(\lim_{n\rightarrow\infty}b^{\left(\frac{1}{n}\right)}=1\) if \(b>0\)} Let \(b>0\). Use the definition to prove \(\limitt{n}{\infty}{b^{\left(\frac{1}{n}\right)}}=1\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{SeqRealNum-47-2}{}\quad{}You will probably need to separate this into two cases: \(0\lt b\lt 1\) and \(b\geq 1\).%
\end{problem}
\begin{problem}{Problem}{}{SeqRealNum-48}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Provide a rigorous definition for \(\limitt{n}{\infty}{s_n}\neq s\).%
\item{}Use your definition to show that for any real number \(a\), \(\limitt{n}{\infty}{\left(\left(-1\right)^n\right)}\neq a\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{SeqRealNum-48-4-2}{}\quad{}Choose \(\eps=1\) and use the fact that \(\abs{a-(-1)^n}\lt 1\) is equivalent to \(\left(-1\right)^n-1\lt a\lt \left(-1\right)^n+1\) to show that no choice of \(N\) will work for this \(\eps\).%
\end{enumerate}%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 6.2 The Limit as a Primary Tool}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{The Limit as a Primary Tool}{}{The Limit as a Primary Tool}{}{}{LimitAsPrimary}
As you've seen from the previous sections, the formal definition of the convergence of a sequence is meant to capture rigorously our intuitive understanding of convergence.  However, the definition itself is an unwieldy tool.  If only there was a way to be rigorous without having to run back to the definition each time.  Fortunately, there is a way.  If we can use the definition to prove some general rules about limits then we could use these rules whenever they applied and be assured that everything was still rigorous.  A number of these should look familiar from Calculus.%
\begin{problem}{Problem}{}{LimitAsPrimary-3}%
Let \(\left(c\right)_{n=1}^\infty=(c,c,c,\ldots)\) be a constant sequence.  Show that \(\limitt{n}{\infty}{c}=c\).%
\end{problem}
In proving the familiar limit theorems, the following will prove to be a very useful tool.%
\begin{lemma}{Lemma}{}{}{Tri-RevTri-Ineq}%
\index{Triangle Inequalities}%
%
\begin{descriptionlist}
\begin{dlimedium}{Triangle Inequality:}{TriangleIneq}%
\index{Triangle Inequalities!Triangle Inequality}Let \(a\) and \(b\) be real numbers. Then%
\begin{equation*}
\abs{a+b}\leq \abs{a}+\abs{b}\text{.}
\end{equation*}
%
\end{dlimedium}%
\begin{dlimedium}{Reverse Triangle Inequality:}{InvTriangleIneq}%
\index{Triangle Inequalities!Reverse Triangle Inequalitiy}Let \(a\) and \(b\) be real numbers. Then%
\begin{equation*}
\abs{a}-\abs{b}\leq\abs{a-b}
\end{equation*}
%
\end{dlimedium}%
\end{descriptionlist}
%
\end{lemma}
\begin{problem}{Problem}{}{Tri-RevTri-Ineq_prob}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Prove \hyperref[Tri-RevTri-Ineq]{Lemma~{\xreffont\ref{Tri-RevTri-Ineq}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{Tri-RevTri-Ineq_prob-2-2}{}\quad{}For the Reverse Triangle Inequality, consider \(\abs{a}=\abs{a-b+b}\).%
\item{}Show \(\Big||a|-|b|\Big|\leq|a-b|\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{Tri-RevTri-Ineq_prob-3-2}{}\quad{}You want to show that%
\begin{equation*}
|a|-|b|\leq|a-b|
\end{equation*}
and that%
\begin{equation*}
-\left(|a|-|b|\right)\leq|a-b|\text{.}
\end{equation*}
%
\end{enumerate}%
\end{problem}
\begin{theorem}{Theorem}{}{}{thm_SumOfSequences}%
\index{limit!termwise sums of}%
If \(\limitt{n}{\infty}{a_n}=a\) and \(\limitt{n}{\infty}{b_n}=b\), then%
\begin{equation*}
\limit{n}{\infty}{\left(a_n+b_n\right)}=a+b\text{.}
\end{equation*}
%
\end{theorem}
We will often informally state this theorem as ``the limit of a sum is the sum of the limits.'' However, to be absolutely precise, what it says is that if we already know that two sequences converge, then the sequence formed by summing the corresponding terms of those two sequences will converge and, in fact, converge to the sum of those individual limits. We'll provide the scrapwork for the proof of this and leave the formal write-up as an exercise. Note the use of the triangle inequality in the proof.%
\par
%
\begin{problem}{Problem}{}{LimitAsPrimary-10}%
\index{sequences!termwise sums of} Prove \hyperref[thm_SumOfSequences]{Theorem~{\xreffont\ref{thm_SumOfSequences}}}.%
\par
\terminology{SCRAPWORK:}%
\par
If we let \(\eps>0\), then we want \(N\) so that if \(n>N\), then \(\big|\left(a_n+b_n\right)-\left(a+b\right)\big|\lt \eps\). We \emph{know} that \(\limit{n}{\infty}{a_n}=a\) and \(\limit{n}{\infty}{b_n}=b\), so we can make \(\big|a_n-a\big|\) and  \(\big|b_n-b\big|\) as small as we wish, provided we make \(n\) large enough. Let's go back to what we want, to see if we can close the gap between what we know and what we want. We know,           by the triangle inequality, that%
\begin{equation*}
\left|\left(a_n+b_n\right)-\left(a+b\right)\right|=\left|\left(a_n-a\right)+\left(b_n-b\right)\right|\leq\left|a_n-a\right|+\left|b_n-b\right|\text{.}
\end{equation*}
To make this whole thing less than \(\eps\), it makes sense to make each part less than \(\frac{\eps}{2}\).%
\par
Fortunately, we can do that as the definitions of \(\limit{n}{\infty}{a_n}=a\) and \(\limit{n}{\infty}{b_n}=b\) allow us to make \(\abs{a_n-a}\) and \(\abs{b_n-b}\) arbitrarily small.%
\par
Specifically, since \(\limit{n}{\infty}{a_n}=a\), there exists an \(N_1\) such that if \(n>N_1\) then \(\abs{a_n-a}\lt \frac{\eps}{2}\).  Also since \(\limit{n}{\infty}{b_n}=b\), there exists an \(N_2\) such that if \(n>N_2\) then \(\abs{b_n-b}\lt
\frac{\eps}{2}\).  Since we want both of these to occur, it makes sense to let \(N=\)max\(\left(N_1,N_2\right)\).  This should be the \(N\) that we seek.%
\end{problem}
\begin{theorem}{Theorem}{}{}{thm_LimitOfProduct}%
\index{limit!products of} If \(\displaystyle\lim_{n\rightarrow\infty}a_n=a\) and \(\displaystyle\lim_{n\rightarrow\infty}b_n=b\), then \(\displaystyle\lim_{n\rightarrow\infty}\left(a_n b_n\right)=a b\).%
\end{theorem}
\terminology{SCRAPWORK:} Given \(\eps>0\), we want \(N\) so that if \(n>N\), then \(\abs{a_n  b_n-a  b}\lt \eps\). One of the standard tricks in analysis is to ``uncancel.'' In this case we will subtract and add a convenient term. Normally these would ``cancel out,'' which is why we say that we will uncancel to put them back in. You already saw an example of this in proving the Reverse Triangle Inequality (\hyperref[Tri-RevTri-Ineq_prob]{Problem~{\xreffont\ref{Tri-RevTri-Ineq_prob}}}). In the present case, consider%
\begin{align*}
\abs{a_n  b_n-a  b}\amp =\abs{a_n  b_n-a_n b+a_n b-a  b}\\
\amp \leq \abs{a_n  b_n-a_n  b}+\abs{a_n b-a b}|\\
\amp =\abs{a_n}\abs{b_n-b}+\abs{b}\abs{a_n-a}\text{.}
\end{align*}
%
\par
We can make this whole thing less than \(\eps\), provided we make each term in the sum less than \(\frac{\eps}{2}\).  We can make \(\big|b\big|\big|a_n-a\big|\lt \frac{\eps}{2}\) if we make \(\big|a_n-a\big|\lt \frac{\eps}{2|b|}\).  But wait! What if \(b=0\)?  We could handle this as a separate case or we can do the following ``slick trick.'' Notice that we can add one more line to the above string of inequalities:%
\begin{equation*}
\left|a_n\right|\left|b_n-b\right|+\left|b\right|\left|a_n-a\right|\lt
\left|a_n
\right|\left|b_n-b\right|+\left(\left|b\right|+1\right)\left|a_n-a
\right|\text{.}
\end{equation*}
Now we can make \(\left|a_n-a\right|\lt
\frac{\eps}{2\left(|b|+1\right)}\) and not worry about dividing by zero.%
\par
Making \(\abs{a_n}\abs{b_n-b}\lt \frac{\eps}{2}\) requires a bit more finesse. At first glance, one would be tempted to try and make \(\abs{b_n-b}\lt \frac{\eps}{2|a_n|}\). Even if we ignore the fact that we could be dividing by zero (which we could handle), we have a bigger problem. According to the definition of \(\limitt{n}{\infty}{b_n}=b\), we can make \(\abs{b_n-b}\) smaller than any given fixed positive number, as long as we make \(n\) large enough (larger than some \(N\) which goes with a given epsilon). Unfortunately, \(\frac{\eps}{2|a_n|}\) is not fixed as it has the variable \(n\) in it; there is no reason to believe that a single \(N\) will work with all possible values of \(n\) simultaneously. To handle this impasse, we need the following:%
\begin{lemma}{Lemma}{}{}{lemma_BoundedConvergent}%
\index{sequences!convergence of!convergent sequences are bounded}%
\alert{A Convergent Sequence Is Bounded.}%
\par
If \(\limitt{n}{\infty}{a_n}=a\), then there exists \(B>0\) such that \(\abs{a_n}\leq B\) for all \(n\).%
\end{lemma}
\begin{problem}{Problem}{}{prob_BoundedConvergent}%
Prove \hyperref[lemma_BoundedConvergent]{Lemma~{\xreffont\ref{lemma_BoundedConvergent}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{prob_BoundedConvergent-3}{}\quad{}We know that there exists \(N\) such that if \(n>N\), then \(\abs{a_n-a}\lt 1\).  Let \(B=\)max\(\left(\abs{a_1},\abs{a_2},\ldots,\abs{a_{\lceil{N}\rceil}},\abs{a}+1\right)\), where \(\lceil{N}\rceil\) represents the smallest integer greater than or equal to \(N\).  Also, notice that this is not a convergence proof so it is not safe to think of \(N\) as a large number.%
\begin{aside}{Aside}{Comment.}{prob_BoundedConvergent-3-2}%
Actually, this is a dangerous habit to fall into even in convergence proofs.%
\end{aside}
\end{problem}
Armed with this bound \(B\), we can add on one more inequality to the above scrapwork to get%
\begin{align*}
\abs{a_n\cdot b_n-a\cdot b}\amp =\abs{a_n\cdot b_n-a_n\cdot b+a_n\cdot b-a\cdot b}\\
\amp \leq \abs{a_n\cdot b_n-a_n\cdot b}+ \abs{a_n\cdot b-a\cdot b}\\
\amp = \abs{a_n}\abs{b_n-b}+\abs{b}\abs{a_n-a}\\
\amp \lt B \abs{b_n-b}+\left(\abs{b}+1\right)\abs{a_n-a}
\end{align*}
%
\par
At this point, we should be able to make the last line of this less than \(\eps\).%
\par
\terminology{END OF SCRAPWORK}%
\begin{problem}{Problem}{}{LimitAsPrimary-20}%
\index{sequences!termwise product of} Prove \hyperref[thm_LimitOfProduct]{Theorem~{\xreffont\ref{thm_LimitOfProduct}}}.%
\end{problem}
\begin{corollary}{Corollary}{}{}{cor_1}%
If \(\displaystyle\lim_{n\rightarrow\infty}a_n=a\) and \(c\in\mathbb{R}\), then \(\displaystyle\lim_{n\rightarrow\infty}c\cdot a_n=c\cdot a\).%
\begin{aside}{Aside}{Comment.}{cor_1-1-2}%
This is a corollary to \hyperref[thm_LimitOfProduct]{Theorem~{\xreffont\ref{thm_LimitOfProduct}}}.%
\end{aside}
\end{corollary}
\begin{problem}{Problem}{}{LimitAsPrimary-22}%
\index{sequences!constant multiples of}\index{limit!of a constant times a sequence} Prove  \hyperref[cor_1]{Corollary~{\xreffont\ref{cor_1}}}.%
\end{problem}
Just as \hyperref[thm_LimitOfProduct]{Theorem~{\xreffont\ref{thm_LimitOfProduct}}} says that the limit of a product is the product of the limits, we can prove the analogue for quotients.%
\begin{theorem}{Theorem}{}{}{thm_LimitOfQuotient}%
\index{limit!quotients of}%
Suppose \(\limitt{n}{\infty}{a_n}=a\) and \(\limitt{n}{\infty}{b_n}=b\). Also suppose \(b\neq 0\) and \(b_n\neq 0, \forall n\).%
\par
Then%
\begin{equation*}
\limit{n}{\infty}{\left(\frac{a_n}{b_n}\right)}=\frac{a}{b}\text{.}
\end{equation*}
%
\end{theorem}
\begin{problem}{Problem}{}{LimitAsPrimary-25}%
\index{sequences!termwise quotient of} Prove \hyperref[thm_LimitOfQuotient]{Theorem~{\xreffont\ref{thm_LimitOfQuotient}}}.%
\par
\terminology{SCRAPWORK}%
\par
To prove this we'll  look first  at  special case. We'll  prove  that%
\begin{equation*}
\limitt{n}{\infty}{\left(\frac{1}{b_n}\right)}=\frac{1}{b}\text{.}
\end{equation*}
The general case will follow from this and \hyperref[thm_LimitOfProduct]{Theorem~{\xreffont\ref{thm_LimitOfProduct}}}. Consider \(\big|\frac{1}{b_n}-\frac{1}{b}\big|=\frac{|b-b_n|}{|b_n||b|}\). We are faced with the same dilemma as before; we need to get \(\big|\frac{1}{b_n}\big|\) bounded above. This means we need to get \(|b_n|\) bounded away from zero (at least for large enough \(n\)).%
\par
This can be done as follows. Since \(b\neq 0\), then \(\frac{|b|}{2}>0\). Thus, by the definition of \(\lim_{n\rightarrow\infty}b_n=b\) , there exists \(N_1\) such that if \(n>N_1\), then \(|b\mathopen|-|b_n|\leq\big|b-b_n\big|\lt \frac{|b|}{2}\). Thus when \(n>N_1\), \(\frac{|b|}{2}\lt |b_n|\) and so \(\frac{1}{|b_n|}\lt \frac{2}{|b|}\). This says that for \(n>N_1\), \(\frac{|b-b_n|}{|b_n||b|}\lt \frac{2}{|b|^2}|b-b_n|\). We should be able to make this smaller than a given \(\eps>0\), provided we make \(n\) large enough.%
\end{problem}
These theorems allow us to compute limits of complicated sequences and rigorously verify that these are, in fact, the correct limits without resorting to the definition of a limit.%
\begin{problem}{Problem}{}{LimitAsPrimary-27}%
\index{limit!identifing the theorems used in a limit} Identify all of the theorems implicitly used to show that%
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{3n^3-100n+1}{5n^3+4n^2-7}=\lim_{n \rightarrow\infty}\frac{n^3\left(3-\frac{100}{n^2}+\frac{1}{n^3}\right)}{n^3 \left(5+\frac{4}{n}-\frac{7}{n^3}\right)}=\frac{3}{5}\text{.}
\end{equation*}
%
\par
Notice that this presumes that all of the individual limits exist. That this is true will become evident as the proof is fleshed out.%
\end{problem}
There is one more tool that will prove to be valuable.%
\begin{theorem}{Theorem}{}{}{thm_SqueezeTheorem}%
\index{limit!Squeeze Theorem for Sequences}%
\terminology{Squeeze Theorem for Sequences}%
\par
Let \(\left(r_n\right),\left(s_n\right)\), and \(\left(t_n\right)\) be sequences of real numbers with \(r_n\leq s_n\leq t_n,\forall\) positive integers \(n\). Suppose \(\limit{n}{\infty}{r_n}=s=\limit{n}{\infty}{t_n}\). Then \(\left(s_n\right)\) must converge and \(\limit{n}{\infty}{s_n}=s\).%
\end{theorem}
\begin{problem}{Problem}{}{LimitAsPrimary-30}%
Prove \hyperref[thm_SqueezeTheorem]{Theorem~{\xreffont\ref{thm_SqueezeTheorem}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{LimitAsPrimary-30-3}{}\quad{}This is probably a place where you would want to use \(s-\eps\lt s_n\lt s+\eps\) instead of \(|s_n-s|\lt
\eps\).%
\end{problem}
The Squeeze Theorem holds even if \(r_n\leq s_n\leq t_n\) holds for only sufficiently large \(n\); i.e., for \(n\) larger than some fixed \(N_0\). This is true because when you find an \(N_1\) that works in the original proof, this can be modified by choosing \(N=\)max\(\left(N_0,N_1\right)\). Also note that this theorem really says two things: \(\left(s_n\right)\) converges and it converges to \(s\). This subtle point affects how one should properly use the Squeeze Theorem.%
\begin{example}{Example}{}{example_SqzeEx}%
Prove \(\displaystyle\lim_{n\rightarrow\infty}\frac{n+1}{n^2}=0\).%
\end{example}
\begin{proof}{Proof}{}{LimitAsPrimary-33}
Notice that \(0\leq\frac{n+1}{n^2}\leq\frac{n+n}{n^2}=\frac{2}{n}\). Since \(\displaystyle\lim_{n\rightarrow\infty}0=0=\lim_{n\rightarrow\infty}\frac{2}{n}\), then by the Squeeze Theorem, \(\displaystyle\lim_{n\rightarrow\infty}\frac{n+1}{n^2}=0\).%
\end{proof}
Notice that this proof is completely rigorous. Also notice that this is the proper way to use the Squeeze Theorem. Here is an example of an \emph{improper} use of the Squeeze Theorem.%
\par
How \emph{not} to prove \hyperref[example_SqzeEx]{Example~{\xreffont\ref{example_SqzeEx}}}. Notice that%
\begin{equation*}
0\leq\frac{n+1}{n^2}\leq\frac{n+n}{n^2}=\frac{2}{n}\text{,}
\end{equation*}
so%
\begin{equation*}
0=\lim_{n\rightarrow\infty}0 \leq \lim_{n\rightarrow\infty}\frac{n+1}{n^2}\leq\lim_{n\rightarrow\infty}\frac{2}{n}=0
\end{equation*}
and%
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{n+1}{n^2}=0\text{.}
\end{equation*}
%
\par
This is incorrect in form because it presumes that \(\lim_{n\rightarrow\infty}\frac{n+1}{n^2}\) exists, which we don't yet know. If we knew that the limit existed to begin with, then this would be fine. The Squeeze Theorem proves that the limit does in fact exist, but it must be so stated.%
\par
These general theorems will allow us to rigorously explore convergence of power series in the next chapter without having to appeal directly to the definition of convergence. However, you should remember that we used the definition to prove these results and there will be times when we will need to apply the definition directly. However, before we go into that, let's examine divergence a bit more closely.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 6.3 Divergence}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Divergence}{}{Divergence}{}{}{DivergentSeq}
In \hyperref[thm_rearrangements]{Theorem~{\xreffont\ref{thm_rearrangements}}} we saw that there is a rearrangment of the alternating Harmonic series which diverges to \(\infty\) or \(-\infty\). In that section we did not fuss over any formal notions of divergence. We assumed instead that you are already familiar with the concept of divergence, probably from taking Calculus in the past.%
\par
However we are now in the process of building precise, formal definitions for the concepts we will be using so we define the divergence of a sequence as follows.%
\begin{definition}{Definition}{}{thm_divergence_of_a_sequence}%
\index{sequences!divergence of}\index{divergence!of a sequence} A sequence of real numbers \(\left(s_n\right)_{n=1}^\infty\) diverges if it does not converge to any \(a\in\RR\).%
\end{definition}
It may seem unnecessarily pedantic of us to insist on formally stating such an obvious definition. After all ``converge'' and ``diverge'' are opposites in ordinary English. Why wouldn't they be mathematically opposite too? Why do we have to go to the trouble of formally defining both of them? Since they are opposites defining one implicitly defines the other doesn't it?%
\par
One way to answer that criticism is to state that in mathematics we \emph{always} work from precisely stated definitions and tightly reasoned logical arguments.%
\par
But this is just more pedantry. It is a way of saying, ``Because we said so'' all dressed up in imposing language. We need to do better than that.%
\par
One reason for providing formal definitions of both convergence and divergence is that in mathematics we frequently co-opt words from natural languages like English and imbue them with mathematical meaning that is only tangentially related to the original English definition. When we take two such words which happen to be opposites in English and give them mathematical meanings which are \emph{not} opposites it can be very confusing, especially at first.%
\par
This is what happened with the words ``open'' and ``closed.'' These are opposites in English: ``not open'' is ``closed,'' ``not closed'' is ``open,'' and there is nothing which is both open and closed. But recall that an open interval on the real line, \((a,b)\), is one that does not include \emph{either} of its endpoints while a closed interval, \([a,b]\), is one that includes both of them.%
\par
These may seem like opposites at first but they are not.  To see this observe that the interval \((a,b]\) is neither open nor closed since it only contains one of its endpoints. If ``open'' and ``closed'' were mathematically opposite then \emph{every} interval would be either open or closed.%
\begin{aside}{Aside}{Comment: Open Sets vs. Closed Sets.}{DivergentSeq-11}%
It is also true that \((-\infty,\infty)\) is \emph{both} open and closed. We will discuss this a bit further in \hyperref[SECTIONRiseSetTheory]{Section~{\xreffont\ref{SECTIONRiseSetTheory}}}.%
\end{aside}
Mathematicians have learned to be extremely careful about this sort of thing. In the case of convergence and divergence of a series, even though these words are actually opposites mathematically (every sequence either converges or diverges and no sequence converges \emph{and} diverges) it is better to say this explicitly so there can be no confusion.%
\par
A sequence \(\left(a_n\right)_{n=1}^\infty\) can only converge to a real number, \(a\), in one way: by getting arbitrarily close to \(a\). However there are several ways a sequence might diverge.%
\begin{example}{Example}{}{DivergentSeq-14}%
Consider the sequence, \(\left(n\right)_{n=1}^\infty\). This clearly diverges by getting larger and larger \(\ldots\) Ooops! Let's be careful. The sequence \(\left(1-\frac1n\right)_{n=1}^\infty\) gets larger and larger too, but it converges. What we meant to say was that the terms of the sequence \(\left(n\right)_{n=1}^\infty\) become \emph{arbitrarily} large as \(n\) increases.%
\par
This is clearly a divergent sequence but it may not be clear how to prove this formally. Here's one way.%
\par
To show divergence we must show that the sequence satisfies the \emph{negation} of the definition of convergence. That is, we must show that for every \(r\in\RR\) there is an \(\eps>0\) such that for every \(N\in\RR\), there is an \(n>N\) with \(\left|n-r\right|\ge\eps\).%
\par
So let \(\eps=1\), and let \(r\in\RR\) be given. Let \(N=r+2\). Then for every \(n>N\) \(\abs{n-r} > \abs{(r+2)-r}=2>1\). Therefore the sequence diverges.%
\end{example}
This seems to have been rather more work than we should have to do for such a simple problem. Here's another way which highlights this particular type of divergence.%
\par
First we'll need a new definition:%
\begin{definition}{Definition}{Divergence to Infinity.}{def_DivToInf}%
\index{sequences!divergence to \(\infty\)}\index{\(\infty\)!positive infinity!divergence to} A sequence, \(\left(a_n\right)_{n=1}^\infty\), \terminology{diverges to positive infinity} if for every real number \(r\), there is a real number \(N\) such that \(n>N\imp a_n>r\).%
\par
\index{\(\infty\)!negative infinity!divergence to} A sequence, \(\left(a_n\right)_{n=1}^\infty\), \terminology{diverges to negative infinity} if for every real number \(r\), there is a real number \(N\) such that \(n>N\imp a_n\lt r\).%
\par
\index{\(\infty\)!divergence to} A sequence is said to \terminology{diverge to infinity} if it diverges to either positive or negative infinity.%
\end{definition}
In practice we want to think of \(\abs{r}\) as a very large number. This definition says that a sequence diverges to infinity if it becomes arbitrarily large as \(n\) increases, and similarly for divergence to negative infinity.%
\begin{problem}{Problem}{}{DivergentSeq-19}%
\index{sequences!the sequence of positive integers diverges to infinity} Show that \(\left(n\right)_{n=1}^\infty\) diverges to infinity.%
\end{problem}
\begin{problem}{Problem}{}{DivergentSeq-20}%
\index{sequences!divergence to \(\infty\)}\index{divergence!divergence to infinity implies divergence} Show that if \(\left(a_n\right)_{n=1}^\infty\) diverges to infinity then \(\left(a_n\right)_{n=1}^\infty\) diverges.%
\end{problem}
We will denote divergence to infinity as%
\begin{equation*}
\limit{n}{\infty}{a_n}=\pm\infty\text{.}
\end{equation*}
%
\par
However, strictly speaking this is an abuse of notation since the symbol \(\infty\) does not represent a real number. This notation can be very problematic since it looks so much like the notation we use to denote convergence: \(\limit{n}{\infty}{a_n}=a\).%
\par
Nevertheless, the notation is appropriate because divergence to infinity is ``nice'' divergence in the sense that it shares many of the properties of convergence, as the next problem shows.%
\begin{problem}{Problem}{}{DivergentSeq-24}%
Suppose \(\limit{n}{\infty}{a_n}=\infty\) and \(\limit{n}{\infty}{b_n}=\infty\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Show that \(\limit{n}{\infty}{(a_n+b_n)}=\infty\)%
\item{}Show that \(\limit{n}{\infty}{a_nb_n}=\infty\)%
\item{}Is it true that \(\limit{n}{\infty}{\frac{a_n}{b_n}}=\infty?\) Explain.%
\end{enumerate}%
\end{problem}
Because divergence to positive or negative infinity shares some of the properties of convergence it is easy to get careless with it.  Remember that even though we write \(\limit{n}{\infty}{a_n}=\infty\) this is still a divergent sequence in the sense that \(\limit{n}{\infty}{a_n}\) does not exist.  The symbol \(\infty\) does not represent a real number.  This is just a convenient notational shorthand telling us that the sequence diverges by becoming arbitrarily large.%
\begin{problem}{Problem}{}{DivergentSeq-26}%
Suppose \(\limit{n}{\infty}{a_n}=\infty\) and \(\limit{n}{\infty}{b_n}=-\infty\) and \(\alpha\in\RR\).  Prove or give a counterexample:%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\limit{n}{\infty}{a_n+b_n}=\infty\)%
\item{}\(\limit{n}{\infty}{a_nb_n}=-\infty\)%
\item{}\(\limit{n}{\infty}{\alpha a_n}=\infty\)%
\item{}\(\limit{n}{\infty}{\alpha b_n}=-\infty\)%
\end{enumerate}%
\end{problem}
Finally, a sequence can diverge in other ways as the following problem displays.%
\begin{problem}{Problem}{}{DivergentSeq-28}%
Show that each of the following sequences diverge.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\left(\left(-1\right)^n\right)_{n=1}^\infty\)%
\item{}\(\left(\left(-1\right)^nn\right)_{n=1}^\infty\)%
\item{}\(a_n = \begin{cases}1\amp \text{ if \(n=2^p\) for some \(p\in\NN\) } \\ \frac1n\amp \text{ otherwise. } \end{cases}\)%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{DivergentSeq-29}%
Suppose that \(\left(a_n\right)_{n=1}^\infty\) diverges but not to infinity and that \(\alpha\) is a real number. What conditions on \(\alpha\) will guarantee that:%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\left(\alpha a_n\right)_{n=1}^\infty\) converges?%
\item{}\(\left(\alpha a_n\right)_{n=1}^\infty\) diverges?%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{DivergentSeq-30}%
Show that if \(\abs{r}>1\) then \(\left(r^n\right)_{n=1}^\infty\) diverges.  Will it diverge to infinity?%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 6.4 Additional Problems}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Additional Problems}{}{Additional Problems}{}{}{Convergence-5}
\begin{problem}{Problem}{}{Convergence-5-2}%
Prove that if \(\lim_{n\rightarrow\infty}s_n=s\) then \(\lim_{n\rightarrow\infty}|s_n|=|s|\).  Prove that the converse is true when \(s=0\), but it is not necessarily true otherwise.%
\end{problem}
\begin{problem}{Problem}{}{Convergence-5-3}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Let \(\left(s_n\right)\) and \(\left(t_n\right)\) be sequences with \(s_n\leq t_n,\forall n\).  Suppose \(\lim_{n\rightarrow\infty}s_n=s\) and \(\lim_{n\rightarrow\infty}t_n=t\).%
\par
Prove \(s\leq t\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{Convergence-5-3-2-2}{}\quad{}Assume for contradiction, that \(s>t\) and use the definition of convergence with \(\eps=\frac{s-t}{2}\) to produce an \(n\) with \(s_n>t_n\).%
\item{}Prove that if a sequence converges, then its limit is unique.  That is, prove that if \(\lim_{n\rightarrow\infty}s_n=s\) and \(\lim_{n\rightarrow\infty}s_n=t\), then \(s=t\).%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{Convergence-5-4}%
Prove that if the sequence \(\left(s_n\right)\) is bounded then \(\lim_{n\rightarrow\infty}\left(\frac{s_n}{n}\right)=0\).%
\end{problem}
\begin{problem}{Problem}{}{prob_series-geometric}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Prove that if \(x\neq 1\), then%
\begin{equation*}
1+x+x^2+\cdots+x^n=\frac{1-x^{n+1}}{1-x}\text{.}
\end{equation*}
%
\item{}Use (a) to prove that if \(|x|\lt 1\), then \(\lim_{n\rightarrow\infty}\left(\sum_{j=0}^nx^j\right)=\frac{1}{1-x}\).%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{Convergence-5-6}%
\index{limit!of ratios of polynomials} Prove%
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{a_0+a_1n+a_2n^2+
\cdots+a_kn^k}{b_0+b_1n+b_2n^2+\cdots+b_kn^k}=\frac{a_k}{b_k}\text{,}
\end{equation*}
provided \(b_k\neq 0\).%
\par
Notice division by zero is not an issue.  Since a polynomial only has finitely many roots, the denominator will be non-zero when \(n\) is sufficiently large.%
\end{problem}
\begin{problem}{Problem}{}{Convergence-5-7}%
Prove that if \(\limit{n}{\infty}{s_n}=s\) and \(\limit{n}{\infty}{\left(s_n-t_n\right)}=0\), then \(\limit{n}{\infty}{t_n}=s\).%
\end{problem}
\begin{problem}{Problem}{}{Convergence-5-8}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Prove that if \(\limit{n}{\infty}{s_n}=s\) and \(s\lt
t\), then there exists a real number \(N\) such that if \(n>N\) then \(s_n\lt t\).%
\item{}Prove that if \(\limit{n}{\infty}{s_n}=s\) and \(r\lt
s\), then there exists a real number \(M\) such that if \(n>M\) then \(r\lt s_n\).%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{prob_RatioTest}%
Suppose \(\left(s_n\right)\) is a sequence of positive numbers such that%
\begin{equation*}
\limit{n}{\infty}{\left(\frac{s_{n+1}}{s_n}\right)}=L\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Prove that if \(L\lt 1\), then \(\limit{n}{\infty}{s_n}=0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{prob_RatioTest-3-2}{}\quad{}Choose \(R\) with \(L\lt R\lt 1\).  By the previous problem, \(\exists\) \(N\) such that if \(n>N\), then \(\frac{s_{n+1}}{s_n}\lt R\).  Let \(n_0>N\) be fixed and show \(s_{n_0+k}\lt R^ks_{n_0}\). Conclude that \(\lim_{k\rightarrow\infty}s_{n_0+k}=0\) and let \(n=n_0+k\).%
\item{}Let \(c\) be a positive real number.  Prove \(\displaystyle\lim_{n\rightarrow\infty}\left(\frac{c^n}{n!}\right)=0\).%
\end{enumerate}%
\end{problem}
\end{sectionptx}
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 7 A ``Tayl'' of Three Remainders}
\typeout{************************************************}
%
\begin{chapterptx}{Chapter}{A ``Tayl'' of Three Remainders}{}{A ``Tayl'' of Three Remainders}{}{}{TaylorSeries}
\renewcommand*{\chaptername}{Chapter}
%
%
\typeout{************************************************}
\typeout{Section 7.1 The Integral Form of the Remainder}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{The Integral Form of the Remainder}{}{The Integral Form of the Remainder}{}{}{TaylorSeries-IntFormRem}
Now that we have a rigorous definition of the convergence of a sequence, let's apply this to Taylor series.  Recall that the Taylor series of a function \(f(x)\) expanded about the point \(a\) is given by%
\begin{equation*}
\sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n=f(a)+\frac{f^{\,\prime}(a)}{1!}(x-a)+\frac{f^{\,\prime\prime}(a)}{2!}(x-a)^2+\cdots
\end{equation*}
%
\par
When we say that \(f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n\) for a particular value of \(x\), what we mean is that the sequence of partial sums%
\begin{alignat*}{1}
\amp\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)_{n=0}^\infty\\
\amp= \left(f(a), f(a)+\frac{f^{\prime}(a)}{1!}(x-a),f(a)
+\frac{f^{\prime}(a)}{1!}(x-a)+\frac{f^{\prime\prime}(a)}{2!}(x-a)^2,\ldots\right)
\end{alignat*}
converges to the number \(f(x)\). Note that the index in the summation was changed to \(j\) to allow \(n\) to represent the index of the sequence of partial sums. As intimidating as this may look, bear in mind that for a fixed real number \(x\), this is still a sequence of real numbers so, that saying \(f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n\) means that \(\lim_{n\rightarrow\infty}\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)=f(x)\) and in the previous chapter we developed some tools to examine this phenomenon. In particular, we know that \(\lim_{n\rightarrow\infty}\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)=f(x)\) is equivalent to%
\begin{equation*}
\lim_{n\rightarrow\infty}\Biggl[f(x)-\left(\sum_{j=0}^n \frac{f^{(j)}(a)}{j!}(x-a)^j\right)\Biggr]=0\text{.}
\end{equation*}
%
\par
We have seen an example of this already. \hyperref[prob_series-geometric]{Problem~{\xreffont\ref{prob_series-geometric}}} of the last chapter basically had you show that  the geometric series, \(1+x+x^2+x^3+\cdots\) converges to \(\frac{1}{1-x}\),for \(|x|\lt 1\) by showing that \(\limit{n}{\infty}{\Biggl[\frac{1}{1-x}-\left(\sum_{j=0}^nx^j\right)\Biggr]=0}\).%
\par
For the issue at hand (convergence of a Taylor series), we don't need to analyze the series itself.  What we need to show is that the difference between the function and the \(n\)th partial sum converges to zero.  This difference is called the \terminology{remainder (of the Taylor series)}. (Why?)%
\par
While it is true that the remainder is simply%
\begin{equation*}
f(x)-\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)\text{,}
\end{equation*}
this form is not easy to work with. Fortunately, a number of alternate versions of this remainder are available. We will explore these in this chapter.%
\par
Recall the result from \hyperref[TaylorsTheorem]{Theorem~{\xreffont\ref{TaylorsTheorem}}} from \hyperref[PowerSeriesQuestions]{Chapter~{\xreffont\ref{PowerSeriesQuestions}}},%
\begin{align*}
f(x)=f(a)+\frac{f^{\,\prime}(a)}{1!}(x-a)+\frac{f^{\,\prime\prime}(a)}{2!}(x-a)^2\amp +\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n\\
\amp +\frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\text{.}
\end{align*}
%
\par
We can use this by rewriting it as%
\begin{equation*}
f(x)-\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)=\frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\text{.}
\end{equation*}
%
\par
The expression \(\frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\) is called the \terminology{integral form of the remainder for the Taylor series} of \(f(x)\), and the Taylor series will converge to \(f(x)\) exactly when the sequence%
\begin{equation*}
\limit{n}{\infty}{\left(\frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{t}  \right)}
\end{equation*}
converges to zero. This form of the remainder is often easier to handle than the original \(f(x)-\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)\) and we can use it to obtain some general results.%
\begin{theorem}{Theorem}{}{}{thm_TaylorSeries}%
\terminology{Taylor's series}%
\par
\index{series!Taylor's series} If there exists a real number \(B\) such that \(|f^{(n+1)}(t)|\leq B\) for all nonnegative integers \(n\) and for all \(t\) on an interval containing \(a\) and \(x\), then%
\begin{equation*}
\lim_{n\rightarrow\infty}\left(\frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\right)=0
\end{equation*}
and so%
\begin{equation*}
f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n.{}
\end{equation*}
%
\end{theorem}
In order to prove this, it might help to first prove the following.%
\begin{lemma}{Lemma}{}{}{lemma_TriangleForIntegral}%
If \(f\) and \(\abs{f}\) are integrable functions and \(a\leq b\), then%
\begin{equation*}
\left|\int_{t=a}^bf(t)\dx{ t}\right|\leq\int_{t=a}^b|f(t)|\dx{ t}. {}
\end{equation*}
%
\end{lemma}
\begin{problem}{Problem}{}{TaylorSeries-IntFormRem-13}%
\index{Triangle Inequalities!for Integrals} Prove \hyperref[lemma_TriangleForIntegral]{Lemma~{\xreffont\ref{lemma_TriangleForIntegral}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{TaylorSeries-IntFormRem-13-2}{}\quad{}\(-|f(t)|\leq f(t)\leq|f(t)|\).%
\end{problem}
\begin{problem}{Problem}{}{TaylorSeries-IntFormRem-14}%
\index{series!Taylor's series!\(f^{(n)}\lt B,\forall\  n\in\NN\imp\) Taylor series converges} Prove \hyperref[thm_TaylorSeries]{Theorem~{\xreffont\ref{thm_TaylorSeries}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{TaylorSeries-IntFormRem-14-2}{}\quad{}You might want to use \hyperref[prob_RatioTest]{Problem~{\xreffont\ref{prob_RatioTest}}} of \hyperref[Convergence]{Chapter~{\xreffont\ref{Convergence}}}.  Also there are two cases to consider: \(a\lt x\) and \(x\lt a\) (the case \(x=a\) is trivial).  You will find that this is true in general.  This is why we will often indicate that \(t\) is between \(a\) and \(x\) as in the theorem.  In the case \(x\lt a\), notice that%
\begin{align*}
\left|\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\right|\amp =\left|(-1)^{n+1}\int_{t=x}^af^{(n+1)}(t)(t-x)^n\dx{ t}\right|\\
\amp =\left|\int_{t=x}^af^{(n+1)}(t)(t-x)^n\dx{ t}\right|.
\end{align*}
%
\end{problem}
\begin{problem}{Problem}{}{prob_Taylor_Series-using}%
Use \hyperref[thm_TaylorSeries]{Theorem~{\xreffont\ref{thm_TaylorSeries}}} to prove that for any real number \(x\)%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\displaystyle\sin x=\sum_{n=0}^\infty\frac{(-1)^nx^{2n+1}}{(2n+1)!}\)%
\item{}\(\displaystyle\cos x= \sum_{n=0}^\infty\frac{(-1)^nx^{2n}}{(2n)!}\)%
\item\label{prob_Taylor_Series-using-c}\(\displaystyle e^x=\sum_{n=0}^\infty\frac{x^n}{n!}\)%
\end{enumerate}%
\end{problem}
\hyperref[prob_Taylor_Series-using-c]{Task~{\xreffont\ref{prob_Taylor_Series-using}}.{\xreffont\ref{prob_Taylor_Series-using-c}}} shows that the Taylor series of \(e^x\) expanded at zero converges to \(e^x\) for any real number \(x\). \hyperref[thm_TaylorSeries]{Theorem~{\xreffont\ref{thm_TaylorSeries}}} can be used in a similar fashion to show that%
\begin{equation*}
e^x=\sum_{n=0}^\infty\frac{e^a(x-a)^n}{n!}
\end{equation*}
for any real numbers \(a\) and \(x\).%
\par
Recall that \hyperref[CalcIn17th18thCentury]{Chapter~{\xreffont\ref{CalcIn17th18thCentury}}} we showed that if we define the function \(E(x)\) by the power series \(\sum_{n=0}^\infty\frac{x^n}{n!}\) then \(E(x+y)=E(x)E(y)\). This, of course, is just the familiar addition property of integer exponents extended to any real number. In \hyperref[CalcIn17th18thCentury]{Chapter~{\xreffont\ref{CalcIn17th18thCentury}}} we had to \emph{assume} that defining \(E(x)\) as a series was meaningful because we did not address the convergence of the series in that chapter. Now that we know the series converges for any real number we see that the definition%
\begin{equation*}
f(x) = e^x = \sum_{n=0}^\infty\frac{x^n}{n!}
\end{equation*}
is in fact valid.%
\par
Assuming that we can differentiate this series term-by-term it is straightforward to show that \(f^\prime(x) = f(x)\).%
\begin{aside}{Aside}{Comment.}{TaylorSeries-IntFormRem-19}%
We can, but that proof will have to wait for a few more chapters.%
\end{aside}
Along with Taylor's formula this can then be used to show that \(e^{a+b}=e^ae^b\) more elegantly than the rather cumbersome proof in \hyperref[eq_ExponentAdditionProperty]{equation~({\xreffont\ref{eq_ExponentAdditionProperty}})}, as the following problem shows.%
\begin{problem}{Problem}{}{TaylorSeries-IntFormRem-21}%
\index{\(e^x\)!\(e^{a+b}=e^ae^b\)} Recall that if \(f(x)=e^x\) then \(f^\prime(x) = e^x\). Use this along with the Taylor series expansion of \(e^x\) about \(a\) to show that%
\begin{equation*}
e^{a+b}=e^ae^b.
\end{equation*}
%
\end{problem}
\hyperref[thm_TaylorSeries]{Theorem~{\xreffont\ref{thm_TaylorSeries}}} is a nice ``first step'' toward a rigorous theory of the convergence of Taylor series, but it is not applicable in all cases. For example, consider the function \(f(x)=\sqrt{1+x}\). As we saw in \hyperref[CalcIn17th18thCentury]{Chapter~{\xreffont\ref{CalcIn17th18thCentury}}}, \hyperref[prob_SqrtSeriesProb]{Problem~{\xreffont\ref{prob_SqrtSeriesProb}}}, this function's Maclaurin series (the binomial series for \(\left(1+x\right)^{1/2})\)appears to be converging to the function for \(x\in(-1,1)\). While this is, in fact, true, the above proposition does not apply. If we consider the derivatives of \(f(t)=(1+t)^{1/2}\), we obtain:%
\begin{align*}
f^\prime(t)\amp =\frac{1}{2}(1+t)^{\frac{1}{2}-1}\\
f^{\prime\prime}(t)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)(1+t)^{\frac{1}{2}-2}\\
f^{\prime\prime\prime}(t)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2 \right)(1+t)^{\frac{1}{2}-3}\\
\amp \vdots\\
f^{(n+1)}(t)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right) \cdots\left(\frac{1}{2}-n\right)(1+t)^{\frac{1}{2}-(n+1)}\text{.}
\end{align*}
%
\par
Notice that%
\begin{equation*}
\left|f^{(n+1)}(0)\right|=\frac{1}{2}\left(1-\frac{1}{2}\right)\left(2-\frac{1}{2}\right)\cdots\left(n-\frac{1}{2}\right)\text{.}
\end{equation*}
%
\par
Since this sequence grows without bound as \(n\rightarrow\infty\), then there is no chance for us to find a number \(B\) to act as a bound for all of the derviatives of \(f\) on any interval containing 0 and\(\) \(x\), and so the hypothesis of \hyperref[thm_TaylorSeries]{Theorem~{\xreffont\ref{thm_TaylorSeries}}} will never be satisfied. We need a more delicate argument to prove that%
\begin{equation*}
\sqrt{1+x}=1+\frac{1}{2}x+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)}{2!}x^2+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right)}{3!}x^3+\cdots
\end{equation*}
is valid for \(x\in(-1,1)\). To accomplish this task, we will need to express the remainder of the Taylor series differently. Fortunately, there are at least two such alternate forms.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 7.2 Lagrange's Form of the Remainder}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Lagrange's Form of the Remainder}{}{Lagrange's Form of the Remainder}{}{}{TaylorSeries-LagrFormRem}
Joseph-Louis Lagrange \index{Lagrange, Joseph-Louis} provided an alternate form for the remainder in Taylor series in his 1797 work \pubtitle{Thorie des functions analytiques.} Lagrange's form of the remainder is as follows.%
\begin{theorem}{Theorem}{}{}{thm_LagrangeRemainder}%
\terminology{Lagrange's Form of the Remainder}%
\par
\index{Lagrange's form of the remainder} Suppose \(f\) is a function such that \(f^{(n+1)}(t)\) is continuous on an interval containing \(a\) and \(x\). Then%
\begin{equation*}
f(x)-\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)=\frac{f^{\, (n+1)}(c)}{(n+1)!}(x-a)^{n+1}
\end{equation*}
where \(c\) is some number between \(a\) and \(x\).%
\end{theorem}
\begin{proof}{Proof}{}{TaylorSeries-LagrFormRem-4}
Note first that the result is true when \(x=a\) as both sides reduce to 0 (in that case \(c=x=a\).) We will prove the case where \(a\lt x\); the case \(x\lt a\) will be an exercise.%
\par
First, we already have%
\begin{equation*}
f(x)-\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)=\frac{1}{n!} \int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}
\end{equation*}
so it suffices to show that%
\begin{equation*}
\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}=\,\frac{f^{\,(n+1)}(c)}{n+1}(x-a)^{n+1}
\end{equation*}
for some \(c\) with \(c\in[\,a,x]\). To this end, let%
\begin{equation*}
M=\max_{a\le t\le x}\left(f^{(n+1)}(t)\right)
\end{equation*}
and%
\begin{equation*}
m=\min_{a\le t\le x}\left(f^{(n+1)}(t)\right)\text{.}
\end{equation*}
%
\par
Note that for all \(t\in[\,a,x]\), we have \(m\leq f^{(n+1)}(t)\leq M\). Since \(x-t\geq 0\), this gives us%
\begin{equation}
m\left(x-t\right)^n\leq f^{(n+1)}(t)(x-t)^n\leq M(x-t)^n\label{eq_LagRem1}
\end{equation}
and so%
\begin{equation}
\int_{t=a}^xm\left(x-t\right)^n\dx{ t}\leq\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\leq \int_{t=a}^xM(x-t)^n\dx{ t}\text{.}\label{eq_LagRem2}
\end{equation}
%
\par
Computing the outside integrals, we have%
\begin{equation*}
m\int_{t=a}^x\left(x-t\right)^n\dx{ t}\leq\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\leq M\int_{t=a}^x(x-t)^n\dx{ t}
\end{equation*}
%
\begin{equation}
m\frac{(x-a)^{n+1}}{n+1}\leq\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\leq M\frac{(x-a)^{n+1}}{n+1}\label{eq_LagRem3}
\end{equation}
%
\begin{equation*}
m\leq\frac{\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}}{\left(\frac{(x-a)^{n+1}}{n+1} \right)}\leq M\text{.}
\end{equation*}
%
\par
Since%
\begin{equation*}
\frac{\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}}{\left(\frac{(x-a)^{n+1}}{n+1} \right)}
\end{equation*}
is a value that lies between the maximum and minimum of \(f^{(n+1)}\) on \([\,a,x]\), then by the \hyperref[IntermediateValueTheorem]{Intermediate Value Theorem}, there must exist a number \(c\in[\,a,x]\) with%
\begin{equation*}
f^{(n+1)}(c)=\frac{\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}}{\left( \frac{(x-a)^{n+1}}{n+1}\right)}\text{.}
\end{equation*}
%
\begin{aside}{Aside}{Comment.}{TaylorSeries-LagrFormRem-4-6}%
We have not mentioned the Intermediate Value Theorem until now and the citation we've given is from \hyperref[IVTandEVT]{Chapter~{\xreffont\ref{IVTandEVT}}} which we have not yet reached. If it feels like we are cheating, good. That means you feel the need to conclusively justify every step in a proof before going on to the next. This is exactly how you should feel.%
\par
However, sometimes the presentation of ideas can be made clearer by taking them out of order. This is such a case. We will return to, and prove, both the Intermediate and Extreme Value Theorems in \hyperref[IVTandEVT]{Chapter~{\xreffont\ref{IVTandEVT}}}.%
\par
So accept them as true for now, but know that this proof is not concluded until the \hyperref[IntermediateValueTheorem]{Intermediate Value Theorem} has also been proved.%
\end{aside}
This gives us%
\begin{equation*}
\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}=\,\frac{f^{\,(n+1)}(c)}{n+1}(x-a)^{n+1}\text{.}
\end{equation*}
%
\par
And the result follows.%
\end{proof}
\begin{problem}{Problem}{}{TaylorSeries-LagrFormRem-5}%
\index{Lagrange's form of the remainder!\(x\lt a\)} Prove \hyperref[thm_LagrangeRemainder]{Theorem~{\xreffont\ref{thm_LagrangeRemainder}}} for the case where \(x\lt a\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{TaylorSeries-LagrFormRem-5-2}{}\quad{}Note that%
\begin{equation*}
\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}=(-1)^{n+1}\int_{t=x}^af^{(n+1)}(t)(t-x)^n\dx{ t}\text{.}
\end{equation*}
%
\par
Use the same argument on this integral.  It will work out in the end.  Really!  You just need to keep track of \emph{all} of the negatives.%
\end{problem}
\index{Lagrange, Joseph-Louis}\index{Extreme Value Theorem (EVT)}\index{Intermediate Value Theorem (IVT)} This is \emph{not} Lagrange's proof.  He did not use the integral form of the remainder.  However, this is similar to Lagrange's proof in that he also used the \hyperref[IntermediateValueTheorem]{Intermediate Value Theorem} (IVT) and \hyperref[thm_EVT]{Extreme Value Theorem} (EVT) much as we did just now.%
\par
\index{Lagrange, Joseph-Louis} In Lagrange's day, these were taken to be obviously true for a continuous function and we have followed Lagrange's lead by assuming the \hyperref[IntermediateValueTheorem]{IVT} and the \hyperref[thm_EVT]{EVT}. However, in mathematics we need to keep our assumptions few and simple.  The \hyperref[IntermediateValueTheorem]{IVT} and the \hyperref[thm_EVT]{EVT} do not satisfy this need in the sense that both can be proved from simpler ideas. We will return to this in \hyperref[IVTandEVT]{Chapter~{\xreffont\ref{IVTandEVT}}}.%
\par
Also, a word of caution about this: Lagrange's form of the remainder is \(\frac{f^{\,(n+1)}(c)}{(n+1)!}\) \((x-a)^{n+1}\), where \(c\) is some number between \(a\) and \(x\).  The proof does not indicate what this \(c\) might be and, in fact, this \(c\) changes as \(n\) changes. All we know is that this \(c\) lies between \(a\) and \(x\).  To illustrate this issue and its potential dangers, consider the following problem where we have a chance to compute the value of \(c\) for the function \(f(x)=\frac{1}{1+x}\).%
\begin{problem}{Problem}{}{TaylorSeries-LagrFormRem-9}%
This problem investigates the Taylor series representation%
\begin{equation*}
\frac{1}{1+x}=1-x+x^2-x^3+\cdots\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Use the fact that \(\frac{1-(-x)^{n+1}}{1+x}=1-x+x^2-x^3+\cdots+(-x)^n\) to compute the remainder \(\frac{1}{1+x}-\left(1-x+x^2-x^3+\cdots+(-x)^n\right)\). Specifically, compute this remainder when \(x=1\) and conclude that the Taylor series does not converge to \(\frac{1}{1+x}\) when \(x=1\).%
\item{}Compare the remainder in part a with the Lagrange form of the remainder to determine what \(c\) is when \(x=1\).%
\item{}Consider the following argument: If \(f(x)=\frac{1}{1+x}\), then%
\begin{equation*}
f^{(n+1)}(c)=\frac{(-1)^{n+1}(n+1)!}{(1+c)^{n+2}}
\end{equation*}
so the Lagrange form of the remainder when \(x=1\) is given by%
\begin{equation*}
\frac{(-1)^{n+1}(n+1)!}{(n+1)!(1+c)^{n+2}}=\frac{(-1)^{n+1}}{(1+c)^{n+2}}
\end{equation*}
where \(c\in[\,0,1]\).  It can be seen in part b that \(c\neq 0\).  Thus \(1+c>1\) and so by \hyperref[prob_sequences3]{Problem~{\xreffont\ref{prob_sequences3}}} of \hyperref[Convergence]{Chapter~{\xreffont\ref{Convergence}}}, the Lagrange remainder converges to \(0\) as \(n\rightarrow\infty\).  This argument would suggest that the Taylor series converges to \(\frac{1}{1+x}\) for \(x=1\).  However, we know from part (a) that this is incorrect.  What is wrong with the argument?%
\end{enumerate}%
\end{problem}
Even though there are potential dangers in misusing the Lagrange form of the remainder, it is a useful form.  For example, armed with the Lagrange form of the remainder, we can prove the following theorem.%
\begin{theorem}{Theorem}{}{}{thm_BinomialSeriesConverges}%
\index{series!Binomial Series, the}%
\index{Binomial Series, the!converges on the interval \([0,1]\)}%
The binomial series%
\begin{equation*}
1+\frac{1}{2}x+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)}{2!}x^2+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right)}{3!}x^3+\cdots
\end{equation*}
converges to \(\sqrt{1+x}\) for \(x\in[0,1]\).%
\end{theorem}
\begin{proof}{Proof}{}{TaylorSeries-LagrFormRem-12}
First note that the binomial series is, in fact, the Taylor series for the function \(f(x)=\sqrt{1+x}\) expanded about \(a=0\). If we let \(x\) be a fixed number with \(0\leq x\leq 1\), then it suffices to show that the Lagrange form of the remainder converges to \(0\). With this in mind, notice that%
\begin{equation*}
f^{(n+1)}(t)=\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-n\right)\left(1+t\right)^{\frac{1}{2}-(n+1)}
\end{equation*}
and so the Lagrange form of the remainder is%
\begin{equation*}
\frac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}= \frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots \left(\frac{1}{2}-n\right)}{(n+1)!}\frac{x^{n+1}}{(1+c)^{n+\frac{1}{2}}}
\end{equation*}
where \(c\) is some number between \(0\) and \(x\). Since \(0\leq x\leq 1\) and \(1+c\geq 1\), then we have \(\frac{1}{1+c}\leq 1\), and so%
\begin{align*}
0\amp \leq \left|\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-n\right)}{(n+1)!}\frac{x^{n+1}}{(1+c)^{n+\frac{1}{2}}}\right|\\
\amp =\frac{\left(\frac{1}{2}\right)\left(1-\frac{1}{2}\right)\cdots\left(n-\frac{1}{2}\right)}{(n+1)!}\frac{x^{n+1}}{(1+c)^{n+\frac{1}{2}}}\\
\amp =\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)\left(\frac{5}{2}\right)\cdots\left(\frac{2n-1}{2}\right)}{(n+1)!}\left(x^{n+1}\right)\frac{1}{(1+c)^{n+\frac{1}{2}}}\\
\amp \leq\frac{1\cdot 1\cdot 3\cdot5\cdot\,\cdots\,\cdot\left(2n-1\right)}{2^{^{n+1}}(n+1)!}\\
\amp =\frac{1\cdot 3\cdot 5\cdot\,\cdots\,\cdot\left(2n-1\right)\cdot 1}{2\cdot4\cdot 6\cdot\,\cdots\,\cdot 2n\cdot\left(2n+2\right)}\\
\amp =\frac{1}{2}\cdot\frac{3}{4}\cdot\frac{5}{6}\cdot\cdots\,\cdot\frac{2n-1}{2n}\cdot\frac{1}{2n+2}\\
\amp \leq\frac{1}{2n+2}\text{.}
\end{align*}
%
\par
Since \(\limitt{n}{\infty}{\frac{1}{2n+2}}=0=\limitt{n}{\infty}{0}\), then by the Squeeze Theorem,%
\begin{equation*}
\lim_{n\rightarrow\infty}\abs{\frac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}}=0, \text{ so } \lim_{n\rightarrow\infty}\left(\frac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}\right)=0\text{.}
\end{equation*}
%
\par
Thus the Taylor series%
\begin{equation*}
1+\frac{1}{2}x+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)}{2!}x^2+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right)}{3!}x^3+\cdots
\end{equation*}
converges to \(\sqrt{1+x}\) for \(0\leq x\leq 1\).%
\end{proof}
Unfortunately, this proof will not work for \(-1\lt x\lt 0\). In this case, the fact that \(x\leq c\leq 0\) makes\(\,1+c\leq 1\). Thus \(\frac{1}{1+c}\geq 1\) and so the inequality%
\begin{equation*}
\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)\left(\frac{5}{2}\right)\cdots\left(\frac{2n-1}{2}\right)}{(n+1)!}\frac{|x|^{n+1}}{(1+c)^{n+\frac{1}{2}}}\leq\frac{1\cdot 1\cdot 3\cdot 5\cdot\,\cdots\,\cdot\left(2n-1\right)}{2^{^{n+1}}(n+1)!}
\end{equation*}
may not hold.%
\begin{problem}{Problem}{}{prob_Taylor_Series-Binomial_Series_and}%
\index{series!Binomial Series, the!Binomial Series is a Taylor series} Show that if \(-\frac{1}{2}\leq x\leq c\leq 0\), then \(|\frac{x}{1+c}|\leq 1\) and modify the above proof to show that the binomial series converges to \(\sqrt{1+x}\) for \(x\in\left[-\frac{1}{2},0\right]\).%
\end{problem}
To take care of the case where \(-1\lt x\lt -\frac{1}{2}\), we will use yet another form of the remainder for Taylor series. However before we tackle that, we will use the Lagrange form of the remainder to address something mentioned in \hyperref[PowerSeriesQuestions]{Chapter~{\xreffont\ref{PowerSeriesQuestions}}}. Recall that we noticed that the series representation%
\begin{equation*}
\frac{1}{1+x}=1-x+x^2-x^3+\cdots
\end{equation*}
did not work when \(x=1\), however we noticed that the series obtained by integrating term by term did seem to converge to the antiderivative of \(\frac{1}{1+x}\). Specifically, we have the Taylor series%
\begin{equation*}
\ln\left(1+x\right)=x-\frac{1}{2}x^2+\frac{1}{3}x^3-\cdots\text{.}
\end{equation*}
%
\par
Substituting \(x=1\) into this provided the convergent series \(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots\). We made the claim that this, in fact, converges to \(\ln 2\), but that this was not obvious. The Lagrange form of the remainder gives us the machinery to prove this.%
\begin{problem}{Problem}{}{TaylorSeries-LagrFormRem-17}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Compute the Lagrange form of the remainder for the Maclaurin series for \(\ln\left(1+x\right)\).%
\item{}Show that when \(x=1\), the Lagrange form of the remainder converges to \(0\) and so the equation \(\ln 2=1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots\) is actually correct.%
\end{enumerate}%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 7.3 Cauchy's Form of the Remainder}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Cauchy's Form of the Remainder}{}{Cauchy's Form of the Remainder}{}{}{TaylorSeries-CauchyFormRem}
In his 1823 work, \textit{Rsume des leons donnes  l'ecole royale polytechnique sur le calcul infintsimal,} Augustin Cauchy \index{Cauchy, Augustin} provided another form of the remainder for Taylor series.%
\begin{theorem}{Theorem}{}{}{thm_CauchyRemainder}%
\terminology{Cauchy's Form of the Remainder}%
\par
\index{sequences!Cauchy sequences!Cauchy's remainder} Suppose \(f\) is a function such that \(f^{(n+1)}(t)\) is continuous on an interval containing \(a\) and \(x\). Then%
\begin{equation*}
f(x)-\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)=\frac{f^{\, (n+1)}(c)}{n!}(x-c)^n(x-a)
\end{equation*}
where \(c\) is some number between \(a\) and \(x\).%
\end{theorem}
\begin{problem}{Problem}{}{TaylorSeries-CauchyFormRem-4}%
\index{series!Taylor's series!Cauchy Remainder} Prove \hyperref[thm_CauchyRemainder]{Theorem~{\xreffont\ref{thm_CauchyRemainder}}} using an argument similar to the one used in the proof of \hyperref[thm_LagrangeRemainder]{Theorem~{\xreffont\ref{thm_LagrangeRemainder}}}. Don't forget there are two cases to consider.%
\end{problem}
Using Cauchy's form of the remainder, we can prove that the binomial series%
\begin{equation*}
1+\frac{1}{2}x+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)}{2!}x^2+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right)}{3!}x^3+\cdots
\end{equation*}
converges to \(\sqrt{1+x}\) for \(x\in(-1,0).\)%
\begin{aside}{Aside}{Comment.}{TaylorSeries-CauchyFormRem-6}%
Strictly speaking we only need to show this for \(x\in(-1,-1/2).\)In \hyperref[prob_Taylor_Series-Binomial_Series_and]{Problem~{\xreffont\ref{prob_Taylor_Series-Binomial_Series_and}}} we covered\(x\in (-1/2,0)\).%
\end{aside}
With this in mind, let \(x\) be a fixed number with \(-1\lt x\lt 0\) and consider that the binomial series is the Maclaurin series for the function \(f(x)=(1+x)^{\frac{1}{2}}\). As we saw before,%
\begin{equation*}
f^{(n+1)}(t)=\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-n\right)\left(1+t\right)^{\frac{1}{2}-(n+1)}\text{,}
\end{equation*}
so the Cauchy form of the remainder is given by%
\begin{equation*}
0\le\abs{\frac{f^{(n+1)}(c)}{n!}(x-c)^n(x-0)}= \abs{\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-n\right)}{n!}\frac{(x-c)^n}{(1+c)^{n+\frac{1}{2}}}\cdot x}
\end{equation*}
where \(c\) is some number with \(x\leq c\leq 0\). Thus we have%
\begin{align*}
0\amp \leq\left|\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-n\right)}{n!}\frac{(x-c)^nx}{(1+c)^{n+\frac{1}{2}}}\right|\\
\amp =\frac{\left(\frac{1}{2}\right)\left(1-\frac{1}{2}\right)\cdots\left(n-\frac{1}{2}\right)}{n!}\frac{|x-c|^n|x|}{(1+c)^{n+\frac{1}{2}}}\\
\amp =\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)\left(\frac{5}{2}\right)\cdots\left(\frac{2n-1}{2}\right)}{n!}\frac{(c-x)^n}{(1+c)^n}\frac{|\,x|}{\sqrt{1+c}}\\
\amp =\frac{1\cdot 1\cdot 3\cdot 5\cdot\,\cdots\,\cdot\left(2n-1\right)}{2^{^{n+1}}n!}\left(\frac{c-x}{1+c}\right)^n\frac{|\,x|}{\sqrt{1+c}}\\
\amp =\frac{1\cdot 1\cdot 3\cdot 5\cdot\,\cdots\,\cdot\left(2n-1\right)}{2\cdot 2\cdot 4\cdot 6\cdot\,\cdots\,\cdot 2n}\left(\frac{c-x}{1+c}\right)^n\frac{|\,x|}{\sqrt{1+c}}\\
\amp =\frac{1}{2}\cdot\frac{1}{2}\cdot\frac{3}{4}\cdot\frac{5}{6}\cdot\cdots\,\cdot\frac{2n-1}{2n}\cdot\left(\frac{c-x}{1+c}\right)^n\frac{|\,x|}{\sqrt{1+c}}\\
\amp \leq\left(\frac{c-x}{1+c}\right)^n\frac{|\,x|}{\sqrt{1+c}}\text{.}
\end{align*}
%
\par
Notice that if \(-1\lt x\leq c\),\(\) then \(0\lt 1+x\leq 1+c\). Thus \(0\lt \frac{1}{1+c}\leq\frac{1}{1+x}\) and \(\frac{1}{\sqrt{1+c}}\leq\frac{1}{\sqrt{1+x}}\). Thus we have%
\begin{equation*}
0\leq\left|\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-n\right)}{n!}\frac{(x-c)^nx}{(1+c)^{n+\frac{1}{2}}}\right|\leq\left(\frac{c-x}{1+c}\right)^n\frac{|\,x|}{\sqrt{1+x}}\text{.}
\end{equation*}
%
\begin{problem}{Problem}{}{TaylorSeries-CauchyFormRem-9}%
\index{Binomial Series, the!\(g(c)=\frac{c-x}{1+c}\) is increasing}\index{\(g(c)=\frac{c-x}{1+c}\) is increasing on \([x,0]\)} Suppose \(-1\lt x\leq c\leq 0\) and consider the function \(g(c)=\frac{c-x}{1+c}\). Show that on \([x,0]\), \(g\) is increasing and use this to conclude that for \(-1\lt x\leq c\leq 0\),%
\begin{equation*}
\frac{c-x}{1+c}\leq|x|\text{.}
\end{equation*}
%
\par
Use this fact to finish the proof that the binomial series converges to \(\sqrt{1+x}\) for \(-1\lt x\lt 0\).%
\end{problem}
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Cauchy/}{Augustin Cauchy}\protect\footnotemark{} (1789\textendash{}1857)}{TaylorSeries-CauchyFormRem-10}{}%
\index{Cauchy, Augustin!portrait of}%
\begin{image}{0.36}{0.28}{0.36}{}%
\includegraphics[width=\linewidth]{external/images/Cauchy.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[15]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Cauchy/}\label{TaylorSeries-CauchyFormRem-10-1-2}}%
The proofs of both the Lagrange form and the Cauchy form of the remainder for Taylor series made use of two crucial facts about continuous functions.  First, we assumed the \hyperref[thm_EVT]{Extreme Value Theorem}: Any continuous function on a closed bounded interval assumes its maximum and minimum somewhere on the interval. Second, we assumed that any continuous function satisfied the \hyperref[IntermediateValueTheorem]{Intermediate Value Theorem}: If a continuous function takes on two different values, then it must take on any value between those two values.%
\par
Mathematicians in the late \(1700\)s and early \(1800\)s typically considered these facts to be intuitively obvious.  This was natural since our understanding of continuity at that time was, solely, intuitive.  Intuition is a useful tool, but as we have seen before it is also unreliable.  For example consider the following function.%
\begin{align}
f(x)= 
\begin{cases}
x\sin\left(\frac{1}{x}\right),\amp \text{if } x\neq 0,\\
0, \amp \text{ if } x=0 
\end{cases} \text{.}\label{xsin1x}
\end{align}
%
\par
Is this function continuous at 0?  Near zero its graph looks like this:%
\begin{figureptx}{Figure}{\hyperref[xsin1x]{Formula~({\xreffont\ref{xsin1x}})} near \(0\).}{TaylorSeries-CauchyFormRem-14}{}%
\begin{image}{0}{1}{0}{}%
\includegraphics[width=\linewidth]{external/images/Ch5fig4.png}
\end{image}%
\tcblower
\end{figureptx}%
But this graph must be taken with a grain of salt as \(\sin
\left(\frac{1}{x}\right)\) oscillates infinitely often as \(x\) nears zero.%
\par
No matter what your guess may be, it is clear that it is hard to analyze such a function armed with only an intuitive notion of continuity.  We will revisit this example in the next chapter.%
\par
As with convergence, continuity is more subtle than it first appears.%
\par
We put convergence on solid ground by providing a completely analytic definition in the previous chapter.  What we need to do in the next chapter is provide a completely rigorous definition for continuity.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 7.4 Additional Problems}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Additional Problems}{}{Additional Problems}{}{}{TaylorSeries-AddProb}
\begin{problem}{Problem}{}{TaylorSeries-AddProb-2}%
Find the Integral form, Lagrange form, and Cauchy form of the remainder for Taylor series for the following functions expanded about the given values of \(\,a\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(f(x)=e^x\), \(a=0\)%
\item{}\(f(x)=\sqrt{x}\), \(a=1\)%
\item{}\(f(x)=(1+x)^\alpha\), \(a=0\)%
\item{}\(f(x)=\frac{1}{x}\), \(a=3\)%
\item{}\(f(x)=\ln x,\ a=2\)%
\item{}\(f(x)=\cos x, a=\frac{\pi}{2}\)%
\end{enumerate}%
\end{problem}
\end{sectionptx}
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 8 Continuity: What It Isn't and What It Is}
\typeout{************************************************}
%
\begin{chapterptx}{Chapter}{Continuity: What It Isn't and What It Is}{}{Continuity: What It Isn't and What It Is}{}{}{Continuity}
\renewcommand*{\chaptername}{Chapter}
%
%
\typeout{************************************************}
\typeout{Section 8.1 An Analytic Definition of Continuity}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{An Analytic Definition of Continuity}{}{An Analytic Definition of Continuity}{}{}{Continuity-AnalyticDef}
Before the invention of Calculus, the notion of continuity was treated intuitively if it was treated at all.  At first pass, it seems a very simple idea based solidly in our experience of the real world.  Standing on the bank we see a river flow past us continuously, not by tiny jerks.  Even when the flow might seem at first to be discontinuous, as when it drops precipitously over a cliff, a closer examination shows that it really is not. As the water approaches the cliff it speeds up.  When it finally goes over it accelerates very quickly but no matter how fast it goes it moves continuously, moving from here to there by occupying every point in between.  This is continuous motion. It never disappears over there and instantaneously reappears over here.  That would be discontinuous motion.%
\par
Similarly, a thrown stone flies continuously (and smoothly) from release point to landing point, passing through each point in its path.%
\par
But wait.%
\par
If the stone passes through discrete points it must be doing so by teeny tiny little jerks, mustn't it?  Otherwise how would it get from one point to the next?  Is it possible that motion in the real world, much like motion in a movie, is really composed of tiny jerks from one point to the next but that these tiny jerks are simply too small and too fast for our senses to detect?%
\par
If so, then the real world is more like the rational number line (\(\QQ\)) from \hyperref[NumbersRealRational]{Chapter~{\xreffont\ref{NumbersRealRational}}} than the real number line (\(\RR\)).  In that case, motion really consists of jumping discretely over the ``missing'' points (like \(\sqrt{2}\)) as we move from here to there. That may seem like a bizarre idea to you \textemdash{} it does to us as well \textemdash{} but the idea of continuous motion is equally bizarre.  It's just a little harder to see why since it is so familiar.%
\par
The real world will be what it is regardless of what we believe it to be, but fortunately in mathematics we are not constrained to live in it.  So we won't even try.  We will simply postulate that no such jerkiness exists; that all motion is continuous.%
\par
However we \emph{are} constrained to live with the logical consequences of our assumptions, once they are made.  These will lead us into some very deep waters indeed.%
\par
The intuitive treatment of continuity was maintained throughout the 1700's as it was not generally perceived that a truly rigorous definition was necessary.  Consider the following definition given by Euler in 1748.%
\begin{quote}%
A continuous curve is one such that its nature can be expressed by a single function of \(x.\) If a curve is of such a nature that for its various parts . . . different functions of \(x\) are required for its expression, . . . , then we call such a curve discontinuous.%
\end{quote}
However, the complexities associated with Fourier series and the types of functions that they represented caused mathematicians in the early \(1800\)s to rethink their notions of continuity.  As we saw in \hyperref[Interregnum]{Part~{\xreffont\ref{Interregnum}}}, the graph of the function defined by the Fourier series%
\begin{equation*}
\frac{4}{\pi}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)} \cos \left(\left(2k+1\right)\pi x\right)
\end{equation*}
looked like this:%
\begin{figureptx}{Figure}{}{Continuity-AnalyticDef-12}{}%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/Ch5fig1.png}
\end{image}%
\tcblower
\end{figureptx}%
This function went against Euler's notion of what continuous functions should be.  Here, an infinite sum of continuous cosine curves provided a single expression which resulted in a ``discontinuous'' curve.  But as we've seen this didn't happen with power series and an intuitive notion of continuity is inadequate to explain the difference.  Even more perplexing is the following situation.  Intuitively, one would think that a continuous curve should have a tangent line at at least one point.  It may have a number of jagged points to it, but it should be ``smooth'' somewhere.  An example of this would be \(f(x)=x^{2/3}\).  Its graph is given by%
\begin{figureptx}{Figure}{}{Continuity-AnalyticDef-14}{}%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/Ch5fig2.png}
\end{image}%
\tcblower
\end{figureptx}%
This function is not differentiable at the origin but it is differentiable everywhere else.  One could certainly come up with examples of functions which fail to be differentiable at any number of points but, intuitively, it would be reasonable to expect that a continuous function should be differentiable \emph{somewhere}.  We might conjecture the following:%
\begin{conjecture}{Conjecture}{}{}{conj_ContImplyDiff}%
If \(f\) is continuous on an interval \(I\) then there is some \(a\in I\), such that \(f^\prime(a)\) exists.%
\end{conjecture}
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Weierstrass/}{Karl Weierstrass}\protect\footnotemark{} (1815\textendash{}1897)}{Continuity-AnalyticDef-17}{}%
\index{Weierstrass, Karl!portrait of}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Weierstrass.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[16]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Weierstrass/}\label{Continuity-AnalyticDef-17-2-2}}%
Surprisingly, in \(1872\), Karl Weierstrass \index{Weierstrass, Karl} showed that the above conjecture is \alert{FALSE}. He did this by displaying the counterexample:%
\begin{equation*}
f(x)=\sum_{n=0}^\infty b^n\cos(a^n\pi x)\text{.}
\end{equation*}
%
\par
Weierstrass showed that if \(a\) is an odd integer, \(b\in(0,1)\), and \(ab>1+\frac{3}{2}\pi\), then \(f\) is continuous everywhere, but is nowhere differentiable.  Such a function is somewhat ``fractal'' in nature, and it is clear that a definition of continuity relying on intuition is inadequate to study it.%
\begin{problem}{Problem}{}{Continuity-AnalyticDef-20}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Given \(f(x)=\sum_{n=0}^\infty\left(\frac{1}{2}\right)^n\cos\left(a^n\pi
x\right)\), what is the smallest value of \(a\) for which \(f\) satisfies Weierstrass' criterion to be continuous and nowhere differentiable.%
\item{}Let \(f(x,N)=\sum_{n=0}^N\left(\frac{1}{2}\right)^n\cos\left(13^n\pi
x\right)\) and use a computer algebra system to plot \(f(x,N)\) for \(N=0,1,2,3,4,10\) and \(x\in[0,1]\).%
\item{}Plot \(f(x,10)\) for \(x\in[\,0,c]\), where \(c=0.1,0.01,0.001,0.0001,0.00001\).  Based upon what you see in parts b and c, why would we describe the function to be somewhat ``fractal'' in nature?%
\end{enumerate}%
\end{problem}
Just as it was important to define convergence with a rigorous definition without appealing to intuition or geometric representations, it is imperative that we define continuity in a rigorous fashion not relying on graphs.%
\par
The first appearance of a definition of continuity which did not rely on geometry or intuition was given in 1817 by Bernhard Bolzano \index{Bolzano, Bernhard} in a paper published in the Proceedings of the Prague Scientific Society entitled \textit{Rein analytischer Beweis des Lehrsatzes dass zwieschen je zwey Werthen, die ein entgegengesetztes Resultat gewaehren, wenigstens eine reele Wurzel der Gleichung liege} (Purely Analytic Proof of the Theorem that Between Any Two Values that Yield Results of Opposite Sign There Will be at Least One Real Root of the Equation).%
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Bolzano/}{Bernhard Bolzano}\protect\footnotemark{}(1781\textendash{}1848)}{Continuity-AnalyticDef-23}{}%
\index{Bolzano, Bernhard!portrait of}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Bolzano.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[17]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Bolzano/}\label{Continuity-AnalyticDef-23-1-2}}%
From the title it should be clear that in this paper Bolzano is proving the \hyperref[IntermediateValueTheorem]{Intermediate Value Theorem}.  To do this he needs a completely analytic definition of continuity.  The substance of Bolzano's idea is that if \(f\) is continuous at a point \(a\) then \(f(x)\) should be ``close to'' \(f(a)\) whenever \(x\) is ``close enough to'' \(a\).  More precisely, Bolzano said that \(f\) is continuous at \(a\) provided \(\abs{f(x)-f(a)}\) can be made smaller than any given quantity provided we make \(\abs{x-a}\) sufficiently small.%
\par
The language Bolzano uses is very similar to the language Leibniz \index{Leibniz, Gottfried Wilhelm} used when he postulated the existence of infinitesimally small numbers. Leibniz said that infinitesimals are ``smaller than any given quantity but not zero.'' Bolzano says that ``\(\abs{f(x)-f(a)}\) can be made smaller than any given quantity provided we make \(\abs{x-a}\) sufficiently small.'' But Bolzano stops short of saying that \(\abs{x-a}\) is \emph{infinitesimally} small. He says that given \(a\), we can choose \(x\) so that \(\abs{x-a}\) is smaller than any real number we could name, say \(b\), provided we name \(b\) \emph{first}. But for any given choice of \(x\), \(\abs{x-a}\), and \(b\) are both still real numbers.  Possibly very small real numbers to be sure, but real numbers nonetheless.  Infinitesimals have no place in Bolzano's construction.%
\par
\index{Bolzano, Bernhard} Bolzano's paper was not well known when Cauchy \index{Cauchy, Augustin} proposed a similar definition in his \emph{Cours d'analyse}~\hyperlink{bradley09__cauch_cours}{[{\xreffont 1}]} of 1821 so it is usually Cauchy who is credited with this definition, but even Cauchy's definition is not quite tight enough for modern standards.  It was Karl Weierstrass in 1859 who finally gave the modern definition.%
\begin{definition}{Definition}{Continuity at a Point.}{def_continuity}%
\index{continuity!definition of}\index{continuity} We say that a function \(\boldsymbol{f}\) is continuous at \(\boldsymbol{a}\) provided that for any \(\eps>0\), there exists a \(\delta>0\) such that if \(\abs{x-a}\lt
\delta\) then \(|f(x)-f(a)|\lt \eps\).%
\end{definition}
Notice that the definition of continuity of a function is done point-by-point.  A function can certainly be continuous at some points while discontinuous at others.  When we say that \(f\) is continuous on an interval, then we mean that it is continuous at every point of that interval and, in theory, we would need to use the above definition to check continuity at each individual point.%
\par
\index{Extreme Value Theorem (EVT)!continuity and}\index{continuity!Extreme Value Theorem (EVT) and}\index{Intermediate Value Theorem (IVT)!continuity and}\index{continuity!Intermediate Value Theorem and} Our definition fits the bill in that it does not rely on either intuition or graphs, but it is this very non-intuitiveness that makes it hard to grasp.  It usually takes some time to become comfortable with this definition, let alone use it to prove theorems such as the \hyperref[thm_EVT]{Extreme Value Theorem} and \hyperref[IntermediateValueTheorem]{Intermediate Value Theorem}.  So let's go slowly to develop a feel for it.%
\par
This definition spells out a completely black and white procedure: you give me a positive number \(\eps\), and I must be able to find a positive number \(\delta\) which satisfies a certain property.  If I can always do that then the function is continuous at the point of interest.%
\par
This definition also makes very precise what we mean when we say that \(f(x)\) should be ``close to'' \(f(a)\) whenever \(x\) is ``close enough to'' \(a\).  For example, intuitively we know that \(f(x)=x^2\) should be continuous at \(x=2\).  This means that we should be able to get \(x^2\) to within, say, \(\eps=0.1\) of \(4\) provided we make \(x\) close enough to \(2\).  Specifically, we want \(3.9\lt x^2\lt 4.1\).  This happens exactly when \(\sqrt{3.9}\lt x\lt \sqrt{4.1}\).  Using the fact that \(\sqrt{3.9}\lt 1.98\) and \(2.02\lt \sqrt{4.1}\), then we can see that if we get \(x\) to within \(\delta=.02\) of \(2\), then \(\sqrt{3.9}\lt 1.98\lt x\lt 2.02\lt
\sqrt{4.1}\) and so \(x^2\) will be within .\(1\) of \(\,4\).  This is very straightforward.  What makes this situation more difficult is that we must be able to do this for any \(\eps>0\).%
\par
Notice the similarity between this definition and the definition of convergence of a sequence.  Both definitions have the challenge of an \(\eps>0\).  In the definition of \(\limit{n}{\infty}{s_n}=s\), we had to get \(s_n\) to within \(\eps\) of \(s\) by making \(n\) large enough.  For sequences, the challenge lies in making \(\abs{s_n-s}\) sufficiently small.  More precisely, given \(\eps>0\) we need to decide how large \(n\) should be to guarantee that \(\abs{s_n-s}\lt \eps\).%
\par
In our definition of continuity, we still need to make something small (namely \(\abs{f(x)-f(a)}\lt \eps\)), only this time, we need to determine how close \(x\) must be to \(a\) to ensure this will happen instead of determining how large \(n\) must be.%
\par
What makes \(f\) continuous at \(a\) is the arbitrary nature of \(\eps\) (as long as it is positive).  As \(\eps\) becomes smaller, this forces \(f(x)\) to be closer to \(f(a)\).  That we can always find a positive distance \(\delta\) to work is what we mean when we say that we can make \(f(x)\) as close to \(f(a)\) as we wish, provided we get \(x\) close enough to \(a\).  The sequence of pictures below illustrates that the phrase ``for any \(\eps>0\), there exists a \(\delta>0\) such that if \(|\,x-a|\lt \delta\) then \(|f(x)-f(a)|\lt \eps\)'' can be replaced by the equivalent formulation ``for any \(\eps>0\), there exists a \(\delta>0\) such that if \(a-\delta\lt x\lt a+\delta\) then \(f(a)-\eps\lt f(x)\lt
f(a)+\eps\).'' This could also be replaced by the phrase ``for any \(\eps>0\), there exists a \(\delta>0\) such that if \(x\in(a-\delta,a+\delta)\) then \(f(x)\in(f(a)-\eps,f(a)+\eps)\).'' All of these equivalent formulations convey the idea that we can get \(f(x)\) to within \(\eps\) of \(f(a)\), provided we make \(x\) within \(\delta\) of \(a\), and we will use whichever formulation suits our needs in a particular application.%
\begin{sidebyside}{2}{0.025}{0.025}{0.05}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{external/images/Ch5fig3a.png}
\end{sbspanel}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{external/images/Ch5fig3b.png}
\end{sbspanel}%
\end{sidebyside}%
\begin{sidebyside}{2}{0.025}{0.025}{0.05}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{external/images/Ch5fig3c.png}
\end{sbspanel}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{external/images/Ch5fig3d.png}
\end{sbspanel}%
\end{sidebyside}%
The precision of the definition is what allows us to examine continuity without relying on pictures or vague notions such as ``nearness'' or ``getting closer to.'' We will now consider some examples to illustrate this precision.%
\begin{example}{Example}{}{Continuity-AnalyticDef-37}%
Use the definition of continuity to show that \(f(x)=x\) is continuous at any point \(a\).%
\end{example}
If we were to draw the graph of this function, then you would likely say that this is obvious.  The point behind the definition is that we can back up your intuition in a rigorous manner.%
\begin{proof}{Proof}{}{Continuity-AnalyticDef-39}
Let \(\eps>0\). Let \(\delta=\eps\). If \(|\,x-a|\lt \delta\), then%
\begin{equation*}
|f(x)-f(a)|=|\,x-a|\lt \eps
\end{equation*}
%
\par
Thus by the definition, \(f\) is continuous at \(a\).%
\end{proof}
\begin{problem}{Problem}{}{Continuity-AnalyticDef-40}%
\index{continuity!\(f(x) = mx +b\) is continuous everywhere} Use the definition of continuity to show that if \(m\) and \(b\) are fixed (but unspecified) real numbers then the function%
\begin{equation*}
f(x) = mx+b
\end{equation*}
is continuous at every real number \(a\).%
\end{problem}
\begin{example}{Example}{}{Continuity-AnalyticDef-41}%
Use the definition of continuity to show that \(f(x)=x^2\) is continuous at \(a=0\).%
\end{example}
\begin{proof}{Proof}{}{Continuity-AnalyticDef-42}
Let \(\eps>0\). Let \(\delta=\sqrt{\eps}\). If \(|\,x-0|\lt \delta\), then \(|\,x|\lt \sqrt{\eps}\). Thus%
\begin{equation*}
\abs{x^2-0^2}=|\,x|^2\lt \left(\sqrt{\eps}\right)^2=\eps\text{.}
\end{equation*}
%
\par
Thus by the definition, \(f\) is continuous at \(0\).%
\end{proof}
Notice that in these proofs, the challenge of an \(\eps>0\) was given first.  This is because the choice of \(\delta\) must depend upon \(\eps\).  Also notice that there was no explanation for our choice of \(\delta\).  We just supplied it and showed that it worked.  As long as \(\delta>0\), then this is all that is required.  In point of fact, the \(\delta\) we chose in each example was not the only choice that worked; any smaller \(\delta\) would work as well.%
\begin{problem}{Problem}{}{Continuity-AnalyticDef-44}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Given a particular \(\eps>0\) in the definition of continuity, show that if a particular \(\delta_0>0\) satisfies the definition, then any \(\delta\) with \(0\lt \delta\lt \delta_0\) will also work for this \(\eps\).%
\item{}Show that if a \(\delta\) can be found to satisfy the conditions of the definition of continuity for a particular \(\eps_0>0\), then this \(\delta\) will also work for any \(\,\eps\) with \(0\lt \eps_0\lt
\eps\).%
\end{enumerate}%
\end{problem}
It wasn't explicitly stated in the definition but when we say ``if \(\abs{x-a}\lt \delta\) then \(|f(x)-f(a)|\lt
\eps\),'' we should be restricting ourselves to \(x\) values which are in the domain of the function \(f\), otherwise \(f(x)\) doesn't make sense.  We didn't put it in the definition because that definition was complicated enough without this technicality.  Also in the above examples, the functions were defined everywhere so it was a moot point.  We will continue with the convention that when we say ``if \(|\,x-a|\lt \delta\) then \(|f(x)-f(a)|\lt \eps\),'' we will be restricting ourselves to \(x\) values which are in the domain of the function \(f\).  This will allow us to examine continuity of functions not defined for all \(x\) without restating this restriction each time.%
\begin{problem}{Problem}{}{prob_extended_sqrt_is_continuous_at_zero}%
\index{continuity!\(\pm\sqrt{x}\) is continuous at zero} Use the definition of continuity to show that%
\begin{equation*}
f(x)= \begin{cases}\sqrt{x} \amp  \text{ if }  x\ge0\\ -\sqrt{-x} \amp  \text{ if }  x\lt 0 \end{cases}
\end{equation*}
is continuous at \(a=0\).%
\end{problem}
\begin{problem}{Problem}{}{Continuity-AnalyticDef-47}%
\index{\(\sqrt{x}\)!is continuous at zero} Use the definition of continuity to show that \(f(x)=
\sqrt{x}\) is continuous at \(a=0\).  How is this problem different from \hyperref[prob_extended_sqrt_is_continuous_at_zero]{Problem~{\xreffont\ref{prob_extended_sqrt_is_continuous_at_zero}}}? How is it similar?%
\end{problem}
Sometimes the \(\delta\) that will work for a particular \(\eps\) is fairly obvious to see, especially after you've gained some experience.  This is the case in the above examples (at least after looking back at the proofs).  However, finding \(\delta\) is usually not so obvious and requires some scrapwork.  This scrapwork is vital toward producing a \(\delta\), but again is not part of the polished proof. This can be seen in the following example.%
\begin{example}{Example}{}{example_SqrtContinuous}%
Use the definition of continuity to prove that \(f(x)=\sqrt{x}\) is continuous at \(a=1\).%
\par
\terminology{SCRAPWORK}%
\par
As before, the scrapwork for these problems often consists of simply working backwards.  Specifically, given an \(\eps>0\), we need to find a \(\delta>0\) so that \(\abs{\sqrt{x}-\sqrt{1}}\lt \eps\), whenever \(\abs{x-1}\lt
\delta\).  We work backwards from what we want, keeping an eye on the fact that we can control the size of \(\abs{x-1}\).%
\begin{equation*}
\abs{\sqrt{x}-\sqrt{1}}=\abs{\frac{\left(\sqrt{x}-1\right)\left(\sqrt{x}+1\right)}{\sqrt{x}+1}|}=\frac{\abs{x-1}}{\sqrt{x}+1}\lt
\abs{x-1}\text{.}
\end{equation*}
%
\par
This seems to suggest that we should make \(\delta=\eps\). We're now ready for the formal proof.%
\end{example}
\begin{proof}{Proof}{}{Continuity-AnalyticDef-50}
Let \(\eps>0\). Let \(\delta=\eps\). If \(\abs{x-1}\lt \delta\), then \(\abs{x-1}\lt \eps\), and so%
\begin{equation*}
\abs{\sqrt{x}-\sqrt{1}}=\abs{\frac{\left(\sqrt{x}-1\right)\left(\sqrt{x}+1\right)}{ \sqrt{x}+1}}=\frac{\abs{x-1}}{\sqrt{x}+1}\lt \abs{x-1}\lt \eps\text{.}
\end{equation*}
%
\par
Thus by definition, \(f(x)=\sqrt{x}\) is continuous at \(1\).%
\end{proof}
Bear in mind that someone reading the formal proof will not have seen the scrapwork, so the choice of \(\delta\) might seem rather mysterious.  However, you are in no way bound to motivate this choice of \(\delta\) and usually you should not, unless it is necessary for the formal proof.  All you have to do is find this \(\delta\) and show that it works.   To a trained reader, your ideas will come through when you demonstrate that your choice of \(\delta\) works.%
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Halmos/}{Paul Halmos}\protect\footnotemark{}}{Continuity-AnalyticDef-52}{}%
\index{Halmos, Paul!portrait of}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Halmos.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[18]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Halmos/}\label{Continuity-AnalyticDef-52-1-2}}%
Now reverse this last statement.  \emph{As} a trained reader, when you read the proof of a theorem it is \emph{your} responsibility to find the scrapwork, to see how the proof works and understand it fully.  As the renowned mathematical expositor Paul Halmos \index{Halmos, Paul} (1916-2006) said,%
\begin{quote}%
``Don't just read it; fight it! Ask your own questions, look for your own examples, discover your own proofs. Is the hypothesis necessary? Is the converse true? What happens in the classical special case? What about the degenerate cases? Where does the proof use the hypothesis?''%
\end{quote}
This is the way to learn mathematics.  It is really the only way.%
\begin{problem}{Problem}{}{Continuity-AnalyticDef-56}%
Use the definition of continuity to show that \(f(x)=\sqrt{x}\) is continuous at any positive real number \(a\).%
\end{problem}
\begin{problem}{Problem}{}{Continuity-AnalyticDef-57}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Use a unit circle to show that for \(0\leq\theta\lt
\frac{\pi}{2}\), \(\sin \theta\leq\theta\) and \(1-\cos \theta\leq\theta\) and conclude \(\abs{\sin
\theta}\leq\abs{\theta}\) and \(\abs{1-\cos
\theta}\leq\abs{\theta}\) for \(-\frac{\pi}{2}\lt
\theta\) \(\lt \frac{\pi}{2}\).%
\item{}Use the definition of continuity to prove that \(f(x)=\sin x\) is continuous at any point \(a\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{Continuity-AnalyticDef-57-3-2}{}\quad{}\(\sin x=\sin\left(x-a+a\right)\).%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{Continuity-AnalyticDef-58}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Use the definition of continuity to show that \(f(x)=e^x\) is continuous at \(a=0\).%
\item{}Show that \(f(x)=e^x\) is continuous at any point \(a\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{Continuity-AnalyticDef-58-4-2}{}\quad{}Rewrite \(e^x-e^a\) as \(e^{a+(x-a)}-e^a\) and use what you proved in part a.%
\end{enumerate}%
\end{problem}
In the above problems, we used the definition of continuity to verify our intuition about the continuity of familiar functions.  The advantage of this analytic definition is that it can be applied when the function is not so intuitive.  Consider, for example, the function given at the end of the last chapter.%
\begin{equation*}
f(x)= \begin{cases}
x\,\sin\left(\frac{1}{x}\right),\amp \text{ if } x\neq 0\\
0, \amp \text{ if } x=0 
\end{cases}  \text{.}
\end{equation*}
%
\par
Near zero, the graph of \(f(x)\) looks like this:%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/Ch5fig4.png}
\end{image}%
As we mentioned in the previous chapter, since sin\(\left(\frac{1}{x}\right)\) oscillates infinitely often as \(x\) nears zero this graph must be viewed with a certain amount of suspicion.  However our completely analytic definition of continuity shows that this function is, in fact, continuous at 0.%
\begin{problem}{Problem}{The Topologist's Sine Function.}{Continuity-AnalyticDef-63}%
\index{Topologist's sine function!is continuous at zero} Use the definition of continuity to show that%
\begin{equation*}
f(x)= \begin{cases}
x\,\sin\left(\frac{1}{x}\right),\amp \text{ if } x\neq 0\\ 
0, \amp \text{ if } x=0 
\end{cases}
\end{equation*}
is continuous at \(0\).%
\end{problem}
Even more perplexing is the function defined by%
\begin{equation*}
D(x)=
\left\{ 
\begin{matrix}
x\text{,} \amp \text{ if } x\text{ is rational } \\
0\text{,} \amp \text{ if } x\text{ is irrational. } 
\end{matrix}
\right.
\end{equation*}
     %
\par
To the naked eye, the graph of this function looks like the lines \(y=0\) and \(y=x\).  Of course, such a graph would not be the graph of a function.  Actually, both of these lines have holes in them.  Wherever there is a point on one line there is a ``hole'' on the other.  Each of these holes is the width of a single point (that is, their ``width'' is zero!) so they are invisible to the naked eye (or even magnified under the most powerful microscope available).  This idea is illustrated in the following graph%
\begin{image}{0.2}{0.6}{0.2}{}%
\includegraphics[width=\linewidth]{external/images/Ch5fig5.png}
\end{image}%
Can such a function so ``full of holes'' actually be continuous anywhere?  It turns out that we can use our definition to show that this function is, in fact, continuous at \(0\) and at no other point.%
\begin{problem}{Problem}{}{Continuity-AnalyticDef-68}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Use the definition of continuity to show that the function%
\begin{equation*}
D(x)= \begin{cases}
x,\amp \text{ if } x\text{ is rational } \\
0,\amp \text{ if } x\text{ is irrational } \end{cases} 
\end{equation*}
is continuous at \(0\).%
\item{}Let \(a\neq 0\).  Use the definition of continuity to show that \(D\) is not continuous at \(a\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{Continuity-AnalyticDef-68-3-2}{}\quad{}You might want to break this up into two cases where \(a\) is rational or irrational.  Show that no choice of \(\delta>0\) will work for \(\eps=|\,a|\).  Note that \hyperref[thm_IrrationalBetweenIrrationals]{Theorem~{\xreffont\ref{thm_IrrationalBetweenIrrationals}}} of \hyperref[NumbersRealRational]{Chapter~{\xreffont\ref{NumbersRealRational}}} will probably help here.%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{Continuity-AnalyticDef-69}%
Use the definition of continuity to prove that the constant function \(g(x)=c\) is continuous at any point \(a\).%
\end{problem}
\begin{problem}{Problem}{}{Continuity-AnalyticDef-70}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Use the definition of continuity to prove that \(\ln x\) is continuous at \(1\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{Continuity-AnalyticDef-70-2-2}{}\quad{}You may want to use the fact \(\abs{\ln x}\lt
\eps\,\Leftrightarrow-\eps\lt \ln x\lt \eps\) to find a \(\delta\).%
\item{}Use part (a) to prove that \(\ln x\) is continuous at any positive real number \(a\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{Continuity-AnalyticDef-70-3-2}{}\quad{}\(\ln(x)=\ln(x/a)+\ln(a)\).  This is a combination of functions which are continuous at \(a\).  Be sure to explain how you know that \(\ln(x/a)\) is continuous at \(a\).%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{Continuity-AnalyticDef-71}%
\index{continuity!formal definition of discontinuity} Write a formal definition of the statement \(f\) is not continuous at \(a\), and use it to prove that the function%
\begin{equation*}
f(x)= 
\begin{cases}
x\amp \text{ if } x\neq 1\\
0\amp \text{if } x=1 
\end{cases}
\end{equation*}
is not continuous at \(a=1\).%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 8.2 Sequences and Continuity}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Sequences and Continuity}{}{Sequences and Continuity}{}{}{SequencesAndContinuity}
There is an alternative way to prove that the function%
\begin{equation*}
D(x)=\left\{ \begin{matrix}x\text{,} \amp \text{ if } x\text{ is rational } \\ 0\text{,} \amp \text{ if } x\text{ is irrational } \end{matrix} \right.
\end{equation*}
is not continuous at \(a\neq 0\).  We will examine this by looking at the relationship between our definitions of convergence and continuity.  The two ideas are actually quite closely connected, as illustrated by the following very useful theorem.%
\begin{theorem}{Theorem}{}{}{thm_LimDefOfContinuity}%
\index{continuity!via limits} The function \(f\) is continuous at \(a\) if and only if \(f\) satisfies the following property:%
\begin{equation*}
\forall\text{ sequences } \left(x_n\right)\text{, if } \,\,\lim_{n\rightarrow\infty}x_n=a \text{ then} \lim_{n\rightarrow\infty}f(x_n)=f(a).{}
\end{equation*}
%
\end{theorem}
\hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}} says that in order for \(f\) to be continuous, it is necessary and sufficient that any sequence \(\left(x_n\right)\) converging to \(a\) must force the sequence \(\left(f(x_n)\right)\) to converge to \(f(a)\).  A picture of this situation is below though, as always, the formal proof will not rely on the diagram.%
\begin{image}{0.315}{0.37}{0.315}{}%
\includegraphics[width=\linewidth]{external/images/Ch5fig6.png}
\end{image}%
This theorem is especially useful for showing that a function \(f,\) is not continuous at a point \(a\); all we need to do is exhibit a sequence \(\left(x_n\right)\) converging to \(a\) such that the sequence \(\lim_{n\rightarrow\infty}f(x_n)\) does \emph{not} converge to \(f(a)\).  Let's demonstrate this idea before we tackle the proof of \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}}.%
\begin{example}{Example}{}{example_HeavisideNotContinuous}%
Use \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}} to prove that%
\begin{equation*}
f(x)= \begin{cases}\frac{|x|}{x}\text{,} \amp \text{ if } x\neq 0\\ 0\text{,} \amp \text{ if } x=0 \end{cases}
\end{equation*}
is not continuous at \(0\).%
\end{example}
\begin{proof}{Proof}{}{SequencesAndContinuity-8}
First notice that \(f\) can be written as%
\begin{equation*}
f(x)= \begin{cases}1\amp \text{ if } x>0\\ -1\amp \text{ if } x\lt 0\\ 0\amp \text{ if } x=0 \end{cases} \text{.}
\end{equation*}
%
\par
To show that \(f\) is not continuous at \(0\), all we need to do is create a single sequence \(\left(x_n\right)\) which converges to \(0\), but for which the sequence \(\left(f\left(x_n\right)\right)\) does not converge to \(f(0)=0\).  For a function like this one, just about any sequence will do, but let's use \(\left(\frac{1}{n}\right)\), just because it is an old familiar friend.%
\par
We have \(\displaystyle\lim_{n\rightarrow\infty}\frac{1}{n}=0\), but \(\displaystyle\lim_{n\rightarrow\infty}f\left(\frac{1}{n}\right)=\lim_{n\rightarrow
\infty}1=1\neq 0=f(0)\).  Thus by \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}}, \(f\) is not continuous at \(0\).%
\end{proof}
\begin{problem}{Problem}{}{SequencesAndContinuity-9}%
\index{continuity!Heaviside's function is not continuous at zero} Use \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}} to show that%
\begin{equation*}
f(x)= \begin{cases}
\frac{\abs{x}}{x},\amp \text{ if } x\neq 0\\
a, \amp \text{ if } x=0 \end{cases}  
\end{equation*}
is not continuous at \(0\), no matter what value \(a\) is.%
\end{problem}
\begin{problem}{Problem}{}{SequencesAndContinuity-10}%
Use \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}} to show that%
\begin{equation*}
D(x)= \begin{cases}
x, \amp \text{ if } x\text{ is rational } \\
0, \amp \text{ if } x\text{ is irrational } \end{cases}  
\end{equation*}
is not continuous at \(a\neq 0\).%
\end{problem}
\begin{problem}{Problem}{}{SequencesAndContinuity-11}%
The function \(T(x)=\sin\left(\frac{1}{x}\right)\) is often called the topologist's sine curve.  Whereas \(\sin
x\) has roots at \(n\pi\), \(n\in\ZZ\) and oscillates infinitely often as \(x\rightarrow\pm\infty\), \(T\) has roots at \(\frac{1}{n\pi},\,n\in\ZZ,\,n\neq
0\), and oscillates infinitely often as \(x\) approaches zero.  A rendition of the graph follows.%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/Ch5fig7.png}
\end{image}%
Notice that \(T\) is not even defined at \(x=0\). We can extend \(T\) to be defined at 0 by simply choosing a value for \(T(0):\)%
\begin{equation*}
T(x)= \begin{cases}
\sin\left(\frac{1}{x}\right),\amp \text{ if } x\neq 0\\ 
b,\amp \text{ if } x=0 \end{cases} \text{.}
\end{equation*}
%
\par
Use \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}} to show that \(T\) is not continuous at \(0\), no matter what value is chosen for \(b\).%
\end{problem}
\begin{proof}{Proof}{Sketch of the Proof of Theorem~{\xreffont\ref*{thm_LimDefOfContinuity}}.}{SequencesAndContinuity-12}
We've seen how we can use \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}}, now we need to prove \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}}.  The forward direction is fairly straightforward.  So we assume that \(f\) is continuous at \(a\) and start with a sequence \(\left(x_n\right)\) which converges to \(a\). What is left to show is that \(\lim_{n\rightarrow\infty}f(x_n)=f(a)\).  If you write down the definitions of \(f\) being continuous at \(a\), \(\lim_{n\rightarrow\infty}x_n=a\), and \(\lim_{n\rightarrow\infty}f(x_n)=f(a)\), you should be able to get from what you are assuming to what you want to conclude.%
\par
To prove the converse, it is convenient to prove its contrapositive.  That is, we want to prove that if \(f\) is not continuous at \(a\) then we can construct a sequence \(\left(x_n\right)\) that converges to \(a\) but \(\left(f(x_n)\right)\)does not converge to \(f(a)\). First we need to recognize what it means for \(f\) to not be continuous at \(a\).  This says that somewhere there exists an \(\eps>0\), such that no choice of \(\delta>0\) will work for this.  That is, for any such \(\delta\), there will exist \(x\), such that \(|\,x-a|\lt \delta\), but \(|f(x)-f(a)|\geq\eps\). With this in mind, if \(\delta=1\), then there will exist an \(x_1\) such that \(|\,x_1-a|\lt 1\), but \(|f(x_1)-f(a)|\geq\eps\).  Similarly, if \(\delta=\frac{1}{2}\), then there will exist an \(x_2\) such that \(|\,x_2-a|\lt \frac{1}{2}\), but \(|\,f(x_2)-f(a)|\geq\eps\).  If we continue in this fashion, we will create a sequence \(\left(x_n\right)\) such that \(|\,x_n-a|\lt \frac{1}{n}\), but \(|f(x_n)-f(a)|\geq\eps\).  This should do the trick.%
\end{proof}
\begin{problem}{Problem}{}{SequencesAndContinuity-13}%
\index{continuity!via limits}\index{limit!\(\limit{x}{a}{f(x)}=f(a)\) implies \(f(x)\) is continuous} Turn the ideas of the previous two paragraphs into a formal proof of \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}}.%
\end{problem}
\hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}} is a very useful result. It is a bridge between the ideas of convergence and continuity so it allows us to bring all of the theory we developed in \hyperref[Convergence]{Chapter~{\xreffont\ref{Convergence}}} to bear on continuity questions. For example consider the following.%
\begin{theorem}{Theorem}{}{}{thm_ContSumProd}%
\index{continuous functions!sum of continuous functions is continuous} Suppose \(f\) and \(g\) are both continuous at \(a\). Then \(f+g\) and \(f\cdot g\) are continuous at \(a\).%
\end{theorem}
\begin{proof}{Proof}{}{SequencesAndContinuity-16}
We could use the definition of continuity to prove \hyperref[thm_ContSumProd]{Theorem~{\xreffont\ref{thm_ContSumProd}}}, but \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}} makes our job much easier.  For example, to show that \(f+g\) is continuous, consider any sequence \(\left(x_n\right)\) which converges to \(a\).  Since \(f\) is continuous at \(a\), then by \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}}, \(\lim_{n\rightarrow\infty}f(x_n)=f(a)\).  Likewise, since \(g\) is continuous at \(a\), then \(\lim_{n\rightarrow\infty}g(x_n)=g(a)\).%
\par
By \hyperref[thm_SumOfSequences]{Theorem~{\xreffont\ref{thm_SumOfSequences}}} of \hyperref[Convergence]{Chapter~{\xreffont\ref{Convergence}}},\(\)%
\begin{alignat*}{1}
\lim_{n\rightarrow\infty}(f+g)(x_n)\amp=\lim_{n\rightarrow\infty} \left(f(x_n)+g(x_n)\right)\\
\amp =\lim_{n\rightarrow\infty}f(x_n)+\,\lim_{n \rightarrow\infty}g(x_n)\\
\amp =f(a)+g(a)\\
\amp =(f+g)(a).
\end{alignat*}
Thus by \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}}, \(f+g\) is continuous at \(a\).  The proof that \(f\cdot g\) is continuous at \(a\) is similar.%
\end{proof}
\begin{problem}{Problem}{}{SequencesAndContinuity-17}%
Use \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}} to show that if \(f\) and \(g\) are continuous at \(a\), then \(f\cdot g\) is continuous at \(a\).%
\end{problem}
By employing \hyperref[thm_ContSumProd]{Theorem~{\xreffont\ref{thm_ContSumProd}}} a finite number of times, we can see that a finite sum of continuous functions is continuous.  That is, if \(f_1,\,f_2,\,\ldots,\,f_n\) are all continuous at \(a\) then \(\sum_{j=1}^nf_j\) is continuous at \(a\).  But what about an infinite sum?  Specifically, suppose \(f_1,\,f_2,f_3,\ldots\) are all continuous at \(a\). Consider the following argument.%
\par
Let \(\eps>0\).  Since \(f_j\) is continuous at \(a\), then there exists \(\delta_j>0\) such that if \(|\,x-a|\lt
\delta_j\), then \(|f_j(x)-f_j(a)|\lt \frac{\eps}{2^j}\). Let \(\delta=\)min\(\left(\delta_1,\,\delta_2,\,\ldots\right)\). If \(|\,x-a|\lt \delta\), then%
\begin{equation*}
\left|\sum_{j=1}^\infty f_j(x)-\sum_{j=1}^\infty f_j(a)\right|=\left|\sum_{j=1}^\infty\left(f_j(x)-f_j(a)\right)\right|
\end{equation*}
%
\begin{equation*}
\leq\,\sum_{j=1}^\infty|f_j(x)-f_j(a)|\lt \sum_{j=1}^\infty\frac{ \eps}{2^j}=\eps\text{.}
\end{equation*}
%
\par
Thus by definition, \(\sum_{j=1}^\infty f_j\) is continuous at \(a\).%
\par
This argument seems to say that an infinite sum of continuous functions must be continuous (provided it converges).  However we know that the Fourier series%
\begin{equation*}
\frac{4}{\pi}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)}\cos\left(\left(2k+1\right)\pi x\right)
\end{equation*}
is a counterexample to this, as it is an infinite sum of continuous functions which does not converge to a continuous function.  Something fundamental seems to have gone wrong here. Can you tell what it is?%
\par
This is a question we will spend considerable time addressing in \hyperref[PowerSeriesRedux]{Chapter~{\xreffont\ref{PowerSeriesRedux}}} (in particular, see \hyperref[prob_Cauchy_s_incorrect_proof]{Problem~{\xreffont\ref{prob_Cauchy_s_incorrect_proof}}}) so if you don't see the difficulty, don't worry; you will.  In the meantime keep this problem tucked away in your consciousness. It is, as we said, fundamental.%
\par
\hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}} will also handle quotients of continuous functions.  There is however a small detail that needs to be addressed first.  Obviously, when we consider the continuity of \(f/g\) at \(a\),\(\)we need to assume that \(g(a)\neq 0\).  However, \(g\) may be zero at other values.  How do we know that when we choose our sequence \(\left(x_n\right)\) converging to \(a\) that \(g(x_n)\) is not zero?  This would mess up our idea of using the corresponding theorem for sequences (\hyperref[thm_LimitOfQuotient]{Theorem~{\xreffont\ref{thm_LimitOfQuotient}}} from \hyperref[Convergence]{Chapter~{\xreffont\ref{Convergence}}}).  This can be handled with the following lemma.%
\begin{lemma}{Lemma}{}{}{lem_BoundedAwayFromZero}%
If \(g\) is continuous at \(a\) and \(g(a)\neq 0\), then there exists \(\delta>0\) such that \(g(x)\neq
0\) for all \(x\in(a-\delta,a+\delta)\).%
\end{lemma}
\begin{problem}{Problem}{}{SequencesAndContinuity-25}%
Prove \hyperref[lem_BoundedAwayFromZero]{Lemma~{\xreffont\ref{lem_BoundedAwayFromZero}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{SequencesAndContinuity-25-3}{}\quad{}Consider the case where \(g(a)>0\).  Use the definition with \(\eps=\frac{g(a)}{2}\).  The picture is below; make it formal.%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/Ch5fig8.png}
\end{image}%
For the case \(g(a)\lt 0\), consider the function \(-g\).%
\end{problem}
A consequence of this lemma is that if we start with a sequence \(\left(x_n\right)\) converging to \(a\), then for \(n\) sufficiently large, \(g(x_n)\neq 0\).%
\begin{problem}{Problem}{}{SequencesAndContinuity-27}%
Use \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}}, to prove that if \(f\) and \(g\) are continuous at \(a\) and \(g(a)\neq 0\), then \(f/g\) is continuous at \(a\).%
\end{problem}
\begin{theorem}{Theorem}{}{}{thm_ContComp}%
\index{continuous functions!the composition of continuous functions is continuous}%
Suppose \(f\) is continuous at \(a\) and \(g\) is continuous at \(f(a)\).  Then \(g\circ f\) is continuous at \(a.\) (Note that \((g\circ
f)(x)=g(f(x))\).)%
\end{theorem}
\begin{problem}{Problem}{}{SequencesAndContinuity-29}%
Prove \hyperref[thm_ContComp]{Theorem~{\xreffont\ref{thm_ContComp}}}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Using the definition of continuity.%
\item{}Using \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}}.%
\end{enumerate}%
\end{problem}
The above theorems allow us to build continuous functions from other continuous functions.  For example, knowing that \(f(x)=x\) and \(g(x)=c\) are continuous, we can conclude that any polynomial,%
\begin{equation*}
p(x)=a_nx^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0
\end{equation*}
is continuous as well.  We also know that functions such as \(f(x)=\sin\left(e^x\right)\) are continuous without having to rely on the definition.%
\begin{problem}{Problem}{}{SequencesAndContinuity-31}%
Show that each of the following is a continuous function at every point in its domain.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Any polynomial.%
\item{}Any rational function. (A rational function is defined to be a ratio of polynomials.)%
\item{}\(\cos x\).%
\item{}The other trig functions: \(\tan(x)\), \(\cot(x)\), \(\sec(x)\), and \(\csc(x)\).%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{SequencesAndContinuity-32}%
What allows us to conclude that \(f(x)=\sin\left(e^x\right)\) is continuous at any point \(a\) without referring back to the definition of continuity?%
\end{problem}
\hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}} can also be used to study the convergence of sequences.  For example, since \(f(x)=e^x\) is continuous at any point and \(\lim_{n\rightarrow\infty}\frac{n+1}{n}=1\), then \(\lim_{n\rightarrow\infty}e^{\left(\frac{n+1}{n}\right)}=e\). This also illustrates a certain way of thinking about continuous functions.  They are the ones where we can ``commute'' the function and a limit of a sequence.  Specifically, if \(f\) is continuous at \(a\) and \(\lim_{n\rightarrow\infty}x_n=a\), then \(\lim_{n\rightarrow\infty}f(x_n)=f(a)=f\left(\lim_{n\rightarrow\infty}x_n\right)\).%
\begin{problem}{Problem}{}{SequencesAndContinuity-34}%
Compute the following limits. Be sure to point out how continuity is involved.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\displaystyle\lim_{n\rightarrow\infty}\sin\left(\frac{n\pi}{2n+1}\right)\)%
\item{}\(\displaystyle\lim_{n\rightarrow\infty}\sqrt{\frac{n}{n^2+1}}\)%
\item{}\(\displaystyle\lim_{n\rightarrow\infty}e^{\sin (1/n)}\)%
\end{enumerate}%
\end{problem}
Our motive for rigorously defining continuity was to resolve the apparent discrepencies introduced by Fourier's work. But that resolution is still rather distant because we still haven't proved the Extreme and Intermediate Value Theorems. In the following chapter we will close that gap.%
\end{sectionptx}
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 9 Intermediate and Extreme Values}
\typeout{************************************************}
%
\begin{chapterptx}{Chapter}{Intermediate and Extreme Values}{}{Intermediate and Extreme Values}{}{}{IVTandEVT}
\renewcommand*{\chaptername}{Chapter}
%
%
\typeout{************************************************}
\typeout{Section 9.1 The Nested Interval Property: Completeness of the Real Number System}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{The Nested Interval Property: Completeness of the Real Number System}{}{The Nested Interval Property: Completeness of the Real Number System}{}{}{IVTandEVT-Completeness}
In deriving the \hyperref[thm_LagrangeRemainder]{Lagrange} and \hyperref[thm_CauchyRemainder]{Cauchy} forms of the remainder for Taylor series, we made use of the \hyperref[thm_EVT]{Extreme Value Theorem} (\terminology{EVT}) and Intermediate Value Theorem (\terminology{IVT}).  In \hyperref[Continuity]{Chapter~{\xreffont\ref{Continuity}}}, we produced an analytic definition of continuity that we can use to prove these theorems.  Before we can work out the rest of the tools needed for those proofs we need to explore the make\textendash{}up of the real number system.%
\par
To illustrate what we mean, suppose that we have only the rational number system available.  We can still use our definition of continuity and can still consider continuous functions such as \(f(x)=x^2\).%
\begin{image}{0.315}{0.37}{0.315}{}%
\includegraphics[width=\linewidth]{external/images/Ch6fig1.png}
\end{image}%
Notice that \(2\) is a value that lies between \(f(1)=1\) and \(f(2)=4\) so the \hyperref[IntermediateValueTheorem]{IVT} says that somewhere between \(1\) and \(2\), \(f\) must take on the value \(2\).  That is, there must exist some number \(c\in[1,2]\) such that \(f(c)=2\).  You might say, ``Big deal!  Everyone knows \(c=\sqrt{2}\) works.''%
\par
However, we are only working with rational numbers and \(\sqrt{2}\) \(\)is not rational.  As we saw in \hyperref[NumbersRealRational]{Chapter~{\xreffont\ref{NumbersRealRational}}} the rational number system has holes in it, whereas the real number system doesn't. Again, ``Big deal!  Let's just say that the real number system contains (square) roots.''%
\par
This sounds reasonable and it actually works for square roots, but consider the function \(f(x)=x-\cos x\).  We know this is a continuous function.  We also know that \(f(0)=-1\) and \(f(\frac{\pi}{2})=\frac{\pi}{2}\).  Thus according to the \hyperref[IntermediateValueTheorem]{IVT}, there should be some number \(c\in[\,0,\frac{\pi}{2}]\), where \(f(c)=0\).  The graph of \(f(x)=x-\cos x\) is show below.%
\begin{image}{0.315}{0.37}{0.315}{}%
\includegraphics[width=\linewidth]{external/images/Ch6fig2.png}
\end{image}%
The situation is not as transparent as before.  What is this mysterious \(c\)  where the curve crosses the \(x\) axis?  Somehow we need to convey the idea that the real number system is a continuum.  That  it has no ``holes'' in it.%
\par
How about this?  Why don't we just say that it has no holes in it?  Sometimes the simple answer works best!  But not in this case. Just like with convergence and continuity, what we need is a rigorous way to convey the idea that the real number system does not have any holes, that it is \emph{complete.} How are we going to formulate a rigorous proof based on this statement?%
\par
We will see that there are several different, but equivalent ways to convey this notion of completeness.  We will explore some of them in this chapter.  For now we adopt the following as our \terminology{Completeness Axiom} for the real number system.%
\begin{axiom}{Axiom}{Nested Interval Property of the Real Number System (\terminology{NIP}).}{}{NIP}%
\index{Completeness Axiom, (the NIP)}%
Suppose we have two sequences of real numbers \(\left(x_n\right)\)and \(\left(y_n\right)\) satisfying the following conditions:%
\par
%
\begin{enumerate}
\item{}\(x_1\leq x_2\leq x_3\leq\ldots\) (this says that the sequence, \(\left(x_n\right)\), is non-decreasing)%
\item{}\(y_1\geq y_2\geq y_3\geq\ldots\) (this says that the sequence, \(\left(y_n\right)\), is non-increasing)%
\item{}\(\forall\) \(n\), \(x_n\leq y_n\)%
\item{}\(\displaystyle \lim_{n\rightarrow\infty}\left(y_n-x_n\right)=0\)%
\end{enumerate}
%
\par
Then there exists a unique number \(c\) such that \(x_n\leq c\leq y_n\) for all \(n\).%
\end{axiom}
Geometrically, we have the following situation.%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/Ch6fig3.png}
\end{image}%
Notice that we have two sequences \(\left(x_n\right)\) and \(\left(y_n\right)\), one increasing (really non-decreasing) and one decreasing (non-increasing).  These sequences do not pass each other.  In fact, the following is true:%
\begin{problem}{Problem}{}{IVTandEVT-Completeness-16}%
Let \((x_n), (y_n)\) be sequences as in the \hyperref[NIP]{NIP}. Show that for all \(n, m \in\NN\), \(x_n\le y_m\).%
\end{problem}
They are also coming together in the sense that \(\lim_{n\rightarrow\infty}\left(y_n-x_n\right)=0\).  The \hyperref[NIP]{NIP} says that in this case there is a unique real number \(c\) in the middle of all of this \(x_n\leq c\leq y_n\) for all \(n\in\NN\).%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/Ch6fig4.png}
\end{image}%
If there was no such \(c\) then there would be a hole where these two sequences come together.  The \hyperref[NIP]{NIP} guarantees that there is no such hole. We do not need to prove this since an axiom is, by definition, a self evident truth.  We are taking it on faith that the real number system has this property.  The next problem shows that the completeness property distinguishes the real number system from the rational number system.%
\begin{problem}{Problem}{}{IVTandEVT-Completeness-20}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Find two sequences of rational numbers \(\left(x_n\right)\)and \(\left(y_n\right)\) which satisfy properties 1-4 of the \hyperref[NIP]{NIP}  and such that there is no rational number \(c\) satisfying the conclusion of the \hyperref[NIP]{NIP}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{IVTandEVT-Completeness-20-2-2}{}\quad{}Consider the decimal expansion of an irrational number.%
\item{}Find two sequences of rational numbers \(\left(x_n\right)\) and \(\left(y_n\right)\) which satisfy properties 1-4 of the \hyperref[NIP]{NIP}  and such that there is a rational number \(c\) satisfying the conclusion of the \hyperref[NIP]{NIP}.%
\end{enumerate}%
\end{problem}
You might find the name ``Nested Interval Property'' to be somewhat curious.  One way to think about this property is to consider that we have a sequence of ``nested closed intervals'' \([\,x_1,y_1]\supseteq[\,x_2,y_2]\supseteq[\,x_3,y_3]\supseteq\cdots\) whose lengths \(\,y_n-x_n\) are ``shrinking to \(0\).'' The conclusion is that the intersection of these intervals is non-empty and, in fact, consists of a single point.  That is, \(\bigcap_{n=1}^\infty[x_n,y_n]=\{c\}\).%
\par
It appears that the sequences \(\left(x_n\right)\) and \(\left(y_n\right)\) in the \hyperref[NIP]{NIP}  converge to \(c\). This is, in fact, true and can be proven rigorously. In what follows, this will prove to be a valuable piece of information.%
\begin{theorem}{Theorem}{}{}{thm_ConvergeToC}%
\index{Nested Interval Property (NIP)!endpoints}\index{limit!of interval endpoints in the NIP} Suppose that we have two sequences \(\left(x_n\right)\) and \(\left(y_n\right)\) satisfying all of the assumptions of the Nested Interval Property.  If \(c\) is the unique number such that \(x_n\leq c\leq y_n\) for all \(n\), then%
\begin{equation*}
\limit{n}{\infty}{x_n}=c =
\limit{n}{\infty}{y_n}\text{.}
\end{equation*}
%
\end{theorem}
\begin{problem}{Problem}{}{IVTandEVT-Completeness-24}%
\index{Nested Interval Property (NIP)!endpoints} Prove \hyperref[thm_ConvergeToC]{Theorem~{\xreffont\ref{thm_ConvergeToC}}}.%
\end{problem}
To illustrate the idea that the \hyperref[NIP]{NIP}  ``plugs the holes'' in the real line, we will prove the existence of square roots of nonnegative real numbers.%
\begin{theorem}{Theorem}{}{}{thm_SqrtsExist}%
\index{square roots exist}%
Suppose \(a\in\mathbb{R},\,a\geq 0\).  There exists a real number \(c\geq 0\) such that \(c^2=a\).%
\end{theorem}
Notice that we can't just say, ``Let \(c=\sqrt{a}\),'' since the idea is to show that this square root exists.  In fact, throughout this proof, we cannot really use a square root symbol as we haven't yet proved that they (square roots) exist. We will give the idea behind the proof as it illustrates how the \hyperref[NIP]{NIP}  is used.%
\begin{proof}{Proof}{Sketch of Proof.}{IVTandEVT-Completeness-28}
Our strategy is to construct two sequences which will ``narrow in'' on the number \(c\) that we seek.  With that in mind, we need to find a number \(x_1\) such that \(x_1^2\leq a\) and a number \(y_1\) such that \(y_1^2\geq a\).  (Remember that we can't say \(\sqrt{x_1}\) or\(\sqrt{y_1}\).)  There are many possibilities, but how about \(x_1=0\) and \(y_1=a+1?\) You can check that these will satisfy \(x_1^2\leq a\leq\) \(y_1^2\).  Furthermore \(x_1\leq y_1\).  This is the starting point.%
\par
The technique we will employ is often called a bisection technique, and is a useful way to set ourselves up for applying the \hyperref[NIP]{NIP}. Let \(m_1\) be the midpoint of the interval \([\,x_1,y_1]\).  Then either we have \(m_1^2\leq
a\) or \(m_1^2\geq a\).  In the case \(m_1^2\leq a\), we really want \(m_1\) to take the place of \(x_1\) since it is larger than \(\,x_1\), but still represents an underestimate for what would be the square root of \(a\). This thinking prompts the following move.  If \(m_1^2\leq
a\), we will relabel things by letting \(x_2=m_1\) and \(y_2=y_1\).  The situation looks like this on the number line.%
\begin{image}{0.22}{0.56}{0.22}{}%
\includegraphics[width=\linewidth]{external/images/Ch6fig5.png}
\end{image}%
In the other case where \(a\leq m_1^2\), we will relabel things by letting \(x_2=x_1\) and \(y_2=m_1\).  The situation looks like this on the number line.%
\begin{image}{0.22}{0.56}{0.22}{}%
\includegraphics[width=\linewidth]{external/images/Ch6fig6.png}
\end{image}%
In either case, we've reduced the length of the interval where the square root lies to half the size it was before.  Stated in more specific terms, in either case we have the same results:%
\begin{equation*}
x_1\leq x_2\leq y_2\leq y_1;\ \  x_1^2\leq a\leq y_1^2;\ \  x_2^2\leq a\leq y_2^2
\end{equation*}
and%
\begin{equation*}
y_2-x_2=\frac{1}{2}\left(y_1-x_1\right)\text{.}
\end{equation*}
%
\par
Now we play the same game, but instead we start with the interval \([x_2,y_2]\).  Let \(m_2\) be the midpoint of \([x_2,y_2]\).  Then we have \(m_2^2\leq a\) or \(m_2^2\geq a\).  If \(m_2^2\leq a\), we relabel \(x_3=m_2\) and \(y_3=y_2\).  If \(a\leq m_2^2\), we relabel \(x_3=x_2\) and \(y_3=m_2\).  In either case, we end up with%
\begin{equation*}
x_1\leq x_2\leq x_3\leq y_3\leq y_2\leq y_1;\ \   x_1^2\leq a\leq y_1^2;\ \  x_2^2\leq a\leq y_2^2;\ \   x_3^2\leq a\leq y_3^2
\end{equation*}
and%
\begin{equation*}
y_3-x_3=\frac{1}{2}\left(y_2-x_2\right)=\frac{1}{2^2}\left(y_1-x_1\right)\text{.}
\end{equation*}
%
\par
Continuing in this manner, we will produce two sequences, \(\left(x_n\right)\)and \(\left(y_n\right)\) satisfying the following conditions:%
\begin{enumerate}
\item{}\(\displaystyle x_1\leq x_2\leq x_3\leq\ldots\)%
\item{}\(\displaystyle y_1\geq y_2\geq y_3\geq\ldots\)%
\item{}\(\forall\) \(n\), \(x_n\leq y_n\)%
\item{}\(\displaystyle \lim_{n\rightarrow\infty}\left(y_n-x_n\right)=\,\lim_{n\rightarrow\infty}\frac{1}{2^{n-1}}\left(y_1-x_1\right)=0\)%
\item{}These sequences also satisfy the following property:%
\begin{equation*}
\forall n,\,x_n^2\leq a\leq y_n^2
\end{equation*}
%
\end{enumerate}
%
\par
Properties 1-4 tell us that \(\left(x_n\right)\)and \(\left(y_n\right)\) satisfy all of the conditions of the \hyperref[NIP]{NIP}, so we can conclude that there must exist a real number \(c\) such that \(x_n\leq c\leq y_n\) for all \(n\). At this point, you should be able to use property 5. to show that \(c^2=a\) as desired.%
\end{proof}
\begin{problem}{Problem}{}{IVTandEVT-Completeness-29}%
\index{Nested Interval Property (NIP)!square roots of integers, and}\index{Nested Interval Property (NIP)!implies the existence of square roots of integers} Turn the above outline into a formal proof of \hyperref[thm_SqrtsExist]{Theorem~{\xreffont\ref{thm_SqrtsExist}}}.%
\end{problem}
The bisection method we employed in the proof of \hyperref[thm_SqrtsExist]{Theorem~{\xreffont\ref{thm_SqrtsExist}}} is pretty typical of how we will use the \hyperref[NIP]{NIP}, as taking midpoints ensures that we will create a sequence of ``nested intervals.'' We will employ this strategy in the proofs of the \hyperref[IntermediateValueTheorem]{IVT} and \hyperref[thm_EVT]{EVT}. Deciding how to relabel the endpoints of our intervals will be determined by what we want to do with these two sequences of real numbers. This will typically lead to a fifth property, which will be crucial in proving that the \(c\) guaranteed by the \hyperref[NIP]{NIP}  does what we want it to do.  Specifically, in the above example, we always wanted our candidate for \(\sqrt{a}\) to be in the interval \([\,x_n,y_n]\).  This judicious choice led to the extra Property~5: \(\forall\) \(n,\,x_n^2\leq a\leq
y_n^2\).  In applying the \hyperref[NIP]{NIP}  to prove the \hyperref[IntermediateValueTheorem]{IVT} and \hyperref[thm_EVT]{EVT}, we will find that properties 1-4 will stay the same.  Property~5 is what will change based on the property we want \(c\) to have.%
\par
Before we tackle the \hyperref[IntermediateValueTheorem]{IVT} and \hyperref[thm_EVT]{EVT}, let's use the \hyperref[NIP]{NIP}  to address an interesting question about the Harmonic Series. \index{series!Harmonic Series!slow divergence of} Recall that the Harmonic Series, \(1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots\), grows without bound, that is, \(\sum_{n=1}^\infty\frac{1}{n}=\infty\).  The question is how slowly does this series grow?  For example, how many terms would it take before the series surpasses 100? 1000?  10000?%
\par
Leonhard Euler \index{Euler, Leonhard} decided to tackle this problem in the following way.  Euler decided to consider the%
\begin{equation*}
\limit{n}{\infty}{\left(\left(1+\frac{1}{2}+\frac{1}{3}+\cdots+
\frac{1}{n}\right)-\ln(n+1)\right)}\text{.}
\end{equation*}
This limit is called Euler's constant and is denoted by \(\gamma\).  This says that for \(n\) large, we have \(1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\approx\)ln\(\left(n+1\right)+\gamma\). If we could approximate \(\gamma\), then we could replace the inequality \(1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\geq
100\) with the more tractable inequality ln\(\left(n+1\right)+\gamma\) \(\geq 0\) and solve for \(n\) in this.  This should tell us roughly how many terms would need to be added in the Harmonic Series to surpass 100. Approximating \(\gamma\) with a computer is not too bad.  We could make \(n\) as large as we wish in \(\left(1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\right)-\)ln\(\left(1+n\right)\) to make closer approximations for \(\gamma\).  The real issue is, \alert{HOW DO WE KNOW THAT}%
\begin{equation*}
\limit{n}{\infty}{\left(\left(1+\frac{1}{2}+\frac{1}{3}+\cdots+
\frac{1}{n}\right)-\ln(n+1)\right)}
\end{equation*}
\alert{ACTUALLY EXISTS?}%
\par
You might want to say that obviously it should, but let us point out that as of the time we are writign this book it is not even known if \(\gamma\) is rational or irrational.  So in our opinion, the existence of this limit is not so obvious.%
\par
This is where the \hyperref[NIP]{NIP} will come into play; we will use it to show that this limit does in fact exist.  The details are in the following problem.%
\begin{problem}{Problem}{}{IVTandEVT-Completeness-35}%
The purpose of this problem is to show that%
\begin{equation*}
\lim_{n\rightarrow\infty}\left(\left(1+\frac{1}{2}+\frac{1}{3}+\cdots+ \frac{1}{n}\right)-\ln\left(n+1\right)\right)
\end{equation*}
exists.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Let \(x_n=\left(1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\right)-\ln\left(n+1
\right)\).  Use the following diagram to show%
\begin{equation*}
x_1\leq x_2\leq x_3\leq\cdots
\end{equation*}
%
\begin{image}{0.08}{0.84}{0.08}{}%
\includegraphics[width=\linewidth]{external/images/Ch6fig7.png}
\end{image}%
\item{}Let \(z_n=\ln\left(n+1\right)-\left(\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n+1}
\right)\).  Use a similar diagram to show that \(z_1\leq z_2\leq z_3\leq\cdots\).%
\item{}Let \(y_n=1-z_n\).  Show that \(\left(x_n\right)\) and \(\left(y_n\right)\) satisfy the hypotheses of the nested interval property and use the \hyperref[NIP]{NIP}  to conclude that there is a real number \(\gamma\) such that \(x_n\leq\gamma\leq y_n\) for all \(n\).%
\item{}Conclude that \(\lim_{n\rightarrow\infty}\left(\left(1+\frac{1}{2}+\frac{1}{3}+\cdots+
\frac{1}{n}\right)-\ln\left(n+1\right)\right)=\gamma\).%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{IVTandEVT-Completeness-36}%
Use the fact that \(x_n\leq\gamma\leq y_n\) for all \(n\) to approximate \(\gamma\) to three decimal places.%
\end{problem}
\begin{problem}{Problem}{}{IVTandEVT-Completeness-37}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Use the fact that for large \(n\), \(1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\approx
\ln\left(n+1\right)+ \gamma\) to determine approximately how large \(n\) must be to make%
\begin{equation*}
1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\geq 100\text{.}
\end{equation*}
%
\item{}Suppose we have a supercomputer which can add 10 trillion terms of the Harmonic Series per second.  Approximately how many Earth Ages (the current age of the Earth) would it take for this computer to sum the Harmonic Series until it surpasses 100?%
\end{enumerate}%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 9.2 Proof of the Intermediate Value Theorem}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Proof of the Intermediate Value Theorem}{}{Proof of the Intermediate Value Theorem}{}{}{IVTandEVT-ProofOfIVT}
We now have all of the tools we need to prove the \hyperref[IntermediateValueTheorem]{Intermediate Value Theorem} (\terminology{IVT}).%
\begin{theorem}{Theorem}{}{}{IntermediateValueTheorem}%
\terminology{Intermediate Value Theorem (IVT)}%
\par
\index{Intermediate Value Theorem (IVT)} Suppose \(f(x)\) is continuous on \([a,b]\) and \(v\) is any real number between \(f(a)\) and \(f(b)\).  Then there exists a real number \(c\in[\,a,b]\) such that \(f(c)=v\).%
\end{theorem}
\begin{proof}{Proof}{Sketch of Proof.}{IVTandEVT-ProofOfIVT-4}
We have two cases to consider: \(f(a)\leq v\leq f(b)\) and \(f(a)\geq v\geq f(b)\).%
\par
We will look at the case \(f(a)\leq v\leq f(b)\).  Let \(x_1=a\) and \(y_1=b\), so we have \(x_1\leq y_1\) and \(f(x_1)\leq v\leq f(y_1)\).  Let \(\,m_1\) be the midpoint of \([\,x_1,y_1]\) and notice that we have either \(f(m_1)\leq v\) or \(f(m_1)\geq v\).  If \(f(m_1)\leq
v\) , then we relabel \(x_2=m_1\) and \(y_2=y_1\).  If \(f(m_1)\geq v\) , then we relabel \(x_2=x_1\) and \(y_2=m_1\).  In either case, we end up with \(x_1\leq
x_2\leq y_2\leq y_1\), \(y_2-x_2=\frac{1}{2}\left(y_1-x_1\right)\), \(f(x_1)\leq
v\leq f(y_1)\), and \(f(x_2)\leq v\leq f(y_2)\).%
\par
Now play the same game with the interval \([\,x_2,y_2]\). If we keep playing this game, we will generate two sequences \(\left(x_n\right)\) and \(\left(y_n\right)\), satisfying all of the conditions of the nested interval property.  These sequences will also satisfy the following extra property: \(\forall\) \(n,\,f(x_n)\leq v\leq
f(y_n)\).  By the \hyperref[NIP]{NIP}, there exists a \(c\) such that \(\,x_n\leq c\leq y_n\), \(\forall\) \(n\).  This should be the \(c\) that we seek though this is not obvious.  Specifically, we need to show that \(f(c)=v\). This should be where the continuity of \(f\) at \(c\) and the extra property on \(\left(x_n\right)\)and \(\left(y_n\right)\) come into play.%
\end{proof}
\begin{problem}{Problem}{}{IVTandEVT-ProofOfIVT-5}%
\index{Intermediate Value Theorem (IVT)!the case \(f(a)\leq v\leq f(b)\)} Turn the ideas of the previous paragraphs into a formal proof of the \hyperref[IntermediateValueTheorem]{IVT} for the case \(f(a)\leq v\leq f(b)\).%
\end{problem}
\begin{problem}{Problem}{}{IVTandEVT-ProofOfIVT-6}%
\index{Intermediate Value Theorem (IVT)!the case \(f(a)\geq v\geq f(b)\)} We can modify the proof of the case \(f(a)\leq v\leq
f(b)\) into a proof of the \hyperref[IntermediateValueTheorem]{IVT} for the case \(f(a)\geq
v\geq f(b)\).  However, there is a sneakier way to prove this case by applying the \hyperref[IntermediateValueTheorem]{IVT} to the function \(-f\).  Do this to prove the \hyperref[IntermediateValueTheorem]{IVT} for the case \(f(a)\geq v\geq
f(b)\).%
\end{problem}
\begin{problem}{Problem}{}{IVTandEVT-ProofOfIVT-7}%
\index{polynomials!with odd degree must have a root}\index{Intermediate Value Theorem (IVT)!a polynomial with odd degree must have a root} Use the \hyperref[IntermediateValueTheorem]{IVT} to prove that any polynomial of odd degree must have a real root.%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 9.3 The Bolzano-Weierstrass Theorem}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{The Bolzano-Weierstrass Theorem}{}{The Bolzano-Weierstrass Theorem}{}{}{IVTandEVT-BWT}
Once we introduced the \hyperref[NIP]{Nested Interval Property}, the \hyperref[IntermediateValueTheorem]{Intermediate Value Theorem}  followed pretty readily.  The proof of \hyperref[thm_EVT]{Extreme Value Theorem} takes a bit more work.  First we need to show that a function that satisfies the conditions of the \hyperref[thm_EVT]{EVT} is bounded.%
\begin{theorem}{Theorem}{}{}{thm_CompactBounded}%
\index{continuous functions!continuous function on a closed, bounded interval is bounded}%
A continuous function defined on a closed, bounded interval must be bounded.  That is, let \(f\) be a continuous function defined on \([\,a,b]\).  Then there exists a positive real number \(B\) such that \(|f(x)|\leq B\) for all \(x\in[\,a,b]\).%
\end{theorem}
\alert{Sketch of Alleged Proof:} Let's assume, for contradiction, that there is no such bound \(B\). This says that for any positive integer \(n\), there must exist \(x_n\in[a,b]\) such that \(\abs{f(x_n)}>n\). (Otherwise \(n\) would be a bound for \(f\).) \alert{IF} the sequence \(\left(x_n\right)\) converged to something in \([\,a,b]\), say \(c\), then we would have our contradiction. Indeed, we would have \(\lim_{n\rightarrow\infty}x_n=c\). By the continuity of \(f\) at \(c\) and \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}} of \hyperref[Continuity]{Chapter~{\xreffont\ref{Continuity}}}, we would have \(\lim_{n\rightarrow\infty}f(x_n)=f(c)\). This would say that the sequence \(\left(f(x_n)\right)\) converges, so by \hyperref[lemma_BoundedConvergent]{Lemma~{\xreffont\ref{lemma_BoundedConvergent}}} of \hyperref[Convergence]{Chapter~{\xreffont\ref{Convergence}}}, it must be bounded. This would provide our contradiction, as we had \(|f(x_n)|>n\), for all positive integers \(n\). \alert{QED?}%
\par
This would all work well except for one little problem. The way it was constructed, there is no reason to expect the sequence \(\left(x_n\right)\) to converge to anything and we can't make such an assumption. That is why we emphasized the \alert{IF} above. Fortunately, this idea can be salvaged. While it is true that the sequence \(\left(x_n\right)\) may not converge, part of it will. We will need the following definition.%
\begin{definition}{Definition}{}{def_subsequences}%
\index{sequences!subsequences} Let \(\left(n_k\right)_{k=1}^\infty\) be a strictly increasing sequence of positive integers; that is, \(n_1\lt n_2\lt n_3\lt \cdots\) . If \(\left(x_n\right)_{n=1}^\infty\) is a sequence, then \(\left(x_{n_k}\right)_{k=1}^\infty=\left(x_{n_1},x_{n_2},x_{n_3},\ldots \right)\) is called a subsequence of \(\left(x_n\right)\).%
\end{definition}
The idea is that a subsequence of a sequence is a part of the sequence, \((x_n)\), which is itself a sequence. However, it is a little more restrictive. We can choose any term in our sequence to be part of the subsequence, but once we choose that term, we can't go backwards. This is where the condition \(n_1\lt n_2\lt n_3\lt \cdots\) comes in. For example, suppose we started our subsequence with the term \(x_{100}\). We could not choose our next term to be \(x_{99}\). The subscript of the next term would have to be greater than 100. In fact, the thing about a subsequence is that it is all in the subscripts; we are really choosing a subsequence \(\left(n_k\right)\) of the sequence of subscripts \(\left(n\right)\) in \(\left(x_n\right)\).%
\begin{example}{Example}{}{IVTandEVT-BWT-8}%
Given the sequence \(\left(x_n\right)\), the following are subsequences.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\left(x_2,\,x_4,\,x_6,\,\ldots\right)=\left(x_{2k}\right)_{k=1}^\infty\)%
\item{}\(\left(x_1,\,x_4,\,x_9,\,\ldots\right)=\left(x_{k^2}\right)_{k=1}^ \infty\)%
\item{}\(\left(x_n\right)\) itself.%
\end{enumerate}%
\end{example}
\begin{example}{Example}{}{IVTandEVT-BWT-9}%
The following are \alert{NOT} subsequences.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\left(x_1,\,x_1,\,x_1,\,\ldots\right)\)%
\item{}\(\left(x_{99},\,x_{100},\,x_{99},\,\ldots\right)\)%
\item{}\(\left(x_1,\,x_2,\,x_3\right)\)%
\end{enumerate}%
\end{example}
The subscripts in the examples we have seen so far have a discernable pattern, but this need not be the case. For example,%
\begin{equation*}
\left(x_2,\,x_5,\,x_{12},\,x_{14},\,x_{23},\,\ldots\right)
\end{equation*}
would be a subsequence as long as the subscripts form an increasing sequence themselves.%
\begin{problem}{Problem}{}{IVTandEVT-BWT-11}%
\index{sequences!all subsequences of a convergent sequence converge} Suppose \(\lim_{n\rightarrow\infty}x_n=c\). Prove that \(\lim_{k\rightarrow\infty}x_{n_k}=c\) for any subsequence \(\left(x_{n_k}\right)\) of \(\left(x_n\right)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{IVTandEVT-BWT-11-2}{}\quad{}First prove that \(n_k\geq k\).%
\end{problem}
A very important theorem about subsequences was introduced by Bernhard Bolzano \index{Bolzano, Bernhard} and, later, independently proven \index{Weierstrass, Karl} by Karl Weierstrass. Basically, this theorem says that any bounded sequence of real numbers has a convergent subsequence.%
\begin{theorem}{Theorem}{}{}{BolzanoWeierstrass}%
\alert{The Bolzano-Weierstrass Theorem}%
\par
\index{Bolzano-Weierstrass Theorem} Let \(\left(x_n\right)\) be a sequence of real numbers such that \(x_n\in[\,a,b]\), \(\forall\) \(n\).  Then there exists \(c\in[\,a,b]\) and a subsequence \(\left(x_{n_k}\right)\), such that \(\lim_{k\rightarrow\infty}x_{n_k}=c\).%
\end{theorem}
As an example of this theorem, consider the sequence%
\begin{equation*}
\left((-1)^n\right)=\left(-1,1,-1,1,\ldots\right) \text{.}
\end{equation*}
%
\par
This sequence does not converge, but the subsequence%
\begin{equation*}
\left((-1)^{2k}\right)=\left(1,1,1,\ldots\right)
\end{equation*}
converges to \(1\). This is not the only convergent subsequence, as \(\left((-1)^{2k+1}\right)=(-1,-1,-1,\,\ldots)\) converges to \(-1\). Notice that if the sequence is unbounded, then all bets are off; the sequence may have a convergent subsequence or it may not. The sequences \(\left(\left((-1)^n+1\right)n\right)\) and \(\left(n\right)\) represent these possibilities as the first has, for example, \(\left(\left((-1)^{2k+1}+1\right)(2k+1)\right)=(0,0,0,\,\ldots)\) and the second one has none.%
\par
The Bolzano-Weierstrass Theorem says that no matter how ``random'' the sequence \(\left(x_n\right)\) may be, as long as it is bounded then some part of it must converge. This is very useful when one has some process which produces a ``random'' sequence such as what we had in the idea of the alleged proof in \hyperref[thm_CompactBounded]{Theorem~{\xreffont\ref{thm_CompactBounded}}}.%
\begin{proof}{Proof}{Sketch of a Proof of the Bolzano-Weierstrass Theorem.}{IVTandEVT-BWT-17}
Suppose we have our sequence \(\left(x_n\right)\) such that \(x_n\in[a,b]\), \(\forall\) \(n\). To find our \(c\) for the subsequence to converge to we will use the \hyperref[NIP]{NIP}. Since we are already using \(\left(x_n\right)\) as our original sequence, we will need to use different letters in setting ourselves up for the \hyperref[NIP]{NIP}. With this in mind, let \(a_1=a\) and \(b_1=b\), and notice that \(x_n\in[a_1,b_1]\) for infinitely many \(n\). (This is, in fact true for all \(n\), but you'll see why we said it the way we did.) Let \(m_1\) be the midpoint of \([a_1,b_1]\) and notice that either \(x_n\in[a_1,m_1]\) for infinitely many \(n\) or \(x_n\in[m_1,b_1]\) for infinitely many \(n\). If \(x_n\in[a_1,m_1]\) for infinitely many \(n\), then we relabel \(a_2=a_1\) and \(b_2=m_1\). If \(x_n\in[m_1,b_1]\) for infinitely many \(n\), then relabel \(a_2=m_1\) and \(b_2=b_1\). In either case, we get \(a_1\leq a_2\leq b_2\leq b_1\), \(b_2-a_2=\frac{1}{2}\left(b_1-a_1\right)\), and \(x_n\in[a_2,b_2]\) for infinitely many \(n\).%
\par
Now we consider the interval \([a_2,b_2]\) and let \(m_2\) be the midpoint of \([a_2,b_2]\). Since \(x_n\in[a_2,b_2]\) for infinitely many \(n\), then either \(x_n\in[a_2,m_2]\) for infinitely many \(n\) or \(x_n\in[m_2,b_2]\) for infinitely many \(n\). If \(x_n\in[a_2,m_2]\) for infinitely many \(n\), then we relabel \(a_3=a_2\) and \(b_3=m_2\). If \(x_n\in[m_2,b_2]\) for infinitely many \(n\), then we relabel \(a_3=m_2\) and \(b_3=b_2\). In either case, we get \(a_1\leq a_2\leq a_3\leq b_3\leq b_2\leq b_1\), \(b_3-a_3=\frac{1}{2}\left(b_2-a_2\right)=\frac{1}{2^2}\left(b_1-a_1\right)\), and \(x_n\in[a_3,b_3]\) for infinitely many \(n\).%
\par
If we continue in this manner, we will produce two sequences \(\left(a_k\right)\)and \(\left(b_k\right)\) with the following properties:%
\begin{enumerate}
\item{}\(\displaystyle a_1\leq a_2\leq a_3\leq\cdots\)%
\item{}\(\displaystyle b_1\geq b_2\geq b_3\geq\cdots\)%
\item{}\(\forall\) \(k,\,a_k\leq b_k\)%
\item{}\(\displaystyle \displaystyle\lim_{k\rightarrow\infty}\left(b_k-a_k\right)=\,\lim_{k\rightarrow\infty} \frac{1}{2^{k-1}}\left(b_1-a_1\right)=0\)%
\item{}For each \(k\), \(x_n\in[\,a_k,b_k]\) for infinitely many \(n\)%
\end{enumerate}
%
\par
By properties 1-5 and the \hyperref[NIP]{NIP}, there exists a unique \(c\) such that \(c\in[a_k,b_k]\), for all \(k\). In particular, \(c\in[a_1,b_1]=[a,b]\).%
\par
We have our \(c\). Now we need to construct a subsequence converging to it. Since \(x_n\in[a_1,b_1]\) for infinitely many \(n\), choose an integer \(n_1\) such that \(x_{n_1}\in[a_1,b_1]\). Since \(x_n\in[a_2,b_2]\) for infinitely many \(n\), choose an integer \(n_2>n_1\) such that \(x_{n_2}\in[a_2,b_2]\). (Notice that to make a subsequence it is crucial that \(n_2>n_1\), and this is why we needed to insist that \(x_n\in[a_2,b_2]\) for infinitely many \(n\).) Continuing in this manner, we should be able to build a subsequence \(\left(x_{n_k}\right)\) that will converge to \(c\). You can supply the details in the following problem.%
\end{proof}
\begin{problem}{Problem}{}{IVTandEVT-BWT-18}%
\index{Bolzano-Weierstrass Theorem (BWT)} Turn the ideas of the above outline into a formal proof of the Bolzano-Weierstrass Theorem.%
\end{problem}
\begin{problem}{Problem}{}{IVTandEVT-BWT-19}%
\index{Bolzano-Weierstrass Theorem (BWT)!implies that a  continuous functions on a closed set is bounded}\index{continuous functions!on a closed set, and the Bolzano-Weierstrass Theorem}\index{continuity!Bolzano-Weierstrass Theorem implies a continuous function on a closed set is bounded} Use the Bolzano-Weierstrass Theorem to complete the proof of \hyperref[thm_CompactBounded]{Theorem~{\xreffont\ref{thm_CompactBounded}}}.%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 9.4 The Supremum and the Extreme Value Theorem}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{The Supremum and the Extreme Value Theorem}{}{The Supremum and the Extreme Value Theorem}{}{}{IVTandEVT-SupremumAndEVT}
\hyperref[thm_CompactBounded]{Theorem~{\xreffont\ref{thm_CompactBounded}}} says that a continuous function on a closed, bounded interval must be bounded. Boundedness, in and of itself, does not ensure the existence of a maximum or minimum. We must also have a closed, bounded interval. To illustrate this, consider the continuous function \(f(x)=\)tan\(^{-1}x\) defined on the (unbounded) interval \(\left(-\infty,\infty\right)\).%
\begin{image}{0.125}{0.75}{0.125}{}%
\includegraphics[width=\linewidth]{external/images/Ch6fig8.png}
\end{image}%
This function is bounded between \(-\frac{\pi}{2}\)and \(\frac{\pi}{2}\), but it does not attain a maximum or minimum as the lines \(y=\pm\frac{\pi}{2}\) are horizontal asymptotes. Notice that if we restricted the domain to a closed, bounded interval then it would attain its extreme values on that interval (as guaranteed by the \hyperref[thm_EVT]{\terminology{EVT}}).%
\par
To find a maximum we need to find the smallest possible upper bound for the range of the function. This prompts the following definitions.%
\begin{definition}{Definition}{}{def_UpperBound}%
\index{Upper Bound} Let \(S\subseteq\mathbb{R}\) and let \(b\) be a real number. We say that \(b\) is an upper bound of \(S\) provided \(b\geq x\) for all \(x\in S\).%
\end{definition}
For example, if \(S=(0,1)\), then any \(b\) with \(b\geq 1\) would be an upper bound of \(S\). Furthermore, the fact that \(b\) is not an element of the set \(S\) is immaterial. Indeed, if \(T=[\,0,1]\), then any \(b\) with \(b\geq 1\) would still be an upper bound of \(T\). Notice that, in general, if a set has an upper bound, then it has infinitely many since any number larger than that upper bound would also be an upper bound. However, there is something special about the smallest upper bound.%
\begin{definition}{Definition}{Least Upper Bound Property (\terminology{LUBP}).}{def_LeastUpperBound}%
\index{Least Upper Bound Property (LUBP)}%
Let \(S\subseteq\mathbb{R}\) and let \(b\) be a real number.  We say that \(b\) is the \emph{least upper bound} of \(S\) provided%
\par
%
\begin{enumerate}[label={(\alph*)}]
\item{}\(b\geq x\) for all \(x\in S\). (\(b\) is an upper bound of \(S\))%
\item{}If \(c\geq x\) for all \(x\in S\), then \(c\geq b\). (Any upper bound of \(S\) is at least as big as \(b\).)%
\end{enumerate}
%
\par
In this case, we also say that \(b\) \emph{is the supremum} of \(S\) and we write%
\begin{equation*}
b=\sup\left(S\right)\text{.}
\end{equation*}
%
\end{definition}
Notice that the definition really says that \(b\) is the smallest upper bound of \(S\). Also notice that the second condition can be replaced by its contrapositive so we can say that \(b=\sup S\) if and only if%
\begin{enumerate}[label={(\alph*)}]
\item{}\(\displaystyle b\,\geq\,x\text{ for all } x\,\in S\)%
\item{}If \(c\,\lt \,b\) then there exists \(x\,\in S\) such that \(c\,\lt \,x\).%
\end{enumerate}
%
\par
The second condition says that if a number \(c\) is less than \(b\), then it can't be an upper bound, so that \(b\) really is the smallest upper bound.%
\par
Also notice that the supremum of the set may or may not be in the set itself. This is illustrated by the examples above as in both cases, \(1=\sup(0,1)\) and \(1=\sup [0,1]\). Obviously, a set which is not bounded above such as \(\mathbb{N}=\{1,\,2,\,3,\,\ldots\}\) cannot have a supremum. However, for non-empty sets which are bounded above, we have the following.%
\begin{theorem}{Theorem}{The Least Upper Bound Property (\terminology{LUBP}).}{}{thm_LUB}%
\index{Least Upper Bound Property (LUBP)}%
Let \(S\) be a non\textendash{}empty subset of \(\mathbb{R}\) which is bounded above. Then \(S\) has a supremum.%
\end{theorem}
\begin{proof}{Proof}{Sketch of Proof.}{IVTandEVT-SupremumAndEVT-13}
Since \(S\neq\emptyset\), then there exists \(s\in S\). Since \(S\) is bounded above then it has an upper bound, say \(b\). We will set ourselves up to use the Nested Interval Property. With this in mind, let \(x_1=s\) and \(y_1=b\) and notice that \(\exists\) \(x\in S\) such that \(x\geq x_1\) (namely, \(x_1\) itself) and \(\forall\,x\in S\), \(y_1\geq x\). You probably guessed what's coming next: let \(m_1\) be the midpoint of \([\,x_1,y_1]\). Notice that either \(m_1\geq x,\,\forall\,x\in S\) or \(\exists\) \(x\in S\) such that \(x\geq m_1\). In the former case, we relabel, letting \(x_2=x_1\) and \(y_2=m_1\). In the latter case, we let \(x_2=m_1\) and \(y_2=y_1\). In either case, we end up with \(x_1\leq x_2\leq y_2\leq y_1\), \(y_2-x_2=\frac{1}{2}\left(y_1-x_1\right)\), and \(\exists\) \(x\in S\) such that \(x\geq x_2\) and \(\forall\,x\in S\), \(y_2\geq x\). If we continue this process, we end up with two sequences, \(\left(x_n\right)\)and \(\left(y_n\right)\), satisfying the following conditions:%
\begin{enumerate}
\item{}\(\displaystyle x_1\leq x_2\leq x_3\leq\ldots\)%
\item{}\(\displaystyle y_1\geq y_2\geq y_3\geq\ldots\)%
\item{}\(\forall\) \(n\), \(x_n\leq y_n\)%
\item{}\(\displaystyle \lim_{n\rightarrow\infty}\left(y_n-x_n\right)=\lim_{n\rightarrow\infty} \frac{1}{2^{n-1}}\left(y_1-x_1\right)=0\)%
\item{}\(\forall\) \(n,\exists\) \(x\in S\) such that \(x\geq x_n\) and \(\forall\,x\in S\), \(y_n\geq x\),%
\end{enumerate}
%
\par
By properties 1-5 and the \hyperref[NIP]{NIP}  there exists \(c\) such that \(x_n\leq c\leq y_n,\,\forall\,n\). We will leave it to you to use property 5 to show that \(c=\sup S\).%
\end{proof}
\begin{problem}{Problem}{}{IVTandEVT-SupremumAndEVT-14}%
\index{Nested Interval Property (NIP)!implies the LUBP} Complete the above ideas to provide a formal proof of \hyperref[thm_LUB]{Theorem~{\xreffont\ref{thm_LUB}}}.%
\end{problem}
Notice that we really used the fact that \(S\) was non-empty and bounded above in the proof of \hyperref[thm_LUB]{Theorem~{\xreffont\ref{thm_LUB}}}. This makes sense, since a set which is not bounded above cannot possibly have a least upper bound. In fact, any real number is an upper bound of the empty set so that the empty set would not have a least upper bound.%
\par
The following corollary to \hyperref[thm_LUB]{Theorem~{\xreffont\ref{thm_LUB}}} can be very useful.%
\begin{corollary}{Corollary}{}{}{cor_IncBoundedConverge}%
Let \((x_n)\) be a bounded, increasing sequence of real numbers. That is, \(x_1\leq x_2\leq x_3\leq\cdots\). Then \((x_n)\) converges to some real number \(c\).%
\end{corollary}
\begin{problem}{Problem}{}{IVTandEVT-SupremumAndEVT-18}%
\index{sequences!bounded and non-decreasing} Prove \hyperref[cor_IncBoundedConverge]{Corollary~{\xreffont\ref{cor_IncBoundedConverge}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{IVTandEVT-SupremumAndEVT-18-2}{}\quad{}Let \(c=\sup\{x_n|\,n=1,2,3,\ldots\}\). To show that \(\limit{n}{\infty}{x_n}=c\), let \(\eps >0.\)Note that \(c-\eps \) is not an upper bound. You take it from here!%
\end{problem}
\begin{problem}{Problem}{}{IVTandEVT-SupremumAndEVT-19}%
\index{\(\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{...}}}}\)!value of} Consider the following curious expression%
\begin{equation*}
\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{...}}}}\text{.}
\end{equation*}
We will use \hyperref[cor_IncBoundedConverge]{Corollary~{\xreffont\ref{cor_IncBoundedConverge}}} to show that this actually converges to some real number. After we know it converges we can actually compute what it is. Of course to do so, we need to define things a bit more precisely. With this in mind consider the following sequence \(\left(x_n\right)\) defined as follows:%
\begin{equation*}
x_1=\sqrt{2}
\end{equation*}
%
\begin{equation*}
x_{n+1}=\sqrt{2+x_n}\text{.}
\end{equation*}
%
\begin{enumerate}[label={(\alph*)}]
\item{}Use induction to show that \(x_n\lt 2\) for \(n=1,\,2,\,3,\,\ldots\).%
\item{}Use the result from part (a) to show that \(x_n\lt x_{n+1}\) for \(n=1,\,2,\,3,\,\ldots\) .%
\item{}From \hyperref[cor_IncBoundedConverge]{Corollary~{\xreffont\ref{cor_IncBoundedConverge}}}, we have that \(\left(x_n\right)\) must converge to some number \(c\). Use the fact that \(\left(x_{n+1}\right)\) must converge to \(c\) as well to compute what \(c\) must be.%
\end{enumerate}
%
\end{problem}
We now have all the tools we need to tackle the Extreme Value Theorem.%
\begin{theorem}{Theorem}{}{}{thm_EVT}%
\alert{Extreme Value Theorem (\terminology{EVT})}%
\par
\index{Extreme Value Theorem (EVT)} Suppose \(f\) is continuous on \([\,a,b]\). Then there exists \(c,d\in[\,a,b]\) such that \(f(d)\leq f(x)\leq f(c)\), for all \(x\in[\,a,b]\).%
\end{theorem}
\begin{proof}{Proof}{Sketch of Proof.}{IVTandEVT-SupremumAndEVT-22}
We will first show that \(f\) attains its maximum. To this end, recall that \hyperref[thm_CompactBounded]{Theorem~{\xreffont\ref{thm_CompactBounded}}} tells us that \(f[\,a,b]=\{f(x)|\,x\in[\,a,b]\}\) is a bounded set. By the LUBP, \(f[\,a,b]\) must have a least upper bound which we will label \(s\), so that \(s=\sup f[\,a,b]\). This says that \(s\geq f(x)\),for all \(x\in[\,a,b]\). All we need to do now is find a \(c\in[\,a,b]\) with \(f(c)=s\). With this in mind, notice that since \(s=\sup f[\,a,b]\), then for any positive integer \(n\), \(s-\frac{1}{n}\) is not an upper bound of \(f[\,a,b]\). Thus there exists \(x_n\in[\,a,b]\) with\(\,s-\frac{1}{n}\lt f(x_n)\leq s\). Now, by the Bolzano-Weierstrass Theorem, \(\left(x_n\right)\) has a convergent subsequence\(\,\left(x_{n_k}\right)\) converging to some \(c\in[\,a,b]\). Using the continuity of \(f\) at \(c\), you should be able to show that \(f(c)=s\). To find the minimum of \(f\), find the maximum of \(-f\).%
\end{proof}
\begin{problem}{Problem}{}{IVTandEVT-SupremumAndEVT-23}%
\index{Extreme Value Theorem (EVT)} Formalize the above ideas into a proof of the \hyperref[thm_EVT]{EVT}.%
\end{problem}
Notice that we used the \hyperref[NIP]{NIP}  to prove both the Bolzano-Weierstrass Theorem and the LUBP. This is really unavoidable, as it turns out that all of those statements are equivalent in the sense that any one of them can be taken as the completeness axiom for the real number system and the others proved as theorems. This is not uncommon in mathematics, as people tend to gravitate toward ideas that suit the particular problem they are working on. In this case, people realized at some point that they needed some sort of completeness property for the real number system to prove various theorems. Each individual's formulation of completeness fit in with his understanding of the problem at hand. Only in hindsight do we see that they were really talking about the same concept: the completeness of the real number system. In point of fact, most modern textbooks use the LUBP as the axiom of completeness and prove all other formulations as theorems. We will finish this section by showing that either the Bolzano-Weierstrass Theorem or the LUBP can be used to prove the \hyperref[NIP]{NIP}. This says that they are all equivalent and that any one of them could be taken as the completeness axiom.%
\begin{problem}{Problem}{}{IVTandEVT-SupremumAndEVT-25}%
\index{Bolzano-Weierstrass Theorem (BWT)!implies the NIP} Use the Bolzano-Weierstrass Theorem to prove the NIP. That is, assume that the Bolzano-Weierstrass Theorem holds and suppose we have two sequences of real numbers, \(\left(x_n\right)\) and \(\left(y_n\right)\), satisfying:%
\begin{enumerate}
\item{}\(\displaystyle x_1\le x_2 \le x_3 \le \ldots\)%
\item{}\(\displaystyle y_1\ge y_2 \ge y_3 \ge \ldots\)%
\item{}\(\displaystyle \forall\ n,\ x_n\le y_n\)%
\item{}\(\displaystyle\lim_{n\rightarrow\infty}\left(y_n-x_n\right) = 0\).%
\end{enumerate}
%
\par
Prove that there is a real number \(c\) such that \(x_n\le c\le y_n\), for all \(n\).%
\end{problem}
Since the Bolzano-Weierstrass Theorem and the Nested Interval Property are equivalent, it follows that the Bolzano-Weierstrass Theorem will not work for the rational number system.%
\begin{problem}{Problem}{}{IVTandEVT-SupremumAndEVT-27}%
\index{sequences!find a bounded sequence of rational numbers such that no subsequence converges to a rational number} Find a bounded sequence of rational numbers such that no subsequence of it converges to a rational number.%
\end{problem}
\begin{problem}{Problem}{}{IVTandEVT-SupremumAndEVT-28}%
\index{Least Upper Bound Property (LUBP)!implies the Nested Interval Property (NIP)} Use the Least Upper Bound Property to prove the Nested Interval Property. That is, assume that every non-empty subset of the real numbers which is bounded above has a least upper bound; and suppose that we have two sequences of real numbers \(\left(x_n\right)\) and \(\left(y_n\right)\), satisfying:%
\begin{enumerate}
\item{}\(\displaystyle x_1\le x_2 \le x_3 \le \ldots\)%
\item{}\(\displaystyle y_1\ge y_2 \ge y_3 \ge \ldots\)%
\item{}\(\displaystyle \forall\ n, x_n\le y_n\)%
\item{}\(\displaystyle\lim_{n\rightarrow\infty}\left(y_n-x_n\right) = 0\).%
\end{enumerate}
%
\par
Prove that there exists a real number \(c\) such that \(x_n\le c\le y_n\), for all n. (Again, the \(c\) will, of necessity, be unique, but don't worry about that.)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{IVTandEVT-SupremumAndEVT-28-2}{}\quad{}\hyperref[cor_IncBoundedConverge]{Corollary~{\xreffont\ref{cor_IncBoundedConverge}}} might work well here.%
\end{problem}
\begin{problem}{Problem}{}{IVTandEVT-SupremumAndEVT-29}%
\index{Least Upper Bound Property (LUBP)!doesn't hold in \(\QQ\)} Since the LUBP is equivalent to the \hyperref[NIP]{NIP} it does not hold for the rational number system. Demonstrate this by finding a non-empty set of rational numbers which is bounded above, but whose supremum is an irrational number.%
\end{problem}
We have the machinery in place to clean up a matter that was introduced in \hyperref[NumbersRealRational]{Chapter~{\xreffont\ref{NumbersRealRational}}}. If you recall (or look back) we introduced the Archimedean Property of the real number system. This property says that given any two positive real numbers \(a, b\), there exists a positive integer \(n\) with \(na>b\). As we mentioned in \hyperref[NumbersRealRational]{Chapter~{\xreffont\ref{NumbersRealRational}}}, this was taken to be intuitively obvious. The analogy we used there was to emptying an ocean \(b\) with a teaspoon \(a\) provided we are willing to use it enough times \(n\). The completeness of the real number system allows us to prove it as a formal theorem.%
\begin{theorem}{Theorem}{}{}{thm_ArchmedeanProperty}%
\index{Archimedean Property}%
\alert{Archimedean Property of \(\RR\)}%
\par
Given any positive real numbers \(a\) and \(b\), there exists a positive integer \(n\), such that \(na>b\).%
\end{theorem}
\begin{problem}{Problem}{}{IVTandEVT-SupremumAndEVT-32}%
Prove \hyperref[thm_ArchmedeanProperty]{Theorem~{\xreffont\ref{thm_ArchmedeanProperty}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{IVTandEVT-SupremumAndEVT-32-4}{}\quad{}Assume that there are positive real numbers \(a\) and \(b\), such that \(na\le b\) \(\forall n\in \NN\). Then \(\NN\) would be bounded above by \(b/a\). Let \(s=\sup(\NN)\) and consider \(s-1\).%
\end{problem}
Given what we've been doing, one might ask if the Archimedean Property is equivalent to the LUBP (and thus could be taken as an axiom). The answer lies in the following problem.%
\begin{problem}{Problem}{}{IVTandEVT-SupremumAndEVT-34}%
\index{Archimedean Property!and \(\QQ\)}  Does \(\QQ\) satisfy the Archimedean Property and what does this have to do with the question of taking the Archimedean Property as an axiom of completeness?%
\end{problem}
\begin{problem}{Problem}{}{IVTandEVT-SupremumAndEVT-35}%
\index{Greatest Lower Bound Property (GLBP)!definition of} Mimic the definitions of an upper bound of a set and the least upper bound (supremum) of a set to give definitions for a lower bound of a set and the greatest lower bound (infimum) of a set.%
\par
\alert{Note}: The infimum of a set \(S\) is denoted by \(\inf(S)\).%
\end{problem}
\begin{problem}{Problem}{}{IVTandEVT-SupremumAndEVT-36}%
\index{Least Upper Bound Property (LUBP)!identifying suprema and infima} Find the least upper bound (supremum) and greatest lower bound (infimum) of the following sets of real numbers, if they exist. (If one does not exist then say so.)%
\begin{enumerate}[label={(\alph*)}]
\item{}\(\displaystyle S=\left\{\frac{1}{n}\,|\,n=1,2,3,\ldots\right\}\)%
\item{}\(T=\left\{r\,|\,r\right.\) is rational and \(\left.r^2\lt 2\right\}\)%
\item{}\(\displaystyle (-\infty,0)\cup(1,\infty)\)%
\item{}\(\displaystyle R=\left\{\frac{(-1)^n}{n}\,|\,n=1,2,3,\ldots\right\}\)%
\item{}\(\displaystyle (2,3\pi]\cap\QQ\)%
\item{}The empty set \(\emptyset\)%
\end{enumerate}
%
\end{problem}
\begin{problem}{Problem}{}{IVTandEVT-SupremumAndEVT-37}%
\index{Least Upper Bound Property (LUBP)}\index{\(b\) is an upper bound of \(S\subseteq \RR\) if and only if \(-b\) is a lower bound of \(-S\)} Let \(S\subseteq\RR\) and let \(T=\{-x|\,x\in S\}\).%
\begin{enumerate}[label={(\alph*)}]
\item{}Prove that \(b\) is an upper bound of \(S\) if and only if \(-b\) is a lower bound of \(T\).%
\item{}Prove that \(b=\sup S\) if and only if \(-b=\inf T\).%
\end{enumerate}
%
\end{problem}
\end{sectionptx}
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 10 Limits, Derivatives, Integrals, and the Fundamental Theorem of Calculus}
\typeout{************************************************}
%
\begin{chapterptx}{Chapter}{Limits, Derivatives, Integrals, and the Fundamental Theorem of Calculus}{}{Limits, Derivatives, Integrals, and the Fundamental Theorem of Calculus}{}{}{LimDerivIntFTC}
\renewcommand*{\chaptername}{Chapter}
\begin{introduction}{Why Now?}%
In \hyperref[CalcIn17th18thCentury]{Chapter~{\xreffont\ref{CalcIn17th18thCentury}}} we briefly discussed the invention of Calculus by Newton and Leibniz.  In some of the subsequent examples and problems we have used the fundamental notions of \terminology{limit}, \terminology{derivative} and \terminology{integral} without going to the trouble of rigorously defining these concepts because you should be familiar with their usage from your Calculus course, even if the details of their definitions are still a bit hazy. Which they probably are.%
\par
We (the authors) made this choice primarily because we feel strongly that it is better for beginners to learn to ``play the game'' of analysis with epsilons and deltas before confronting all of the nuance and complexity of a rigorous treatment of these ideas.  It is also a more historically accurate treatment.%
\par
In your Calculus class rigor could (mostly) be dispensed The underlying ideas of the \terminology{limit}, the \terminology{derivative} and the \terminology{integral}, are relatively intuitive. But an intuitive understanding is simply not enough for our purposes here. We will need rigor.%
\par
The formal definitions of these ideas were never meant to be intuitive and they are not. They are intended to establish a rigorous foundation for Calculus, a foundation we can fall back on when an intuitive approach is inadequate.%
\par
So we began by relying heavily on the intuitive understanding of derivatives and integrals you gained from your Calculus course, and we used that to derive%
\begin{itemize}[label=\textbullet]
\item{}the integral, Lagrange, and Cauchy form of the remainders of the Taylor series,%
\item{}the Intermediate Values Theorem,%
\item{}the Extreme Value Theorem%
\item{}the convergence of sequences,%
\item{}continuity, and%
\item{}the completeness of the real number system.%
\end{itemize}
Now that we have all of these tools in place it is time for us to go back and build the  foundations of Calculus rigorously.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Section 10.1 The Definition of the Limit of a Function}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{The Definition of the Limit of a Function}{}{The Definition of the Limit of a Function}{}{}{Continuity-DefLimit}
Because these days the limit concept is generally regarded as the starting point for Calculus, you might think it is a little strange that we've chosen to talk about continuity first. But historically, the formal definition of a limit came after the formal definition of continuity.   The limit concept has become foundational to Calculus.%
\par
To be sure, limits were always lurking in the background.  In his attempts to justify his calculations, Newton \index{Newton, Isaac} used what he called his doctrine of \terminology{Ultimate Ratios}. For example, the ratio \(\frac{(x+h)^2-x^2}{h} =
\frac{2xh+h^2}{h} = 2x+h\) becomes \(2x\) \terminology{ultimately}, or at the last instant of time before \(h\) becomes zero. Newton would have called \(h\) an ``evanescent quantity'' \textemdash{} vanishes~(\hyperlink{grabiner81__origin_cauch_rigor_calculy}{[{\xreffont 4}]}, p. 33).%
\par
Similarly Leibniz's\index{Leibniz, Gottfried Wilhelm} ``infinitely small'' differentials \(\dx{
x}\) and \(\dx{ y}\) can be seen as an attempt to get ``arbitrarily close'' to \(x\) and \(y\), respectively. This is the idea at the heart of Calculus: to get arbitrarily close to, say, \(x\) without actually reaching it.%
\par
\index{Lagrange, Joseph-Louis}\index{Cauchy, Augustin} As we saw in \hyperref[PowerSeriesQuestions]{Chapter~{\xreffont\ref{PowerSeriesQuestions}}}, \href{https://mathshistory.st-andrews.ac.uk/Biographies/Lagrange/}{Lagrange}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Lagrange/}\label{Continuity-DefLimit-5-5}} tried to avoid the entire issue of ``arbitrary closeness,'' both in the limit and differential forms when, in 1797, he attempted to found Calculus on infinite series. Although Lagrange's efforts failed, they set the stage for \href{https://mathshistory.st-andrews.ac.uk/Biographies/Cauchy/}{Cauchy}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Cauchy/}\label{Continuity-DefLimit-5-9}} to provide a definition of derivative which in turn relied on his precise formulation of a limit.  Consider the following example.%
\begin{example}{Example}{}{EXAMPLESyncFunction}%
We wish to determine the slope of the tangent line (derivative) of \(f(x) = \sin x\) at \(x=0\) so we form the usual difference quotient: \(D(x)=\frac{\sin x -
\sin 0}{x-0}\). Now consider the graph of this difference quotient%
\begin{equation*}
D(x) =\frac{\sin x - \sin 0}{x-0}=\frac{\sin x
}{x}\text{.}
\end{equation*}
%
\begin{image}{0.22}{0.56}{0.22}{}%
\includegraphics[width=\linewidth]{external/images/SinGraph.png}
\end{image}%
From the graph, it appears that \(D(0) =1\) but we must be careful.  \(D(0)\) doesn't even exist!  Somehow we must convey the idea that \(D(x)\) will approach \(1\) as \(x\) approaches \(0\), even though the quotient \(D(x)\) is not defined at \(0\). Cauchy's idea was that the limit of \(D(x)\) would equal \(1\) because we can make \(D(x)\) differ from 1 by as little as we wish by taking \(x\) sufficiently close to zero ~(\hyperlink{jahnke03__histor_analy}{[{\xreffont 6}]}, p. 158).%
\end{example}
\index{Weierstrass, Karl} \href{https://mathshistory.st-andrews.ac.uk/Biographies/Weierstrass/}{Karl Weierstrass}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Weierstrass/}\label{Continuity-DefLimit-7-3}} made these ideas precise in his lectures on analysis at the University of Berlin (1859-60) and provided us with our modern formulation.%
\begin{definition}{Definition}{Limit.}{def_limit}%
\index{limit}%
We say \(\limit{x}{a}{f(x)} =L\) provided that for each \(\eps>0\), there exists \(\delta>0\) such that if \(0\lt \abs{x-a}\lt \delta\) then \(\abs{f(x)-L}\lt
\eps\).%
\end{definition}
Before we delve into this, notice that it is very similar to the  \hyperref[def_continuity]{definition of continuity at a point}.   In fact we can readily see that \(f
\text{ is continuous at } x=a \text{ if and only if }
\limit{x}{a}{f(x)} = f(a)\).%
\begin{aside}{Aside}{Comment.}{Continuity-DefLimit-10}%
This presumes that \(a\) is an \terminology{accumulation point} of the domain of \(f\) (\hyperref[def_accumulation-point]{Definition~{\xreffont\ref{def_accumulation-point}}}).  We will discuss accumulation points in \hyperref[BackToFourier]{Chapter~{\xreffont\ref{BackToFourier}}}.%
\end{aside}
There are two differences between this definition and the definition of continuity and they are related.  The first is that we replace the value \(f(a)\) with \(L\).  This is because the function may not be defined at \(a\).  In a sense the limiting value \(L\) is the value \(f\) would have \emph{if it were defined and continuous at \(a\).} The second is that we have replaced%
\begin{equation*}
\abs{x-a}\lt \delta
\end{equation*}
with%
\begin{equation*}
0\lt \abs{x-a}\lt \delta \text{.}
\end{equation*}
%
\par
Again, since \(f\) needn't be defined at \(a\), we will not even consider what happens when \(x=a\).  This is the only purpose for this change.%
\par
As with the definition of the limit of a sequence, this definition does not determine what \(L\) is, it only verifies that your guess for the value of the limit is correct.%
\par
Finally, a few comments on the differences and similiarities between this limit and the limit of a sequence are in order, if for no other reason than because we use the same notation (\(\lim\)) for both.%
\par
When we were working with sequences in \hyperref[Convergence]{Chapter~{\xreffont\ref{Convergence}}} and wrote things like \(\limit{n}{\infty}{a_n}\) we were thinking of \(n\) as an integer that got bigger and bigger. To put that more mathematically, the limit parameter \(n\) was taken from the set of positive integers, or \(n\in \NN\).%
\par
For both continuity and the limit of a function we write things like \(\limit{x}{a}{f(x)}\) and think of \(x\) as a variable that gets arbitrarily close to the number \(a\).  Again, to be more mathematical in our language we would say that the limit parameter \(x\) is taken from the \(\ldots\) Well, actually, this is interesting isn't it?  Do we need to take \(x\) from \(\QQ\) or from \(\RR?\) The requirement in both cases is simply that we be able to choose \(x\) arbitrarily close to \(a\).  From \hyperref[thm_IrrationalBetweenIrrationals]{Theorem~{\xreffont\ref{thm_IrrationalBetweenIrrationals}}} of \hyperref[NumbersRealRational]{Chapter~{\xreffont\ref{NumbersRealRational}}} we see that this is possible whether \(x\) is rational or not, so it seems either will work.  This leads to the pardoxical sounding conclusion that we do not need a continuum \((\RR)\) to have continuity. This seems strange.%
\par
Before we look at the above example, let's look at some algebraic examples to see the \hyperref[def_limit]{Definition~{\xreffont\ref{def_limit}}}  use.%
\begin{example}{Example}{}{Continuity-DefLimit-18}%
Consider the function \(D(x)=\frac{x^2-1}{x-1}\), \(x\neq 1\).  You probably recognize this as the difference quotient used to compute the derivative of \(f(x)=x^2\) at \(x=1\), so we strongly suspect that \(\limit{x}{1}{\frac{x^2-1}{x-1}}=2\). Just as when we were dealing with limits of sequences, we should be able to use the definition to verify this.  And as before, we will start with some scrapwork.%
\par
\terminology{SCRAPWORK}%
\par
Let \(\eps>0\).  We wish to find a \(\delta>0\) such that if \(0\lt \abs{x-1}\lt \delta\) then \(\abs{\frac{x^2-1}{x-1}-2}\lt \eps\).  With this in mind, we perform the following calculations%
\begin{equation*}
\abs{\frac{x^2-1}{x-1}-2}=\abs{(x+1)-2} = \abs{x-1} \text{.}
\end{equation*}
%
\par
Now we have a handle on \(\delta\) that will work in the definition and we'll give the formal proof that%
\begin{equation*}
\limit{x}{1}{\frac{x^2-1}{x-1}}=2 \text{.}
\end{equation*}
%
\end{example}
\begin{proof}{Proof}{}{Continuity-DefLimit-19}
Let \(\eps>0\) and let \(\delta=\eps\).  If \(0\lt
\abs{x-1}\lt \delta\), then%
\begin{equation*}
\abs{\frac{x^2-1}{x-1}-2}=\abs{(x+1)-2}=\abs{x-1}\lt
\delta=\eps \text{.}
\end{equation*}
%
\end{proof}
As in our previous work with sequences and continuity, notice that the scrapwork is not part of the formal proof (though it was necessary to determine an appropriate \(\delta)\).  Also, notice that \(0\lt \abs{x-1}\) was not really used except to ensure that \(x\neq 1\).%
\begin{problem}{Problem}{}{Continuity-DefLimit-21}%
Use the definition of a limit to verify that%
\begin{equation*}
\limit{x}{a}{\frac{x^2-a^2}{x-a}}=2a.{} 
\end{equation*}
%
\end{problem}
\begin{problem}{Problem}{}{Continuity-DefLimit-22}%
Use the definition of a limit to verify each of the following limits.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\limit{x}{1}{\frac{x^3-1}{x-1}}=3\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{Continuity-DefLimit-22-3-2}{}\quad{}%
\begin{align*}
\abs{\frac{x^3-1}{x-1}-3} \amp =
\abs{x^2+x+1-3} \\
\amp \leq\abs{x^2-1}+\abs{x-1}\\
\amp =\abs{(x-1+1)^2-1}+\abs{x-1} \\
\amp =\abs{(x-1)^2+2(x-1)}+\abs{x-1} \\
\amp
\leq\abs{x-1}^2 + 3\abs{x-1} \text{.}
\end{align*}
%
\item{}\(\limit{x}{1}{\frac{\sqrt{x}-1}{x-1}}=1/2\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{Continuity-DefLimit-22-4-2}{}\quad{}%
\begin{align*}
\abs{\frac{\sqrt{x}-1}{x-1}-\frac12}\amp =
\abs{\frac{1}{\sqrt{x}+1}-\frac12} \\
\amp
=\abs{\frac{2-\left(\sqrt{x}+1\right)}{2\left(\sqrt{x}+1\right)}}\\
\amp
=\abs{\frac{1-x}{2\left(1+\sqrt{x}\right)^2}} \\
\amp \leq\frac12\abs{x-1}.  
\end{align*}
%
\end{enumerate}%
\end{problem}
Our definition of a limit, although it is rigorous, is quite cumbersome to use. What we want to do is develop some tools we can use without having to refer directly to \hyperref[def_limit]{Definition~{\xreffont\ref{def_limit}}}.  One such tool is \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}} which allows us to show that a function is continuous (or discontinuous) at a point by examining certain sequences. We will need others.%
\par
As we  observed before if a function \(f(x)\), is not defined at \(x=a\), and therefore is discontinuous at \(x=a\), but \(\limit{x}{a}{f(x)}=L \) then we can make it continuous by defining \(f(a)=L\). Combining this with \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}} we have the following corollary:                 %
\begin{corollary}{Corollary}{}{}{cor_limit-by-sequences}%
\(\limit{x}{a}{f(x)}=L\) if and only if \(f\) satisfies the following property:%
\begin{equation*}
\forall \text{ sequences } (x_n),
x_n\ne a, \text{ if } \limit{n}{\infty}{x_n}=a \text{ then }
\limit{n}{\infty}{f(x_n)}=L. {} 
\end{equation*}
%
\end{corollary}
Armed with this, we can prove the following familiar limit theorems from Calculus.%
\begin{theorem}{Theorem}{}{}{thm_CalcLimits}%
\index{limit!properties of} Suppose \(\limit{x}{a}{f(x)}=L\) and \(\limit{x}{a}{g(x)}=M\), then%
\begin{enumerate}[label={(\alph*)}]
\item{}\(\displaystyle \limit{x}{a}{\left(f(x)+g(x)\right)}=L+M\)%
\item{}\(\displaystyle \limit{x}{a}{\left(f(x)\cdot g(x)\right)}=L\cdot M\)%
\item{}\(\limit{x}{a}{\left(\frac{f(x)}{g(x)}\right)}=L/M\) provided \(M\ne0\) and \(g(x)\ne{}0\), for \(x\) sufficiently close to a (but not equal to \(a\)).%
\end{enumerate}
%
\end{theorem}
We will prove part (a) to give you a feel for this and let you prove parts (b) and (c).%
\begin{proof}{Proof}{}{Continuity-DefLimit-29}
Let \(\left(x_n\right)\) be a sequence such that \(x_n\ne a\) and \(\limit{n}{\infty}{x_n}=a\).  Since \(\limit{x}{a}{f(x)} = L\) and \(\limit{x}{a}{g(x)} = M\) we see that \(\limit{n}{\infty}{f(x_n)} = L\) and \(\limit{n}{\infty}{g(x_n)} = M\).  By \hyperref[thm_SumOfSequences]{Theorem~{\xreffont\ref{thm_SumOfSequences}}} of \hyperref[Convergence]{Chapter~{\xreffont\ref{Convergence}}}, we have \(\limit{n}{\infty}{f(x_n)+g(x_n)}=L+M\).  Since \(\left\{x_n\right\}\) was an arbitrary sequence with \(x_n\ne a\) and \(\limit{n}{\infty}{x_n} = a\) we have%
\begin{equation*}
\limit{x}{a}{f(x)+g(x)} = L+M \text{.}
\end{equation*}
%
\end{proof}
\begin{problem}{Problem}{}{Continuity-DefLimit-30}%
\index{limit!properties of}\index{limit!verify limit laws from Calculus} Prove parts (b) and (c) of \hyperref[thm_CalcLimits]{Theorem~{\xreffont\ref{thm_CalcLimits}}}.%
\end{problem}
More in line with our current needs, we have a reformulation of the Squeeze Theorem.%
\begin{theorem}{Theorem}{}{}{thm_SqueezeTheoremFunctions}%
\alert{Squeeze Theorem for functions}%
\par
\index{Squeeze Theorem!for functions} Suppose \(f(x)\le g(x) \le h(x)\), for \(x\) sufficiently close to \(a\) (but not equal to \(a\)).  If \(\limit{x}{a}{f(x)}=L=\limit{x}{a}{h(x)}\), then \(\limit{x}{a}{g(x)}=L\) also.%
\end{theorem}
\begin{problem}{Problem}{}{Continuity-DefLimit-33}%
\index{Squeeze Theorem!for functions} Prove the \hyperref[thm_SqueezeTheoremFunctions]{Squeeze Theorem for functions}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{Continuity-DefLimit-33-2}{}\quad{}Use \hyperref[thm_SqueezeTheorem]{Theorem~{\xreffont\ref{thm_SqueezeTheorem}}}, the Squeeze Theorem for sequences from \hyperref[Convergence]{Chapter~{\xreffont\ref{Convergence}}}.%
\end{problem}
Returning to \hyperref[EXAMPLESyncFunction]{Example~{\xreffont\ref{EXAMPLESyncFunction}}} we see that the Squeeze Theorem is just what we need.  First notice that since \(D(x)=\sin x/x\) is an even function, we only need to focus on \(x>0\) in our inequalities.  Consider the unit circle.%
\begin{image}{0.2}{0.6}{0.2}{}%
\includegraphics[width=\linewidth]{external/images/UnitCircle.png}
\end{image}%
\begin{problem}{Problem}{}{Continuity-DefLimit-36}%
Use the fact that%
\begin{equation*}
\text{ area } (\Delta OAC)\lt \text{ area } (\text{ sector }
OAC)\lt \text{ area } (\Delta OAB) 
\end{equation*}
to show that if \(0\lt
x\lt \pi/2\), then \(\cos x\lt \sin x/x\lt 1\).  Use the fact that all of these functions are even to extend the inequality for \(-\pi/2\lt x\lt 0\) and use the Squeeze Theorem to show \(\limit{x}{0}{\textstyle\frac{\sin
x}{x}}=1\).%
\end{problem}
\begin{problem}{Problem}{}{PROBLEMBasicLimits}%
Suppose \(\limit{x}{a}{ f(x)}=L\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Prove that if \(L\gt0\), then there exists a \(\delta >0\), such that if \(0\lt\left|x-a\right|\lt\delta \), then \(f\left(x\right)>0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PROBLEMBasicLimits-2-2}{}\quad{}Try \(\delta =\frac{L}{2}\).%
\item{}Prove that if \(L\lt0\), then there exists a \(\delta >0\), such that if \(0\lt\left|x-a\right|\lt\delta\), then \(f\left(x\right)\lt0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PROBLEMBasicLimits-3-2}{}\quad{}Consider \(-f(x)\).%
\item{}Notice that if \(\limit{x}{a}{f(x)}=L\), then the contrapositive of part (a) says that if for each \(\delta >0\), there is an \(x\) with \(0\lt\left|x-a\right|\lt\delta \) and \(f\left(x\right)\le
0\), then \(L\le 0\).%
\par
What does the contrapositive of part (b) say?%
\end{enumerate}%
\end{problem}
\begin{definition}{Definition}{Near.}{DEFINITIONNear}%
We say that a function \(f(x)\) has a property for \(x\) \terminology{near} \(a\), if there exists a \({\delta }_0>0\) such that \(f(x)\) has that property for all \(x\) with \(0\lt\left|x-a\right|\lt{\delta
}_0\)%
\end{definition}
\begin{problem}{Problem}{}{Continuity-DefLimit-39}%
Prove that the following is also a consequence of \hyperref[PROBLEMBasicLimits]{Problem~{\xreffont\ref{PROBLEMBasicLimits}}}.  Suppose \(\limit{x}{a}{f(x)}=L\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}If \(f\left(x\right)\le 0\) for \(x\) near \(a\), then \(L\le 0\).%
\item{}If \(f\left(x\right)\ge 0\) for \(x\) near \(a\), then \(L\ge 0\).%
\end{enumerate}%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 10.2 The Definition of  the Derivative and the Mean Value Theorem}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{The Definition of  the Derivative and the Mean Value Theorem}{}{The Definition of  the Derivative and the Mean Value Theorem}{}{}{Continuity-DerivativeAfterthought}
As we have mentioned in \hyperref[CalcIn17th18thCentury-NewtLeibStart]{Section~{\xreffont\ref{CalcIn17th18thCentury-NewtLeibStart}}}  Leibniz invented his \textit{calculus differentialis} (differential calculus \textemdash{} literally ``rules for (infinitely small) differences'') in the 1600s.%
\par
In the late 1700s Lagrange tried to provide a rigorous foundation for Calculus by discarding differential ratios like the expression \(\dfdx{y}{x} \) in favor of his own ``prime notation'' (\(y^\prime \)). Thus it was Lagrange who established functions and limits, rather than the curves and differentials favored by Leibniz and Newton, as foundational ideas of Calculus%
\par
When you took Calculus you spent at least an entire semester learning about the properties of the derivative and how to use them to explore the properties of functions so there is no need to repeat that effort here. Instead we will establish a rigorous, formal foundation for the derivative concept in terms of limits.%
\begin{definition}{Definition}{The Derivative.}{def_derivative}%
\index{differentiation!definition of the derivative}%
Given a function \(f(x)\) defined on an interval \((a,b)\) we define%
\begin{equation*}
f^\prime(x) =
\limit{h}{0}{\frac{f(x+h)-f(x)}{h}}.{} 
\end{equation*}
%
\end{definition}
There are a few fairly obvious facts about this definition which are nevertheless worth noticing explicitly:%
\par
%
\begin{enumerate}
\item{}If the limit \(f^\prime (x)\) exists at \(x\), then we say that \(f\) is \terminology{differentiable} at \(x\).%
\item{}The derivative is defined \emph{at a point.} If the derivative of \(f(x)\)  is defined at every point in an interval \((a,b)\) then we say that \(f\) is \terminology{differentiable on the interval} \((a,b)\).%
\item{}Since it is defined at a point it is at least theoretically possible for a function to be differentiable at a single point in its entire domain.%
\item{}Since it is defined as a limit and not all limits exist, functions are not necessarily differentiable.%
\item{}Since it is defined as a limit, \hyperref[cor_limit-by-sequences]{Corollary~{\xreffont\ref{cor_limit-by-sequences}}} applies.  That is, \(f^\prime(x)\) exists if and only if \(\forall \text{ sequences } (h_n),\, h_n\ne
0\), if \(\limit{n}{\infty}{h_n}=0\) then%
\begin{equation*}
f^\prime{(x)} =
\limit{n}{\infty}{\frac{f(x+h_n)-f(x)}{h_n}} \text{.}
\end{equation*}
Since \(\limit{n}{\infty}{h_n}=0\) this could also be written as%
\begin{equation*}
f^\prime{(x)} =
\limit{h_n}{0}{\frac{f(x+h_n)-f(x)}{h_n}}\text{.}
\end{equation*}
%
\end{enumerate}
%
\par
If we make the substitution \(y=x+h\) in \hyperref[def_derivative]{Definition~{\xreffont\ref{def_derivative}}}  we obtain the following equivalent definition, which is sometimes easier to use.%
\begin{definition}{Definition}{The Derivative, An Alternative Definition.}{DEFINITIONdef_derivative}%
Given a function \(f(x)\) defined on an interval \((a,b)\), and a point \(x\in (a,b)\), the derivative of \(f\) is given by%
\begin{equation*}
f^\prime(x)=\limit{y}{x}{\frac{f(y)-f(x)}{y-x}}\text{.}
\end{equation*}
%
\end{definition}
\begin{theorem}{Theorem}{}{}{thm_DiffImpCont}%
\index{continuity!implied by differentiability}%
\index{differentiation!differentiability implies continuity}%
\alert{Differentiability Implies Continuity}%
\par
If \(f\) is differentiable at a point \(c\) then \(f\) is continuous at \(c\) as well.%
\end{theorem}
\begin{problem}{Problem}{}{Continuity-DerivativeAfterthought-11}%
Prove \hyperref[thm_DiffImpCont]{Theorem~{\xreffont\ref{thm_DiffImpCont}}}%
\end{problem}
As we mentioned, the derivative is an extraordinarily useful mathematical tool but it is not our intention to learn to \emph{use} it here.  Our purpose here is to define it rigorously (done) and to show that our formal definition does in fact recover the useful properties you came to know and love in your Calculus course.%
\par
The first such property is known as Fermat's Theorem.%
\begin{theorem}{Theorem}{Fermat's   Theorem.}{}{thm_FermatsTheorem}%
\index{Fermat's Theorem}%
Suppose \(f\) is differentiable on \((a,b)\) and \(f\) has an extremum at \(c\in (a,b)\).  Then \(f^\prime\left(c\right)=0\).%
\end{theorem}
\begin{proof}{Proof}{Sketch of Proof.}{Continuity-DerivativeAfterthought-15}
There are two cases:%
\begin{descriptionlist}
\begin{dlimedium}{Case 1:}{Continuity-DerivativeAfterthought-15-2-1-1}%
\(f(c)\) is a maximum, and%
\end{dlimedium}%
\begin{dlimedium}{Case 2:}{Continuity-DerivativeAfterthought-15-2-1-2}%
\(f(c)\) is a minimum.%
\end{dlimedium}%
\end{descriptionlist}
%
\par
Suppose \(f(c)\) is a maximum so that \(f\left(c\right)\ge f(x)\) for all \(x\in (a,b)\).  Since \(f\) is differentiable at \(c\), then we have%
\begin{equation*}
f^\prime\left(c\right)=\limit{x}{c}{ \frac{f(x)-f(c)}{x-c}}
\end{equation*}
%
\par
To show \(f^\prime\left(c\right)=0\), we need to show that%
\begin{align*}
f^\prime\left(c\right)\le 0 \amp{}\amp{} \text{and}\amp{}\amp{}
f^\prime\left(c\right)\ge 0.
\end{align*}
These facts follow directly from parts (a) and (b) of \hyperref[PROBLEMBasicLimits]{Problem~{\xreffont\ref{PROBLEMBasicLimits}}} and the case where \(f(c)\) is a minimum can be handled by looking at \(-f\)..        %
\end{proof}
\begin{problem}{Problem}{}{PROBLEMFermatsTheorem}%
Provide a formal proof for \hyperref[thm_FermatsTheorem]{Fermat's}.%
\end{problem}
Many of the most important properties of the derivative follow from what is called the \terminology{Mean Value Theorem} (MVT) stated below.%
\begin{theorem}{Theorem}{The Mean Value Theorem (\initialismintitle{MVT}).}{}{thm_MVT}%
\index{Mean Value Theorem, the}%
Suppose \(f^\prime\) exists for every \(x\in(a,b)\) and \(f\) is continuous on \([a,b]\).  Then there is a real number \(c\in(a,b)\) such that%
\begin{equation*}
f^\prime(c)=\frac{f(b)-f(a)}{b-a}.{} 
\end{equation*}
%
\end{theorem}
It would be difficult to prove the MVT right now,  so we will first state and prove Rolle's Theorem, which can be seen as a special case of the MVT. The proof of the MVT will then follow easily.%
\par
\href{https://mathshistory.st-andrews.ac.uk/Biographies/Rolle/}{Michel Rolle}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Rolle/}\label{Continuity-DerivativeAfterthought-20-2}} (1652\textendash{}1719) first stated the following theorem in 1691.  Given this date and the nature of the theorem it would be reasonable to suppose that Rolle was one of the early developers of Calculus but this is not so.  In fact, Rolle was disdainful of both Newton \index{Newton, Isaac} and Leibniz's\index{Leibniz, Gottfried Wilhelm} versions of Calculus, once deriding them as a collection of ``ingenious fallacies.'' It is a bit ironic that his theorem is so fundamental to the modern development of the Calculus he ridiculed.%
\begin{theorem}{Theorem}{Rolle's Theorem.}{}{thm_Rolle_s_Theorem}%
\index{Rolle's Theorem}%
Suppose \(f^\prime\) exists for every \(x\in(a,b)\), \(f\) is continuous on \([a,b]\), and%
\begin{equation*}
f(a)=f(b) \text{.}
\end{equation*}
%
\par
Then there is a real number \(c\in(a,b)\) such that%
\begin{equation*}
f^\prime(c)=0 \text{.}
\end{equation*}
%
\end{theorem}
\begin{proof}{Proof}{Sketch of Proof.}{Continuity-DerivativeAfterthought-22}
By the \hyperref[thm_EVT]{EVT}, we know that \(f\) has a maximum \(M\), and a minimum \(m\), on \([a,b]\).  Suppose that both occur at the endpoints.  This would say that \(m=M\) and \(f\) is constant on \([a,b]\).  What does this say about \(f^\prime\)?%
\par
On the other hand, what does \hyperref[thm_FermatsTheorem]{Fermat's Theorem} say if one or both of these extrema is not at an endpoint?%
\end{proof}
\begin{problem}{Problem}{Rolle's Theorem.}{PROBLEMRollesTheorem}%
Turn the ideas in the sketch above into a proof of \hyperref[thm_Rolle_s_Theorem]{Rolle's Theorem}.%
\end{problem}
We can now prove the MVT as a corollary of Rolle's Theorem.  We only need to find the right function to apply Rolle's Theorem to.  The following figure shows a function, \(f(x)\), cut by a secant line, \(L(x)\), from \((a, f(a))\) to \((b,f(b))\).%
\begin{image}{0.3}{0.4}{0.3}{}%
\includegraphics[width=\linewidth]{external/images/MVT.png}
\end{image}%
The vertical difference from \(f(x)\) to the secant line, indicated by \(\phi(x)\) in the figure should do the trick.  You take it from there.%
\begin{problem}{Problem}{}{Continuity-DerivativeAfterthought-27}%
Prove the \hyperref[thm_MVT]{Mean Value Theorem}.%
\end{problem}
Notice that the MVT is a generalization of Rolle's Theorem or, put another way, Rolle's Theorem is a special case of the  MVT.%
\par
The Mean Value Theorem is extraordinarily useful.  Almost all of the properties of the derivative that you used in Calculus follow more or less directly from it.  For example the following is true.%
\begin{corollary}{Corollary}{}{}{cor_PosDerivIncFunc1}%
If \(f^\prime(x) > 0\) for every \(x\) in the interval \((a,b)\) then for every \(c,d\in(a,b)\) where \(d>c\) we have%
\begin{equation*}
f(d) > f(c) \text{.}
\end{equation*}
%
\par
That is, \(f\) is increasing on \((a,b)\).%
\end{corollary}
\begin{proof}{Proof}{}{Continuity-DerivativeAfterthought-31}
Suppose \(c\) and \(d\) are as described in the corollary. Then by the Mean Value Theorem there is some number, say \(\alpha\in(c,d)\subseteq(a,b)\) such that%
\begin{equation*}
f^\prime(\alpha)=\frac{f(d)-f(c)}{d-c} \text{.}
\end{equation*}
%
\par
Since \(f^\prime(\alpha)>0\) and \(d-c>0\) we have \(f(d)-f(c)>0\), or \(f(d)>f(c)\).%
\end{proof}
\begin{problem}{Problem}{}{Continuity-DerivativeAfterthought-32}%
\index{differentiation!if \(f^\prime\lt 0\) on an interval then \(f\) is decreasing} Show that if \(f^\prime(x) \lt 0\) for every \(x\) in the interval \((a,b)\) then \(f\) is decreasing on \((a,b)\).%
\end{problem}
\begin{corollary}{Corollary}{}{}{cor_PosDerivIncFunc2}%
Suppose \(f\) is differentiable on some interval \((a,b)\), \(f^\prime\) is continuous on \((a,b)\), and that \(f^\prime(c)>0\) for some \(c\in (a,b)\). Then there is an interval, \(I\subset (a,b)\), containing \(c\) such that for every \(x, y\) in \(I\) where \(x\ge y\), \(f(x)\ge f(y)\).%
\end{corollary}
\begin{problem}{Problem}{}{Continuity-DerivativeAfterthought-34}%
\index{differentiation!\(f^\prime(a)>0\) implies \(f\) is increasing nearby} Prove \hyperref[cor_PosDerivIncFunc2]{Corollary~{\xreffont\ref{cor_PosDerivIncFunc2}}}.%
\end{problem}
\begin{problem}{Problem}{}{Continuity-DerivativeAfterthought-35}%
\index{differentiation!\(f^\prime(a)\lt 0\) implies \(f\) is decreasing nearby} Suppose that \(f\) is differentiable on some interval \((a,b)\), and \(f^\prime \) is continuous on \((a,b)\). If \(f^\prime(c)\lt 0\) for some \(c\in
(a,b)\) then there is an interval, \(I\subset (a,b)\), containing \(c\) such that for every \(x, y\) in \(I\) where \(x\ge y\), \(f(x)\le f(y)\).%
\end{problem}
\begin{problem}{Problem}{}{DRILLZeroDerivImpConst}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Suppose \(f(x)\) is continuous on \([a,b]\) and \(f^\prime(x)=0\) on \((a,b)\).  Show that \(f(x)\) is constant on \([a,b]\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{DRILLZeroDerivImpConst-1-2}{}\quad{}Show that for any \(x, y\in [a,b]\), \(x\neq y\), \(f(x)=f(y)\).%
\item{}Consider%
\begin{equation*}
f(x)=
\begin{cases}
\frac{\abs{x} }{x}\amp \text{ if } x\neq 0\\
0\amp \text{ if } x=0
\end{cases}
\end{equation*}
   %
\par
Show that \(f^\prime(x)=0\) for \(x\neq 0\).  Why doesn't this contradict part (a)?%
\item{}Suppose \(f(x)\) and \(g(x)\) are continuous on \([a,b]\) with \(f^\prime(x)=g^\prime(x)\) on \((a,b)\).  Show that \(f(x)=g(x)+C\) for some constant \(C\) on \([a,b]\).%
\end{enumerate}%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 10.3 The Fundamental Theorem of Calculus}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{The Fundamental Theorem of Calculus}{}{The Fundamental Theorem of Calculus}{}{}{SECTIONFTC}
If you look back at the derivation of the Integral Form of the Remainder for Taylor Series (\hyperref[TaylorsTheorem]{Theorem~{\xreffont\ref{TaylorsTheorem}}}) you'll see that the \hyperref[THEOREMFTCCauchy]{Fundamental Theorem of Calculus} was our anchoring step, even though we had not yet proved it. In fact we still haven't, but we assumed you were familiar with it from your Calculus course.%
\par
The Fundamental Theorem of Calculus was understood (in at least the limited context of polynomials) before Newton and Leibniz invented Calculus. As a result neither of them dubbed it a ``Fundamental Theorem.'' For them it was simply one of those known results that their Calculus re\textendash{}captured in a more elegant form than had previously been known. They both provided derivations of it via their versions of Calculus, but neither of them dubbed it ``The Fundamental Theorem'' because for them it was a relatively obvious consequence of the way they thought about such things. Both considered it very natural and obvious that areas can be found by antidifferentiation.%
\par
Using the differential and integral notation that Leibniz invented and we still use today it is easy to see why. If we suppose that%
\begin{equation*}
\dfdx{Y}{x}=y\text{,}
\end{equation*}
then it follows that%
\begin{equation}
y\dx{x}=\dx{Y} \text{.}\label{EQUATIONFTCDifferentialEquality}
\end{equation}
Notice that \hyperref[EQUATIONFTCDifferentialEquality]{equation~({\xreffont\ref{EQUATIONFTCDifferentialEquality}})} states that two differentials are equal. Thus it seems apparent that if we add (integrate) together all such differentials between \(x=a\) and \(x=b\) we have (again employing Leibniz' notation)%
\begin{equation}
\int^b_{x=a}{y\dx{x}}=\int^b_{x=a}{\dx{Y}}\label{EQUATIONFTCIntDiffer}
\end{equation}
%
\begin{aside}{Aside}{Comment.}{SECTIONFTC-5}%
We said that \hyperref[EQUATIONFTCIntDiffer]{equation~({\xreffont\ref{EQUATIONFTCIntDiffer}})} uses Leibniz' notation but this is not entirely correct. Fourier innovated the use of upper and lower indices on the integral sign to show the limits of the integration approximately 150 years later. Leibniz didn't use them.%
\par
\hyperref[EQUATIONFTCIntDiffer]{Equation~({\xreffont\ref{EQUATIONFTCIntDiffer}})} indicates that we are summing all of the respective differentials between \(x=a\) and \(x=b\).%
\end{aside}
Since  a finite sum of finite differences collapses into the difference of the extremes:%
\begin{equation*}
\left(a_2-a_1\right)+\left(a_3-a_2\right)+\dots
+\left(a_{n-1}-a_{n-2}\right)+\left(a_n-a_{n-1}\right)=a_n-a_1\text{.}
\end{equation*}
Leibniz assumed that this is also true for an infinite sum of infinitesimals. This is the most intuitive understanding of the Fundamental Theorem of Calculus. In Leibniz' notation it is%
\begin{equation}
\int^b_{x=a}{y\dx{x}}=\int^b_{x=a}{\dx{Y}}=Y\left(b\right)-Y(a)\text{.}\label{EQUATIONFTCLeibniz}
\end{equation}
%
\par
For Leibniz, this is all so natural and obvious that when he wrote about it in his 1693 paper \textit{Supplementum geometriae dimensoriae, seu generalissima omnium tetragonismorum effectio per motum: similiterque multiplex constructio lineae ex data tangentium conditione},     he called it a ``supplementum'' (supplement, or corollary) rather than something more  imposing \textemdash{} like the \terminology{Fundamental Theorem of Calculus}.%
\par
Leibniz included a diagram to support this result, but he rather famously favored very complex diagrams in his publications.  We provide a simpler, more modern rendition below.%
\begin{figureptx}{Figure}{A visual interpretation of the Fundamental Theorem of Calculus as it was understood by Leibniz.  The relationship between the curves is that the function on the  left, \(y=y(x)\), is the  derivative of the function  on the right, \(Y=Y(x)\).}{FIGUREFTC}{}%
\begin{sidebyside}{2}{0.025}{0.025}{0.05}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{external/images/FTC1.png}
\end{sbspanel}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{external/images/FTC2.png}
\end{sbspanel}%
\end{sidebyside}%
\tcblower
\end{figureptx}%
In \hyperref[FIGUREFTC]{Figure~{\xreffont\ref{FIGUREFTC}}} the area of the infinitely thin rectangle on the left is given by \(y\dx{x}\) and is numerically equal to the infinitely small length \(\dx{Y}\) on the right.  Adding the areas on the left gives the area under the curve \(y(x)\) between \(a\) and \(b\).  The sum of the lengths on the right gives the length of the line segment, also between \(a\) and \(b\): \(Y\left(b\right)-Y(a)\).%
\par
Such an approach does not pass modern, or even 19th century, standards of rigor.  Even in the 17th century it was known that there are logical problems with interpreting an integral in terms of infinitiesimals. But the infinitesimal approach was adequate to the needs of the time so a closer investigation into the nature of the integral was left until infinitesimals were no longer sufficient.%
\begin{problem}{Problem}{}{SECTIONFTC-12}%
One question which eventually led to such a closer investigation was, ``Does every continuous function have an antiderivative.'' What do you think?%
\par
Decide whether you believe that every continuous function must necessarily have an antiderivative or not and explain your reasoning.%
\end{problem}
At this point we have the tools necessary for a rigorous proof of the Fundamental Theorem of Calculus. What we do not have is an adequate definition of the integral.    We'll need a definition that is independent of differentiation, but which recovers all of the properties of integration that you are already familiar with from your Calculus course, including the Fundamental Theorem of Calculus.%
\par
We'll provide such a definition in \hyperref[SECTIONDefiningIntegral]{Section~{\xreffont\ref{SECTIONDefiningIntegral}}} (the next section), but since we are already familiar with the properties we'll need we will proceed with the proof of the Fundamental Theorem of Calculus now.%
\par
The following formulation and proof of the Fundamental Theorem of Calculus is from Cauchy's 1823 publication \textit{Rsum des leons donns  l' cole royale polytechnique sur le calcul infinitesimal} (Summary of the lessons given at the Royal Polytechnic School on infinitesimal calculus).%
\begin{theorem}{Theorem}{The Fundamental Theorem of Calculus (Cauchy).}{}{THEOREMFTCCauchy}%
Suppose \(f\left(x\right)\) is continuous on \([a,b]\) and define%
\par
%
\begin{equation*}
I\left(x\right)=\int^x_{t=a}{f\left(t\right)\dx{t}}
\end{equation*}
for \(x\in [a,b]\).  Then \(I\) is continuous on \([a,b]\), differentiable on \((a,b)\) and%
\par
%
\begin{equation*}
I^\prime\left(x\right)=f(x)
\end{equation*}
%
\end{theorem}
Before we dive into the proof, notice that \hyperref[THEOREMFTCCauchy]{Theorem~{\xreffont\ref{THEOREMFTCCauchy}}} and \hyperref[EQUATIONFTCLeibniz]{equation~({\xreffont\ref{EQUATIONFTCLeibniz}})} are very closely related though they come at the question of integration from different viewpoints.%
\par
\hyperref[EQUATIONFTCLeibniz]{Equation~({\xreffont\ref{EQUATIONFTCLeibniz}})} starts with the assumption that \(\dfdx{Y}{x}=y \). It says that if we sum the differentials \(y\dx{x}\) from  \(x=a\) to \(x=b\) then the sum collapses to the difference of the extremes: \(Y(b)-Y(a)\).%
\par
On the other hand \hyperref[THEOREMFTCCauchy]{Theorem~{\xreffont\ref{THEOREMFTCCauchy}}} starts with the assumption that \(\int_{t=a}^x f(t)\dx{t}\) is well defined, uses this to define the function \(I(x)\) ( which is simply \(Y(x)\) by another name), and then concludes where Leibniz began \textemdash{} with the statement that \(I^\prime
(x)=f(x)\), (or \(\dfdx{Y}{x}=y\)). Our task in the next section will be to provide a definition that will support that conclusion.%
\par
In most Calculus texts \hyperref[EQUATIONFTCLeibniz]{equation~({\xreffont\ref{EQUATIONFTCLeibniz}})} is called a \terminology{definite integral}, and the function defined in \hyperref[THEOREMFTCCauchy]{Theorem~{\xreffont\ref{THEOREMFTCCauchy}}} is called an \terminology{indefinite integral}. The two of them are often referred to as parts 1 and 2 of the Fundamental Theorem of Calculus.%
\par
We will now proceed with the proof of Cauchy's version of the Fundamental Theorem of Calculus, with the caveat that the proof is not complete until we have defined the function \(I(x)=\int_{t=a}^x
f(t)\dx{t}\) and shown that, under our definition, it has the properties we expect from an integral. We will need some of these properties in the proof below.%
\begin{proof}{Proof}{Outline of Proof.}{SECTIONFTC-22}
Let \(x\in (a,b)\). To find \(I^\prime (x)\) we apply \hyperref[def_derivative]{Definition~{\xreffont\ref{def_derivative}}}. Thus%
\begin{align*}
I^\prime (x)\amp{} = \limit{h}{0}{\frac{I(x+h)-I(x)}{h}}\\
\amp{} =       \limit{h}{0}{\frac{\int_{t=a}^{x+h}f(t)\dx{t}-\int_{t=a}^x
f(t)\dx{t}}{h}}\\
\amp{}=\limit{h}{0}{\frac{\int_{t=x}^{x+h}f(t)\dx{t}}{h}}.
\end{align*}
        Since \(f(t)\) was assumed to be continuous on \([a,b]\) it is also continuous on \([x, x+h]\in [a,b]\). We know from the \hyperref[thm_EVT]{Extreme Value Theorem} that there are points \(c, C \in [x,x+h]\) such that \(f(c)\) and \(f(C)\) are the global minimum and maximum of \(f\) on \([x, x+h]\), respectively.%
\par
Thus if \(h\gt0\) we have%
\begin{equation*}
f\left(c\right)\cdot h\le
\int^{x+h}_{t=x}{f(t)\dx{t}}\le f(C)\cdot h
\end{equation*}
or%
\begin{equation*}
f\left(c\right)\le \frac{\int^{x+h}_{t=x}{f(t)\dx{t}}}{h}\le
f\left(C\right)\text{.}
\end{equation*}
%
\par
If \(h\lt 0\), we have \(-h>0\), and so%
\begin{equation*}
f\left(c\right)\cdot \left(-h\right)\le
\int^x_{t=x+h}{f(t)\dx{t}}\le f\left(C\right)\cdot (-h)
\end{equation*}
%
\begin{equation*}
f(c)\le \frac{\int^x_{t=x+h}{f(t)\dx{t}}}{-h}\le f(C)
\end{equation*}
In either case we have%
\begin{equation*}
f(c)\le \frac{\int^{x+h}_{t=x}{f(t)\dx{t}}}{h}\le f\left(C\right)
\end{equation*}
%
\par
Applying the squeeze theorem as \(h\to 0\) and continuity of \(f\) at \(x\) should do the trick.%
\par
The above outline shows that \(I(x)\) is differentiable on \((a,b)\), and it follows thata \(I(x)\) is also continuous on \([a,b]\) (why?).%
\par
To show that \(I(x)\) is  continuous  at the endpoints \(a\) and \(b\), we will appeal to \hyperref[thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{thm_LimDefOfContinuity}}}.%
\par
Consider any sequence \((x_n)\) contained in \([a,b]\) and converging to \(a\). We want to show that%
\begin{equation*}
\limit{n}{\infty }{\left(\int^{x_n}_{t=a}{f(t)\dx{t}}-\int^a_{t=a}{f(t)\dx{t}}\right)}
=\limit{n}{\infty }{
\left(\int^{x_n}_{t=a}{f(t)\dx{t}}\right)}=0 
\end{equation*}
%
\par
To get continuity at \(b\), consider any sequence \(\left(y_n\right)\) in \([a,b]\) converging to \(b\).  We want to show that%
\begin{equation*}
\limit{n}{ \infty }
{\left(\int^b_{t=a}{f(t)\dx{t}}-\int^{y_n}_{t=a}{f(t)\dx{t}}\right)
}=\limit{n}{\infty } {\left(\int^b_{t=y_n}{f(t)\dx{t}}\right)}=0
\end{equation*}
%
\end{proof}
We can use an argument similar to what we did before, but in this case, since we are not dividing, we can just consider the minimum \(m\) and maximum \(M\) of \(f\) on \([a,b]\) so that \(m\le f\left(t\right)\le M\).%
\begin{problem}{Problem}{}{SECTIONFTC-24}%
Turn the above ideas into a proof of \hyperref[THEOREMFTCCauchy]{Theorem~{\xreffont\ref{THEOREMFTCCauchy}}}. Don't forget to justify every step in the ``Outline of Proof'' above.%
\end{problem}
\begin{problem}{Problem}{}{SECTIONFTC-25}%
Suppose \(f(x)\) is continuous on \([a,b]\) and \(I(x)\) is the antiderivative of \(f(x)\) from  \hyperref[THEOREMFTCCauchy]{Theorem~{\xreffont\ref{THEOREMFTCCauchy}}}. Suppose further that \(F(x)\) is any other antiderivative of \(f(x)\) on the same interval. Prove that for any \(x\in [a,b]\),%
\begin{equation*}
I(x)=\int^x_{t=a}{f\left(t\right)\dx{t}}=F\left(x\right)-F(a)
\end{equation*}
In particular this means that%
\begin{equation*}
\int^b_{t=a}{f\left(t\right)\dx{t}}=F\left(b\right)-F(a)
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{SECTIONFTC-25-2}{}\quad{}You have two antiderivatives of \(f(x)\).  By part (c) of \hyperref[DRILLZeroDerivImpConst]{Problem~{\xreffont\ref{DRILLZeroDerivImpConst}}}, these must differ by a constant.  What must this constant be?%
\end{problem}
As we said earlier, how do we know that a continuous function on a closed interval has an antiderivative?  Equivalently, how do we know that%
\begin{equation*}
I\left(x\right)=\int^x_{t=a}{f(t)\dx{t}}
\end{equation*}
actually exists?%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 10.4 Defining the Integral}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Defining the Integral}{}{Defining the Integral}{}{}{SECTIONDefiningIntegral}
\begin{introduction}{}%
 In a letter to \href{https://mathshistory.st-andrews.ac.uk/Biographies/Eratosthenes/}{Eratosthenes}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Eratosthenes/}\label{SECTIONDefiningIntegral-2-1-2}} (circa 250 BC), \href{https://mathshistory.st-andrews.ac.uk/Biographies/Archimedes/}{Archimedes}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Archimedes/}\label{SECTIONDefiningIntegral-2-1-4}} described what he called a ``mechanical'' method for finding areas and volumes.  His method consisted of mentally dividing objects into infinitely thin slices and balancing these slices on an imaginary balance.  Archimedes noted that while his method was not rigorous it was still quite useful:%
\begin{quote}%
``. . . I thought it might be appropriate to write down for you a special method, by means of which you will be able to recognize certain mathematical questions with the aid of mechanics.  I am convinced that this is no less useful than finding the proofs of these same theorems.''%
\par
``Some things, which first became clear to me by the mechanical method, were afterwards proved geometrically, because their investigation by that method does not furnish an actual demonstration.  It is easier to supply the proof when we have previously acquired, by the method, some knowledge of the questions than it is to find it without any previous knowledge . . .''%
\end{quote}
Closer to our time \href{https://mathshistory.st-andrews.ac.uk/Biographies/Cavalieri/}{Cavalieri}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Cavalieri/}\label{SECTIONDefiningIntegral-2-3-2}}, \href{https://mathshistory.st-andrews.ac.uk/Biographies/Torricelli/}{Torricelli}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Torricelli/}\label{SECTIONDefiningIntegral-2-3-4}}, \href{https://mathshistory.st-andrews.ac.uk/Biographies/Kepler/}{Kepler}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Kepler/}\label{SECTIONDefiningIntegral-2-3-6}}, \href{https://mathshistory.st-andrews.ac.uk/Biographies/Galileo/}{Galileo}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Galileo/}\label{SECTIONDefiningIntegral-2-3-8}}, \href{https://mathshistory.st-andrews.ac.uk/Biographies/Roberval/}{Roberval}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Roberval/}\label{SECTIONDefiningIntegral-2-3-10}}, and others explored a similar idea. They too mentally  cut geometric areas (and volumes) into infinitely thin slices.  But rather than using an imaginary balance they compared them geometrically.%
\par
This division of objects into infinitely small pieces in order to analyze them is the essential idea underlying Calculus but as Archimedes observed it is questionable as a method of proof. And this is still a useful way to think.%
\par
It had always been recognized that infinitesimals were an inadequate foundation for Calculus     but it was at the beginning of the 19th century, in large part due to the work of Fourier, that it became imperative to replace it.%
\par
Fourier's work raises the question: How ``random'' can a function defined on an interval be and still be represented by a Fourier series?  Since the coefficients are computed via integration, a closely related question is how random can a function be and still be integrable?%
\par
Since this text is intended as a one semester introduction to real analysis we will not be able to fully answer that question here. But we will give one rigorous definition of the integral (there are many) and use it to show that a continuous function on a closed interval is integrable.  This will ``close the gap'' in our proof of the \hyperref[THEOREMFTCCauchy]{Fundamental Theorem of Calculus}.      %
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Cauchy's Integral Definition}
\typeout{************************************************}
%
\begin{subsectionptx}{Subsection}{Cauchy's Integral Definition}{}{Cauchy's Integral Definition}{}{}{SUBSECTIONCauchyRiemannInt}
One of the first mathematicians to provide a rigorous definition of a definite integral was \href{https://mathshistory.st-andrews.ac.uk/Biographies/Cauchy/}{Augustin Louis Cauchy}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Cauchy/}\label{SUBSECTIONCauchyRiemannInt-2-2}} in 1823. Cauchy used the limit idea to bridge the gap between finite sums of (finitely many) very small (but still finite) pieces and infinite sums of infinitesimals.%
\par
It was common practice to approximate an integral whose antiderivative was not readily computable by a finite sum as seen in \hyperref[FIGURECauchyIntFinitSum]{Figure~{\xreffont\ref{FIGURECauchyIntFinitSum}}}.%
\begin{figureptx}{Figure}{}{FIGURECauchyIntFinitSum}{}%
\begin{image}{0.275}{0.45}{0.275}{}%
\includegraphics[width=\linewidth]{external/images/Integration2.png}
\end{image}%
\tcblower
\end{figureptx}%
To approximate \(\int^b_{x=a}{f(x)\dx{x}}\), Cauchy started by partitioning \(P\) of the interval\([a,b]\) into a finite number of subintervals.  Basically, the partition \(P\) is a finite sequence of numbers%
\begin{equation*}
a=x_0\lt x_1\lt x_2\lt\dots \lt x_{n-1}\lt x_n=b \text{.}
\end{equation*}
In the figure \(n=5\). He then formed the sum%
\begin{align*}
f\left(x_0\right)\left(x_1-x_0\right)\amp{}+f\left(x_1\right)\left(x_2-x_1\right)+\dots\\
\amp{}\dots
+f\left(x_{n-1}\right)\left(x_n-x_{n-1}\right)=\sum^{n-1}_{k=0}{f(x_k)(x_{k+1}-x_k)}
\end{align*}
%
\par
If \(f\left(x\right)\ge 0\) as in \hyperref[FIGURECauchyIntFinitSum]{Figure~{\xreffont\ref{FIGURECauchyIntFinitSum}}} we see that we are approximating the area under the curve \(y=f(x)\) with the area of a finite sum of boxes whose bases are the subintervals \([x_k,x_{k+1}]\) and whose heights are obtained by evaluating \(f\) at some point in \([x_k,x_{k+1}]\). In our figure we used the left endpoint \(x_k\) for convenience.       Notice that the subintervals need not be the same length.%
\par
Diagrams like this are the source of the common misunderstanding that an integral computes area. In certain special cases it does, but not always. Yet it is often helpful to think of an integral that way.%
\par
We define the \terminology{norm} of the partition \(\norm{P}\) to be to be the length of the largest subinterval:%
\begin{equation*}
\norm{P}=\max_{k=0, 1, \dots    n-1}(x_{k+1}-x_k) \text{.}
\end{equation*}
Cauchy said that a function \(f(x)\) defined on \([a,b]\) was integrable if there was a number \(I\) such that for all \(\eps >0\), there is a \(\delta >0\) such that whenever the norm of the partition, \(\norm{P}\) is less than \(\delta{}\) the difference between \(I\) and the  associated sum will be less than \(\eps{}\). Symbolically this is%
\begin{equation*}
\norm{P}\lt \delta \imp \abs{\sum^{n-1}_{k=0}{f\left(x_k\right)\left(x_{k+1}-x_k\right)}-I}\lt
\eps \text{.}
\end{equation*}
Notice that \(P\) can be any partition as long as \(\norm{P}\lt\delta \).%
\par
In this case we write%
\begin{equation*}
I=\int^b_{x=a}{f(x)\dx{x}}
\end{equation*}
%
\par
Using this definition Cauchy was then able to show that any continuous function is (Cauchy) integrable and was able to prove the \hyperref[THEOREMFTCCauchy]{Fundamental Theorem of Calculus} as we indicated in the last section. More formally, Cauchy made the following definition.%
\begin{definition}{Definition}{The Riemann Integral.}{DEFINITIONRiemannIntegral}%
Given a function \(f(x)\), defined on the interval \([a,b]\) we say that \(f\) is integrable on \([a,b]\) if and only if, for every \(\eps\gt 0\), and  every partition%
\begin{equation*}
P=\left\{x_0, x_1, x_2, \cdots, x_n\right\} 
\end{equation*}
where \(x_0=a\), \(x_n=b\),1 and \(x_k\lt
x_{k+1}\) \(\forall\) \(k=0,\ldots,n-1\), \(\exists\) \(\delta \gt 0\) such that%
\begin{equation*}
\norm{P}\lt\delta \imp
\abs{\sum^{n-1}_{k=0}{f\left(x_k^*\right)\left(x_{k+1}-x_k\right)}-I}\lt
\eps 
\end{equation*}
where \(x_k\lt{}x_k^*\lt x_{k+1}\).%
\begin{aside}{Aside}{Historical Context.}{DEFINITIONRiemannIntegral-2-2}%
No doubt you are wondering why this is called the \terminology{Riemann Integral} when it was devised by Cauchy.%
\par
It often happens in mathematics that important concepts do not get named for the person who invents them. In this instance Cauchy showed that functions are integrable  under his definition, but left open the question, ``Is it necessary for a function to be continuous for it to be (Cauchy) integrable?''%
\par
The answer to that question is ``No!'' as Riemann showed in 1868. In fact Riemann was able to provide necessary and sufficient conditions for when a function to be integrable under Cauchy's definition that are far weaker than continuity.  We won't go into that as it does not serve our purposes here, but as a result of Riemann's work Cauchy's definition of the integral came to be called the \terminology{Riemann integral}.%
\par
In that case we say that%
\begin{equation*}
\int_a^b f(x)\dx{x} =I\text{.}
\end{equation*}
%
\end{aside}
\end{definition}
The similarity between \hyperref[DEFINITIONRiemannIntegral]{Definition~{\xreffont\ref{DEFINITIONRiemannIntegral}}}  and the definition of a limit is hard to miss so sometimes the Riemann integral is defined via the limit symbol as%
\begin{equation}
\int_a^bf(x)\dx{x} =
\limit{\norm{P}}{0}{\sum^{n-1}_{k=0}{f\left(x_k\right)\left(x_{k+1}-x_k\right)}}\label{EQUATIONCauchyIntAsLim}
\end{equation}
but in our (the authors') opinions this notatation serves to hide the important ideas rather than elucidate them because the limit in \hyperref[EQUATIONCauchyIntAsLim]{equation~({\xreffont\ref{EQUATIONCauchyIntAsLim}})} is very different from the ones we've encountered before.%
\par
In the past, when we had limits like \(\limitt{x}{2}{\frac{x^2-4}{x-2}}\) we only had to think about letting the single variable \(x\) get ``close to'' \(2\). But the limit in \hyperref[EQUATIONCauchyIntAsLim]{equation~({\xreffont\ref{EQUATIONCauchyIntAsLim}})} asks us to think about all possible partitions with the property that \(\norm{P}\lt\delta{}\) and what is happening when \(\norm{P} \rightarrow 0\) at the same time. It can be done, but we'd prefer something a little easier to think about. If we can't get that we'd like to have more than one way to think about integration.%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Darboux's Integral Definition}
\typeout{************************************************}
%
\begin{subsectionptx}{Subsection}{Darboux's Integral Definition}{}{Darboux's Integral Definition}{}{}{SUBSECTIONDarbouxIntegral}
Notice that neither this definition nor the definition of the \hyperref[def_derivative]{derivative~{\xreffont\ref{def_derivative}}} tells us how to compute the quantity in question. In the 19th century how to compute both integrals and derivatives was as well known as it is today.  These definitions are about providing a rigorous foundation for these ideas, not computing them.%
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Darboux/}{Jean Gaston Darboux}\protect\footnotemark{}}{FIGUREDarbouxPortrait}{}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Darboux.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[31]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Darboux/}\label{FIGUREDarbouxPortrait-1-2}}%
In 1875 Jean Gaston Darboux developed a different (yet equivalent) definition of the Riemann integral which uses the least upper and greatest lower bounds we learned about in \hyperref[IVTandEVT]{Chapter~{\xreffont\ref{IVTandEVT}}}.  There are discontinuous functions which are Darboux (and Riemann) integrable, but to keep things simple we will restrict our attention to continuous functions.%
\par
As before, we will start with a partition \(P=\{x_0, x_1,x_2,
\dots , x_n\}\) of the interval \([a,b]\) where \(a=x_0\lt
x_1\lt \dots \lt x_{n-1}\lt x_n=b\).  Let \(m_k\) and \(M_k\) denote the minimum and maximum of \(f(x)\) on \([x_k,x_{k+1}]\), respectively.  Define the lower (Darboux) sum \(L(P)\) by%
\begin{equation*}
L\left(P\right)=\sum^{n-1}_{k=0}{m_k\left(x_{k+1}-x_k\right)}
\end{equation*}
and upper (Darboux) sum \(U(P)\) by%
\begin{equation*}
U\left(P\right)=\sum^{n-1}_{k=0}{M_k\left(x_{k+1}-x_k\right)}\text{.}
\end{equation*}
%
\par
Notice that \alert{if} we knew  the  value of \(\int^b_{x=a}{f(x)\dx{x}}\), then it would be intuitively clear that%
\begin{equation*}
L\left(P\right)\le \int^b_{x=a}{f(x)\dx{x}}\le
U(P)
\end{equation*}
It would also be intuitively clear that as the number of intervals gets larger, then these bounds should get closer to the actual integral (if it exists). If you don't see this try drawing a few representaive examples.%
\begin{definition}{Definition}{Partition Refinement.}{DEFINITIONPartitionRefine}%
Given two partitions%
\begin{equation*}
P^\prime=\{x^\prime_0, x^\prime_1,
x^\prime_2, \dots , x^\prime_{m-1},x^\prime_m\} 
\end{equation*}
and%
\begin{equation*}
P=\{x_0, x_1, x_2, \dots , x_{m-1},x_m\} 
\end{equation*}
\(P^\prime \) is said to be  a \terminology{refinement} of \(P\) if every  point in  \(P^\prime\) is also a point in \(P\).%
\end{definition}
\begin{problem}{Problem}{}{SUBSECTIONDarbouxIntegral-8}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Show that \hyperref[DEFINITIONPartitionRefine]{Definition~{\xreffont\ref{DEFINITIONPartitionRefine}}} implies   that every subinterval \([x^\prime_j,x^\prime_{j+1}]\) in the partition \(P^\prime\) is contained inside some subinterval \([x_k,
x_{k+1}]\) of the partition \(P\).%
\item{}Using this observation above, show that if \(P^\prime\) is a refinement of \(P\), then%
\begin{equation*}
L\left(P\right)\le L\left(P^\prime\right)\le
U\left(P^\prime\right)\le U(P)
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{SUBSECTIONDarbouxIntegral-8-2-2}{}\quad{}First, show that this is true if \(P^\prime\) is obtained by adding one point to \(P\).%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{PROBLEMPartitionIneq1}%
Let \(P \)and \(\Pi \) be any two partitions of \([a,b]\).  Show that%
\begin{equation*}
L\left(P\right)\le
U(\Pi{})
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PROBLEMPartitionIneq1-2}{}\quad{}Consider that \(Q=P\cup \Pi \) is a refinement of both \(P\) and \(\Pi \), and use the previous result.%
\end{problem}
Next observe  that the set of lower sums over all partitions of \([a,b]\) is a non-empty set of real numbers which is bounded above.  Therefore by \hyperref[thm_LUB]{Theorem~{\xreffont\ref{thm_LUB}}}  it has a least upper bound.  We define the lower (Darboux) integral by%
\begin{equation*}
\underline{\int^b_{x=a}}{f(x)}\dx{x}=\sup_{P}
\left(L(P)\right)
\end{equation*}
%
\par
Similarly, the set of all upper sums is a non-empty set of real numbers which is bounded below and therefore has a greatest lower bound. We define the upper (Darboux) integral as the greatest lower bound of this set%
\begin{equation*}
\overline{\int^{b}_{x=a}}{f(x)\dx{x}}=\inf_{P} \left(U(P)\right)
\end{equation*}
%
\begin{problem}{Problem}{}{SUBSECTIONDarbouxIntegral-12}%
Show that%
\begin{equation*}
\underline{\int^b_{x=a}}{f(x)}\dx{x}\le
\overline{\int^{b}_{x=a}}{f(x)\dx{x}}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{SUBSECTIONDarbouxIntegral-12-2}{}\quad{}Let \(P^\prime\) be a fixed partition of \([a,b]\). By \hyperref[PROBLEMPartitionIneq1]{Problem~{\xreffont\ref{PROBLEMPartitionIneq1}}}, \(U(P^\prime)\) is an upper bound for the set of all lower sums.  This says%
\begin{equation*}
\int^b_{\underline{  x=a
}}{f(x)}\dx{x}\le U(P^\prime)
\end{equation*}
%
\par
However, \(P^\prime\) is an arbitrary partition of \([a,b]\).]%
\end{problem}
\begin{definition}{Definition}{Darboux Integrability.}{DEFINITIONDarbouxIntegral}%
A function is said to be (Darboux) integrable provided%
\begin{equation*}
\underline{\int^b_{x=a}}{f(x)}\dx{x}=\overline{\int^{b}_{x=a}}{f(x)\dx{x}}
\end{equation*}
%
\par
In this case we define the (Darboux) integral by%
\begin{equation*}
\int^b_{x=a}{f(x)\dx{x}}=\underline{\int^b_{x=a}}{f(x)}\dx{x}=\overline{\int^{b}_{x=a}}{f(x)\dx{x}}
\end{equation*}
%
\end{definition}
\begin{problem}{Problem}{}{SUBSECTIONDarbouxIntegral-14}%
In 1837 \href{https://mathshistory.st-andrews.ac.uk/Biographies/Dirichlet/}{Lejeune Dirichlet}\footnotemark{} defined the following non\textendash{}integrable function.%
\par
Let%
\begin{equation*}
f(x)=
\begin{cases}
0\amp \text{ if } x \text{ is rational}\\
1\amp \text{ if } x \text{ is irrational}
\end{cases}
\end{equation*}
%
\par
Use \hyperref[DEFINITIONDarbouxIntegral]{Definition~{\xreffont\ref{DEFINITIONDarbouxIntegral}}} to     show that%
\begin{equation*}
\underline{\int^1_{x=0}}{f(x)}\dx{x}\lt
\overline{\int^{1}_{x=0}}{f(x)\dx{x}}
\end{equation*}
so that \(f\left(x\right)\) is not integrable on \([0,1]\).%
\end{problem}
\footnotetext[32]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Dirichlet/}\label{SUBSECTIONDarbouxIntegral-14-1-2}}%
As we mentioned earlier, Darboux's definition of the integral is equivalent to the Riemann integral. This is similar to having both an \hyperref[def_continuity]{analytic definition of continuity} and a \hyperref[thm_LimDefOfContinuity]{sequence\textendash{}based definition of continuity}. We can use whichever definition works better for the problem at hand.  It is easier to prove the typical theorems associated with definite integrals using Riemann's formulation, but it is a little simpler to use Darboux's formulation to show that a continuous function is integrable as we will see next.%
\begin{theorem}{Theorem}{}{}{THEROEMContImpInt}%
If \(f\left(x\right)\) is continuous on \([a,b]\), then%
\begin{equation*}
\underline{\int^b_{x=a}}{f(x)}\dx{x}=\overline{\int^{b}_{x=a}}{f(x)\dx{x}}
\end{equation*}
so \(f(x)\) is integrable.%
\end{theorem}
\begin{proof}{Proof}{Sketch of Alleged Proof.}{SUBSECTIONDarbouxIntegral-17}
Note that we're calling this an ``alleged'' proof. That means that it contains a flaw somewhere. As you read it see if you can find where it goes wrong.%
\par
We already know that%
\begin{equation*}
\underline{\int^b_{x=a}}{f(x)}\dx{x}\le
\overline{\int^{b}_{x=a}}{f(x)\dx{x}}
\end{equation*}
%
\par
If we can also show that%
\begin{equation*}
\overline{\int^{b}_{x=a}}{f(x)\dx{x}}\le
\underline{\int^b_{x=a}}{f(x)}\dx{x}
\end{equation*}
then the conclusion follows immediately.%
\par
Let \(\eps \gt 0\) be given.  Since \(f\left(x\right)\) is continuous at each point in \([a,b]\), we can choose a \(\delta >0\) such that if%
\begin{equation}
\left|x-y\right|\lt \delta \text{,}\label{EQUATIONUniContFlaw1}
\end{equation}
then%
\begin{equation}
\left|f\left(x\right)-f\left(y\right)\right|\lt
\frac{\eps }{b-a} \text{.}\label{EQUATIONUniContFlaw2}
\end{equation}
%
\par
Since \(f\) is continuous on \([a,b]\) it is continuous on each subinterval.  Next define \(m_k\) and \(M_k\) to be the respective minimum and maximum of \(f(x)\) on the subinterval \([x_k, x_{k+1}]\).  If we choose a partition%
\begin{equation*}
P_0=\{x_0, x_1, x_2,\dots , x_{n-1}, x_n\}
\end{equation*}
such that \(\norm{P_0}\lt
\delta \), then on each subinterval \([x_k,
x_{k+1}]\), we have%
\begin{equation*}
M_k-m_k\lt \frac{\eps }{b-a}\text{.}
\end{equation*}
%
\par
Thus%
\begin{align*}
\overline{\int^{b}_{x=a}}{f(x)\dx{x}}-\underline{\int^b_{x=a}}{f(x)}\dx{x}\amp=\inf_{P}
\left(U(P)\right) -\sup_{P} \left(L(P)\right)\\
\amp{}     \le U\left(P_0\right)-L(P_0)\\
\amp{}      =\sum^{n-1}_{k=0}{M_k\left(x_{k+1}-x_k\right)}-\sum^{n-1}_{k=0}{m_k\left(x_{k+1}-x_k\right)}\\
\amp{}      =\sum^{n-1}_{k=0}{\left(M_k-m_k\right)\left(x_{k+1}-x_k\right)}\\
\amp{}        \lt \sum^{n-1}_{k=0}{\frac{\eps
}{b-a}\left(x_{k+1}-x_k\right)}\\
\amp{}        =\frac{\eps
}{b-a}\sum^{n-1}_{k=0}{\left(x_{k+1}-x_k\right)}\\
\amp{}=\frac{\eps }{b-a}\left(b-a\right)\\
\amp{}=\eps
\end{align*}
%
\par
\alert{QED?}%
\end{proof}
Did you find the flaw in the proof? If not, read it carefully once more before reading on.%
\par
Recall that continuity is \hyperref[def_continuity]{defined at a point}, not on an interval. That means that that for each \(x\), there is a \({\delta }_x>0\) which guaratees that \hyperref[EQUATIONUniContFlaw2]{equation~({\xreffont\ref{EQUATIONUniContFlaw2}})} will be true. It does \emph{not} mean that the same \(\delta_x\) will work for all values of \(x\) in \([a,b]\).%
\par
But our alleged proof needs a single \(\delta >0\) which works uniformly for all such \(x, y\). Since this is not guaranteed  \hyperref[EQUATIONUniContFlaw1]{equation~({\xreffont\ref{EQUATIONUniContFlaw1}})} is not necessarily true and our proof fails.%
\par
This leads to the following definition.%
\begin{definition}{Definition}{Uniform Continuity.}{DEFINITIONUnifCont}%
Suppose \(S\subset \RR\).  We say that \(f(x)\) is uniformly continuous on \(S\) provided that for all \(\eps
>0\), there is a \(\delta >0\) such that \(\left|f\left(x\right)-f\left(y\right)\right|\lt \eps \) for all \(x, y\in S\) with \(\left|x-y\right|\lt \delta \).%
\end{definition}
Uniform continuity is a stronger property than continuity in the sense that a function which is uniformly continuous is necessarily continuous (see \hyperref[LEMMAUnifContImpCont]{Lemma~{\xreffont\ref{LEMMAUnifContImpCont}}} below) but the converse is not necessarily true, as the next problem shows.%
\begin{problem}{Problem}{}{DRILLContVSUnifCont}%
Consider \(f(x)=x^2\) on \([0,\infty )\).  Show that for any \(\delta >0\),%
\begin{equation*}
\left(x+\frac{\delta }{2}\right)^2-x^2>1
\end{equation*}
whenever%
\begin{equation*}
x>\frac{1-\frac{{\delta }^2}{4}}{\delta }\text{.}
\end{equation*}
Explain why this says that \(f\left(x\right)=x^2\) is not uniformly continuous on \([0,\infty )\).%
\end{problem}
Since we are trying to show that a continuous function (rather than a uniformly continuous function) is (Darboux) integrable it is not clear how this definition will help. Fortunately for us,  the following lemma is true.%
\begin{lemma}{Lemma}{}{}{LEMMAUnifContImpCont}%
If \(f(x)\) is continuous on the closed, bounded interval \([a,b]\), then \(f(x)\) is uniformly continuous on \([a,b]\).%
\end{lemma}
\begin{proof}{Proof}{Outline of Proof.}{SUBSECTIONDarbouxIntegral-27}
We will do a proof by contradiction.  Suppose \(f(x)\) is not uniformly continuous on \([a,b]\).  Then there is an \(\eps
>0\) such that for any \(\delta >0\), there are \(x,y\) with \(\left|x-y\right|\lt \delta \) , but \(\left|f\left(x\right)-f\left(y\right)\right|\ge \eps \).  If we let \(\delta =\frac{1}{n}, n\in \mathbb{N}\), then we can create two sequences \(\left(x_n\right), (y_n)\) with \(\left|x_n-y_n\right|\lt \frac{1}{n}\), but \(\left|f\left(x_n\right)-f\left(y_n\right)\right|\ge \eps .\) By the Bolzano\textendash{}Weierstrass Theorem, there is a \(c\in [a,b]\) and a subsequence \((x_{n_k})\) with \(\limit{k}{\infty }{ x_{n_k} }=c\).  Given how \((y_n)\) was constructed, \(\limit{k}{\infty }{ y_{n_k} }=c\).  Since \(f(x)\) is continuous at \(c\), you should be able to get a contradiction out of this.%
\end{proof}
\begin{problem}{Problem}{}{SUBSECTIONDarbouxIntegral-28}%
Turn the above outline into a proof of \hyperref[LEMMAUnifContImpCont]{Lemma~{\xreffont\ref{LEMMAUnifContImpCont}}}.%
\end{problem}
\begin{problem}{Problem}{}{SUBSECTIONDarbouxIntegral-29}%
Prove \hyperref[THEROEMContImpInt]{Theorem~{\xreffont\ref{THEROEMContImpInt}}}.%
\end{problem}
We restricted ourselves to proving that continuous functions are integrable so that in our proof we could ensure that on each subinterval the function in question had a minimum \(m_k\) and a maximum \(M_k\).  By tweaking \hyperref[THEROEMContImpInt]{Theorem~{\xreffont\ref{THEROEMContImpInt}}} a bit it is possible to extend integrability to bounded functions. This would require using the infimum and supremum of \(f(x)\) on each subinterval rather than the minimum and maximum, but this is enough for our purposes here.%
\par
The evolution of the modern definition of a function is parallel to, and entwined with the definition of the definite integral.  Issues of integrability were not prevalent in 18th century because for most of that time the words ``integral'' and ``antiderivative'' were synonymous. Thus the only ``integrable'' functions were the ones that were derivatives of some other function. But in the 19th century, and especially after Fourier, we needed to integrate functions that were not clearly derivatives of something else. As a result the need for a precise definition of function became more and more pressing as the years went by.  For example, here is the definition of a function from Euler's \pubtitle{\textit{Introductio in Analysin Infinitorum}} (1748).%
\begin{quote}%
``A function of a variable quantity is an analytic expression composed in any way whatsoever of the variable quantity and numbers or constant quantities.''%
\end{quote}
This is the ``function as input\slash{}output machine'' that you've probably used all of your life so far: A number goes in, gears turn, and a new number is generatied as output. The advent of  Fourier series ushered in a need for a much more general definition. Here is Fourier's definition from his \pubtitle{\textit{Thorie analytique de la Chaleur}} (1822).%
\begin{quote}%
``In general, the function \(f(x)\) represents a succession of values or ordinates each of which is arbitrary. An infinity of values being given of the abscissa \(x\), there are an equal number of ordinates\(f(x)\). All have actual numerical values, either positive or negative or nul. We do not suppose these ordinates to be subject to a common law; they succeed each other in any manner whatever, and each of them is given as it were a single quantity.''%
\end{quote}
This is closer to our modern approach that for each \(x\) in the domain, there is a unique \(f(x)\) assigned to it. It is different from Euler's definition in that no formula (the internal gears) is needed. A function can be defined by simply giving a list of ordered pairs: (input, output).%
\par
As you can see, making the idea of an integral rigorous was a delicate matter.  Perhaps this is why it took so long to develop historically.%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  The Stieltjes Integral}
\typeout{************************************************}
%
\begin{subsectionptx}{Subsection}{The Stieltjes Integral}{}{The Stieltjes Integral}{}{}{SUBSECTIONStieltjesIntegral}
\begin{figureptx}{Figure}{Thomas Jan Stieltjes}{FIGUREStieltjesPortrait}{}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Stieltjes.png}
\end{image}%
\tcblower
\end{figureptx}%
The various integral definitions we've seen so far have all been equivalent in the sense that the value of the integral is the same regardless of which definition is used. This is not true of the Stieltjes integral.%
\end{subsectionptx}
\end{sectionptx}
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 11 Back to Power Series}
\typeout{************************************************}
%
\begin{chapterptx}{Chapter}{Back to Power Series}{}{Back to Power Series}{}{}{PowerSeriesRedux}
\renewcommand*{\chaptername}{Chapter}
%
%
\typeout{************************************************}
\typeout{Section 11.1 Uniform Convergence}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Uniform Convergence}{}{Uniform Convergence}{}{}{PowerSeriesRedux-UnifConv}
We have developed precise analytic definitions of the convergence of a sequence and continuity of a function and we have used these to prove the \hyperref[thm_EVT]{EVT} and \hyperref[IntermediateValueTheorem]{IVT} for a continuous function.  We will now draw our attention back to the question that originally motivated these definitions, ``Why are Taylor series well behaved, but Fourier series are not necessarily?'' More precisely, we mentioned that whenever a power series converges then whatever it converged to was continuous. Moreover, if we differentiate or integrate these series term by term then the resulting series will converge to the derivative or integral of the original series.  This was not always the case for Fourier series.  For example consider the function%
\begin{align*}
f(x) \amp = \frac{4}{\pi}\left(\sum_{k=0}^\infty\frac{(-1)^k}{2k+1}\cos\left((2k+1)\pi x\right)\right)\\
\amp = \frac{4}{\pi}\left(\cos(\pi x)-\frac13\cos(3\pi x)+\frac15 (5\pi x)-\ldots\right)\text{.}
\end{align*}
%
\par
We have seen that the graph of \(f\) is given by%
\begin{image}{0.075}{0.85}{0.075}{}%
\includegraphics[width=\linewidth]{external/images/Ch7fig1.png}
\end{image}%
If we consider the following sequence of functions%
\begin{align*}
f_1(x)=\amp \frac{4}{\pi}\cos\left(\pi x\right)\\
f_2(x)=\amp \frac{4}{\pi}\left(\cos \left(\pi x\right)-\frac{1}{3}\cos\left( 3\pi x\right)\right)\\
f_3(x)=\amp \frac{4}{\pi}\left(\cos\left(\pi x\right)-\frac{1}{3}\cos\left(3\pi x\right)+\frac{1}{5}\cos\left(5\pi x\right)\right)\\
\amp \vdots
\end{align*}
we see the sequence of continuous functions \(\left(f_n\right)\) converges to the non-continuous function \(f\) for each real number \(x\).  This didn't happen with Taylor series.  The partial sums for a Taylor series were polynomials and hence continuous but what they converged to was continuous as well.%
\par
The difficulty is quite delicate and it took mathematicians a while to determine the problem.  There are two very subtly different ways that a sequence of functions can converge: pointwise or uniformly.  This distinction was touched upon by Niels Henrik Abel (1802-1829) in 1826 while studying the domain of convergence of a power series.  However, the necessary formal definitions were not made explicit until Weierstrass did it in his 1841 paper \pubtitle{Zur Theorie der Potenzreihen (On the Theory of Power Series)}. This was published in his collected works in 1894.%
\par
It will be instructive to take a look at an argument that doesn't quite work before looking at the formal definitions we will need.  In 1821 Augustin Cauchy ``proved'' that the infinite sum of continuous functions is continuous.  Of course, it is obvious (to us) that this is not true because we've seen several counterexamples.  But Cauchy, who was a first rate mathematician was so sure of the correctness of his argument that he included it in his textbook on analysis, \textit{Cours d'analyse} (1821).%
\begin{problem}{Problem}{}{prob_Cauchy_s_incorrect_proof}%
\index{Cauchy, Augustin!Cauchy's flawed proof that the limit of continuous functions is continuous}\index{continuity!Cauchy's flawed proof that the limit of continuous functions is continuous} Find the flaw in the following ``proof'' that \(f\) is also continuous at \(a\).%
\par
Suppose \(f_1, f_2, f_3, f_4 \ldots\) are all continuous at \(a\) and that \(\sum_{n=1}^\infty f_n=f\).  Let \(\eps>0\).  Since \(f_n\) is continuous at \(a,\) we can choose \(\delta_n>0\) such that if \(\abs{x-a}\lt
\delta_n,\) then \(\abs{f_n(x)-f_n(a)}\lt
\frac{\eps}{2^n}\).  Let \(\delta=\inf(\delta_1,\delta_2,\delta_3,\ldots)\).  If \(\abs{x-a}\lt \delta\) then%
\begin{align*}
\abs{f(x)-f(a)} \amp =  \abs{\sum_{n=1}^\infty f_n(x)  -  \sum_{n=1}^\infty f_n(a) }\\
\amp = \abs{\sum_{n=1}^\infty \left(f_n(x)-f_n(a)\right) }\\
\amp \le \sum_{n=1}^\infty \abs{f_n(x)-f_n(a) }\\
\amp \le \sum_{n=1}^\infty \frac{\eps}{2^n}\\
\amp \le \eps\sum_{n=1}^\infty \frac{1}{2^n}\\
\amp =   \eps.
\end{align*}
%
\par
Thus \(f\) is continuous at \(a\).%
\end{problem}
\begin{definition}{Definition}{}{def_PointwiseConvergence}%
\index{convergence!of a series!pointwise} Let \(S\) be a subset of the real number system and let \(\left(f_n\right)=\left(f_1,f_2,f_3,\,\ldots\right)\) be a sequence of functions defined on \(S\).  Let \(f\) be a function defined on \(S\) as well.  We say that \(\left(f_n\right)\) \alert{converges to \(f\) pointwise on \(S\)} provided that for all \(x\in
S,\) the sequence of real numbers \(\left(f_n(x)\right)\) converges to the number \(f(x)\).  In this case we write\(\,f_n\ptwise f\) on \(S\).%
\end{definition}
Symbolically, we have \(f_n\ptwise f\text{ on }
S\Leftrightarrow \forall\,x\in S,\forall\ \eps>0,\,\exists\
N\) such that \(\left(n>N \Rightarrow|f_n(x)-f(x)|\lt
\eps\right)\).%
\par
This is the type of convergence we have been observing to this point.  By contrast we have the following new definition.%
\begin{definition}{Definition}{}{def_UniformConvergence}%
\index{uniform convergence} Let \(S\) be a subset of the real number system and let \(\left(f_n\right)=\left(f_1,f_2,f_3,\,\ldots\right)\) be a sequence of functions defined on \(S\).  Let \(f\) be a function defined on \(S\) as well.  We say that \(\left(f_n\right)\) \alert{converges to \(f\) uniformly on \(S\)} provided \(\forall\
\eps>0,\,\exists\ N\) such that \(n>N\Rightarrow|f_n(x)-f(x)|\lt \eps\text{ , } \forall\
x\in S\).%
\par
In this case we write \(f_n\unif f\) on \(S\).%
\end{definition}
The difference between these two definitions is subtle.  In pointwise convergence, we are given a fixed \(x\in S\) and an \(\eps>0\).  Then the task is to find an \(N\) that works for that particular \(x\) and \(\eps\).  In uniform convergence, one is given \(\eps>0\) and must find a single \(N\) that works for that particular \(\eps\) but also simultaneously (uniformly) for all \(x\in S\).  Clearly uniform convergence implies pointwise convergence as an \(N\) which works uniformly for all \(x,\) works for each individual \(x\) also.  However the reverse is not true. This will become evident, but first consider the following example.%
\begin{problem}{Problem}{}{prob_uniform_convergence}%
Let \(0\lt b\lt 1\) and consider the sequence of functions \(\left(f_n\right)\) defined on \([0,b]\) by \(f_n(x)=x^n\).  Use the definition to show that \(f_n\unif 0\) on \([0,b]\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{prob_uniform_convergence-4}{}\quad{}\(|x^n-0|=x^n\leq b^n\).%
\end{problem}
Uniform convergence is not only dependent on the sequence of functions but also on the set \(S\).  For example, the sequence \(\left(f_n(x)\right)=\left(x^n\right)_{n=0}^\infty\) of \hyperref[prob_uniform_convergence]{Problem~{\xreffont\ref{prob_uniform_convergence}}} does not converge uniformly on \([0,1]\).  We could use the negation of the definition to prove this, but instead, it will be a consequence of the following theorem.%
\begin{theorem}{Theorem}{}{}{thm_UnifConv-_Continuity}%
\index{uniform convergence!continuous functions and}\index{continuous functions!uniform convergence and}\index{continuous functions!uniform limit of continuous functions is continuous} Consider a sequence of functions \(\left(f_n\right)\) which are all continuous on an interval \(I\).  Suppose \(f_n\unif f\) on \(I\).  Then \(f\) must be continuous on \(I\).%
\end{theorem}
\begin{proof}{Proof}{Sketch of Proof.}{PowerSeriesRedux-UnifConv-17}
Let \(a\in I\) and let \(\eps>0\).  The idea is to use uniform convergence to replace \(f\) with one of the known continuous functions \(f_n\).  Specifically, by uncancelling, we can write%
\begin{align*}
\left|f(x)-f(a)\right|\amp =\left|f(x)-f_n(x)+f_n(x)-f_n(a)+f_n(a)-f(a)\right|\\
\amp \leq \left|f(x)-f_n(x)\right|+\left|f_n(x)-f_n(a)\right|+\left|f_n(a)-f(a)\right|
\end{align*}
%
\par
If we choose \(n\) large enough, then we can make the first and last terms as small as we wish, noting that the uniform convergence makes the first term uniformly small for all \(x\).  Once we have a specific \(n,\) then we can use the continuity of \(f_n\) to find a \(\delta>0\) such that the middle term is small whenever \(x\) is within \(\delta\) of \(a\).%
\end{proof}
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-18}%
\index{uniform convergence!continuous functions and} Provide a formal proof of \hyperref[thm_UnifConv-_Continuity]{Theorem~{\xreffont\ref{thm_UnifConv-_Continuity}}} based on the above ideas.%
\end{problem}
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-19}%
\index{\(x^n\)!converges pointwise on \([0,1]\)}\index{pointwise convergence}\index{\(x^n\) converges pointwise on \([0,1]\)} Consider the sequence of functions \(\left(f_n\right)\) defined on \([0,1]\) by \(f_n(x)=x^n\).  Show that the sequence converges to the function%
\begin{equation*}
f(x)= \begin{cases}0\amp \text{ if  } x\in[0,1)\\ 1\amp \text{ if } x=1 \end{cases}
\end{equation*}
pointwise on \(\,[0,1],\) but not uniformly on \([0,1]\).%
\end{problem}
Notice that for the Fourier series at the beginning of this chapter,%
\begin{equation*}
f(x)=\frac{4}{\pi}\left(\cos\left(\frac{\pi}{2}x\right)-\frac{1}{3}\cos\left( 3\pi x\right)+\frac{1}{5}\cos\left(5\pi x\right)-\frac{1}{7}\cos\left(7\pi x\right)+\cdots\right)
\end{equation*}
the convergence cannot be uniform on \((-\infty,\infty),\) as the function \(f\) is not continuous.  This never happens with a power series, since they converge to continuous functions whenever they converge.  We will also see that uniform convergence is what allows us to integrate and differentiate a power series term by term.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 11.2 Uniform Convergence: Integrals and Derivatives}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Uniform Convergence: Integrals and Derivatives}{}{Uniform Convergence: Integrals and Derivatives}{}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs}
\begin{introduction}{}%
We saw in the previous section that if \(\left(f_n\right)\) is a sequence of continuous functions which converges uniformly to \(f\) on an interval, then \(f\) must be continuous on the interval as well.  This was not necessarily true if the convergence was only pointwise, as we saw a sequence of continuous functions defined on \((-\infty,\infty)\) converging pointwise to a Fourier series that was not continuous on the real line.  Uniform convergence guarantees some other nice properties as well.%
\begin{theorem}{Theorem}{}{}{th_UniformIntegralConvergence}%
\index{uniform convergence!integration and} Suppose \(f_n\) and \(f\) are integrable and \(f_n\unif f\) on \([a,b]\).  Then%
\begin{equation*}
\lim_{n\rightarrow\infty}\int_{x=a}^b f_n(x)\dx{ x}=\int_{x=a}^bf(x)\dx{x}\text{.}
\end{equation*}
%
\end{theorem}
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-2-3}%
\index{uniform convergence!integration and} Prove \hyperref[th_UniformIntegralConvergence]{Theorem~{\xreffont\ref{th_UniformIntegralConvergence}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesRedux-UnifConv-IntsAndDerivs-2-3-2}{}\quad{}For \(\eps>0,\) we need to make \(|f_n(x)-f(x)|\lt
\frac{\eps}{b-a},\) for all \(x\in[a,b]\).%
\end{problem}
Notice that this theorem is not true if the convergence is only pointwise, as illustrated by the following.%
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-2-5}%
\index{convergence!pointwise convergence}\index{convergence!uniform convergence}\index{convergence!pointwise vs. uniform convergence} Consider the sequence of functions \(\left(f_n\right)\) given by%
\begin{equation*}
f_n(x)= \begin{cases}n\amp \text{ if } x\in\left(0,\frac{1}{n}\right)\\ 0\amp \text{ otherwise } \end{cases} \text{.}
\end{equation*}
%
\begin{enumerate}[label={(\alph*)}]
\item{}Show that \(f_n\ptwise 0\) on \([0,1],\) but \(\limit{n}{\infty}{\int_{x=0}^1f_n(x)\dx{
x}\neq\int_{x=0}^10\dx{ x}.}\)%
\item{}Can the convergence be uniform?  Explain.%
\end{enumerate}
%
\end{problem}
Applying this result to power series we have the following.%
\begin{aside}{Aside}{Comment.}{PowerSeriesRedux-UnifConv-IntsAndDerivs-2-7}%
Notice that we must explicitly assume uniform convergence. This is because we have not \emph{yet} proved that power series actually do converge uniformly.%
\end{aside}
\begin{corollary}{Corollary}{}{}{cor_IntConvUni}%
If \(\sum_{n=0}^\infty a_nx^n\) converges uniformly to \(f\) on an interval containing \(0\) and \(x\) then \(\int_{t=0}^xf(t)\dx{
t}=\sum_{n=0}^\infty\left(\frac{a_n}{n+1}x^{n+1}\right)\).%
\end{corollary}
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-2-9}%
\index{power series!term by term integral of} Prove \hyperref[cor_IntConvUni]{Corollary~{\xreffont\ref{cor_IntConvUni}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesRedux-UnifConv-IntsAndDerivs-2-9-2}{}\quad{}Remember that%
\begin{equation*}
\displaystyle \sum_{n=0}^\infty f_n(x) =
\lim_{N\rightarrow\infty}\sum_{n=0}^N f_n(x).
\end{equation*}
%
\end{problem}
Surprisingly, the issue of term-by-term differentiation depends not on the uniform convergence of \(\left(f_n\right),\) but on the uniform convergence of \(\left(f^\prime_n\right)\).  More precisely, we have the following result.%
\begin{theorem}{Theorem}{}{}{thm_UniformDerivativeConvergence}%
\index{pointwise convergence!derivative and}%
\index{differentiation!of the pointwise limit of functions}%
Suppose for every \(n\in\NN\) \(f_n\) is differentiable, \(f_n^\prime\) is continuous, \(f_n\ptwise f,\) and \(f_n^\prime\unif g\) on an interval, \(I\).  Then \(f\) is differentiable and \(f^\prime = g\) on \(I\).%
\end{theorem}
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-2-12}%
Prove \hyperref[thm_UniformDerivativeConvergence]{Theorem~{\xreffont\ref{thm_UniformDerivativeConvergence}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesRedux-UnifConv-IntsAndDerivs-2-12-4}{}\quad{}Let \(a\) be an arbitrary fixed point in \(I\) and let \(x\in I\).  By the Fundamental Theorem of Calculus, we have%
\begin{equation*}
\int_{t=a}^x f^\prime_n(t)\dx{t}=f_n(x)-f_n(a) \text{.}
\end{equation*}
Take the limit of both sides and differentiate with respect to \(x\).%
\end{problem}
As before, applying this to power series gives the following result.%
\begin{corollary}{Corollary}{}{}{cor_UniformConvergenceDerivative}%
If \(\sum_{n=0}^\infty a_nx^n\) converges pointwise to \(f\) on an interval containing \(0\) and \(x\) and \(\sum_{n=1}^\infty a_nnx^{n-1}\) converges uniformly on an interval containing \(0\) and \(x,\) then \(f^\prime(x)=\sum_{n=1}^\infty a_nnx^{n-1}\).%
\end{corollary}
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-2-15}%
\index{power series!term by term derivative of}\index{differentiation!term by term differentiation of power series} Prove \hyperref[cor_UniformConvergenceDerivative]{Corollary~{\xreffont\ref{cor_UniformConvergenceDerivative}}}.%
\end{problem}
The above results say that a power series can be differentiated and integrated term-by-term as long as the convergence is uniform.  Fortunately it is, in general, true that when a power series converges the convergence of it and its integrated and differentiated series is also uniform (almost).%
\par
However we do not yet have all of the tools necessary to see this.  To build these tools requires that we return briefly to our study, begun in \hyperref[Convergence]{Chapter~{\xreffont\ref{Convergence}}}, of the convergence of sequences.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Cauchy Sequences}
\typeout{************************************************}
%
\begin{subsectionptx}{Subsection}{Cauchy Sequences}{}{Cauchy Sequences}{}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-3}
Knowing that a sequence or a series converges and knowing what it converges to are typically two different matters.  For example, we know that \(\sum_{n=0}^\infty\frac{1}{n!}\)and \(\sum_{n=0}^\infty\frac{1}{n!\,n!}\) both converge.  The first converges to \(e,\) which has meaning in other contexts.  We don't know what the second one converges to, other than to say it converges to \(\sum_{n=0}^\infty\frac{1}{n!\,n!}\).  In fact, that question might not have much meaning without some other context in which \(\sum_{n=0}^\infty\frac{1}{n!\,n!}\) arises naturally.  Be that as it may, we need to look at the convergence of a series (or a sequence for that matter) without necessarily knowing what it might converge to.  We make the following definition.%
\begin{definition}{Definition}{}{def_CauchySequence}%
\index{sequences!Cauchy sequences} Let \(\left(s_n\right)\) be a sequence of real numbers. We say that \(\left(s_n\right)\)is a \terminology{Cauchy sequence} if for any \(\eps>0,\) there exists a real number \(N\) such that if \(m,n>N,\) then \(|s_m-s_n|\lt \eps\).%
\end{definition}
Notice that this definition says that the terms in a Cauchy sequence get arbitrarily close to each other and that there is no reference to getting close to any particular fixed real number.  Furthermore, you have already seen lots of examples of Cauchy sequences as illustrated by the following result.%
\begin{theorem}{Theorem}{}{}{thm_Converge-_Cauchy}%
\index{sequences!convergence}\index{convergence!of a sequence!implies Cauchy sequence} Suppose \(\left(s_n\right)\) is a sequence of real numbers which converges to \(s\).  Then \(\left(s_n\right)\) is a Cauchy sequence.%
\end{theorem}
Intuitively, this result makes sense.  If the terms in a sequence are getting arbitrarily close to \(s,\) then they should be getting arbitrarily close to each other.  This is the basis of the proof.%
\begin{aside}{Aside}{Comment.}{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-7}%
But the converse isn't nearly as clear.  In fact, it isn't true in the rational numbers.%
\end{aside}
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-8}%
Prove \hyperref[thm_Converge-_Cauchy]{Theorem~{\xreffont\ref{thm_Converge-_Cauchy}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-8-4}{}\quad{}\(|s_m-s_n|=|s_m-s+s-s_n|\leq|s_m-s\mathopen|+|s-s_n|\).%
\end{problem}
So any convergent sequence is automatically Cauchy.  For the real number system, the converse is also true and, in fact, is equivalent to any of our completeness axioms: the \hyperref[NIP]{NIP}, the Bolzano-Weierstrass Theorem, or the LUB Property.  Thus, this could have been taken as our completeness axiom and we could have used it to prove the others.  One of the most convenient ways to prove this converse is to use the Bolzano-Weierstrass Theorem.  To do that, we must first show that a Cauchy sequence must be bounded.  This result is reminiscent of the fact that a convergent sequence is bounded (\hyperref[lemma_BoundedConvergent]{Lemma~{\xreffont\ref{lemma_BoundedConvergent}}} of \hyperref[Convergence]{Chapter~{\xreffont\ref{Convergence}}}) and the proof is very similar.%
\begin{lemma}{Lemma}{}{}{lemma_Cauchy-_Bounded}%
Suppose \(\left(s_n\right)\) \(\)is a Cauchy sequence.  Then there exists \(B>0\) such that \(|s_n|\leq B\) for all \(n\).%
\end{lemma}
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-11}%
Prove \hyperref[lemma_Cauchy-_Bounded]{Lemma~{\xreffont\ref{lemma_Cauchy-_Bounded}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-11-3}{}\quad{}This is similar to \hyperref[prob_BoundedConvergent]{Problem~{\xreffont\ref{prob_BoundedConvergent}}} of \hyperref[Convergence]{Chapter~{\xreffont\ref{Convergence}}}.  There exists \(N\) such that if \(m,n>N\)then \(|s_n-s_m|\lt
1\). Choose a fixed \(m>N\) and let \(B=\max\left(\abs{s_1}, \abs{s_2}, \ldots,
\abs{s_{\lceil N\rceil}}, \abs{s_m}+1\right)\).%
\end{problem}
\begin{theorem}{Theorem}{}{}{thm_Cauchy-_Converge}%
\alert{Cauchy sequences converge}%
\par
\index{sequences!Cauchy sequences!convergence of} Suppose \(\left(s_n\right)\) is a Cauchy sequence of real numbers.  There exists a real number \(s\) such that \(\lim_{n\rightarrow\infty}s_n=s\).%
\end{theorem}
\begin{proof}{Proof}{Sketch of Proof.}{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-13}
We know that \(\left(s_n\right)\) \(\)is bounded, so by the Bolzano-Weierstrass Theorem, it has a convergent subsequence \(\left(s_{n_k}\right)\) converging to some real number \(s\).  We have \(|s_n-s|=|s_n-s_{n_k}+s_{n_k}-s|\leq|s_n-s_{n_k}\mathopen|+|s_{n_k}-s|\). If we choose \(n\) and \(n_k\) large enough, we should be able to make each term arbitrarily small.%
\end{proof}
\begin{problem}{Problem}{}{prob_Cauchy_sequences_Cauchy_implies_convergence}%
\index{sequences!Cauchy sequences!convergence of} Provide a formal proof of \hyperref[thm_Cauchy-_Converge]{Theorem~{\xreffont\ref{thm_Cauchy-_Converge}}}.%
\end{problem}
From \hyperref[thm_Converge-_Cauchy]{Theorem~{\xreffont\ref{thm_Converge-_Cauchy}}} we see that every Cauchy sequence converges in \(\RR\).  Moreover the proof of this fact depends on the Bolzano-Weierstrass Theorem which, as we have seen, is equivalent to our completeness axiom, the Nested Interval Property.  What this means is that if there is a Cauchy sequence which does not converge then the \hyperref[NIP]{NIP} is not true.  A natural question to ask is if every Cauchy sequence converges does the \hyperref[NIP]{NIP} follow? That is, is the convergence of Cauchy sequences also equivalent to our completeness axiom?  The following theorem shows that the answer is yes.%
\begin{theorem}{Theorem}{}{}{thm_ConvCauchyEquivNIP}%
\index{sequences!Cauchy sequences!convergence of is equivalent to the NIP} Suppose every Cauchy sequence converges.  Then the Nested Interval Property is true.%
\end{theorem}
\begin{problem}{Problem}{}{prob_Cauchy_sequences_Cauchy_implies_NIP}%
Prove \hyperref[thm_ConvCauchyEquivNIP]{Theorem~{\xreffont\ref{thm_ConvCauchyEquivNIP}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{prob_Cauchy_sequences_Cauchy_implies_NIP-3}{}\quad{}If we start with two sequences \(\left(x_n\right)\) and \(\left(y_n\right)\), satisfying all of the conditions of the \hyperref[NIP]{NIP}, you should be able to show that these are both Cauchy sequences.%
\end{problem}
\hyperref[prob_Cauchy_sequences_Cauchy_implies_convergence]{Problems~{\xreffont\ref{prob_Cauchy_sequences_Cauchy_implies_convergence}}} and \hyperref[prob_Cauchy_sequences_Cauchy_implies_NIP]{Problem~{\xreffont\ref{prob_Cauchy_sequences_Cauchy_implies_NIP}}} tell us that the following are equivalent: the Nested Interval Property, the Bolzano-Weierstrass Theorem, the Least Upper Bound Property, and the convergence of Cauchy sequences.  Thus any one of these could have been taken as the completeness axiom of the real number system and then used to prove the each of the others as a theorem according to the following dependency graph:%
\begin{image}{0}{1}{0}{}%
\includegraphics[width=\linewidth]{external/images/CompletenessAxioms.png}
\end{image}%
Since we can get from any node on the graph to any other, simply by following the implications (indicated with arrows), any one of these statements is logically equivalent to each of the others.%
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-21}%
\index{sequences!Cauchy sequences!don't always converge in \(\QQ\)} Since the convergence of Cauchy sequences can be taken as the completeness axiom for the real number system, it does not hold for the rational number system.  Give an example of a Cauchy sequence of rational numbers which does not converge to a rational number.%
\end{problem}
If we apply the above ideas to series we obtain the following important result, which will provide the basis for our investigation of power series.%
\begin{theorem}{Theorem}{Cauchy Criterion.}{}{thm_CauchyCriterion}%
\index{series!Cauchy sequences!Cauchy Criterion}%
The series \(\sum_{k=0}^\infty a_k\) converges if and only if \(\forall\) \(\eps>0\), \(\ \exists\) \(N\) such that if \(m>n>N\) then \(|\sum_{k=n+1}^ma_k|\lt \eps\).%
\end{theorem}
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-24}%
\index{series!Cauchy Criterion} Prove the Cauchy criterion.%
\end{problem}
At this point several of the tests for convergence that you probably learned in Calculus are easily proved.  For example:%
\begin{problem}{Problem}{}{prob_NthTermTest}%
\alert{The \(n\)th Term Test}%
\par
\index{series!\(n\)th term test}\index{divergence!of a series!\(n\)th term test} Show that if \(\sum_{n=1}^\infty a_n\) converges then \(\limit{n}{\infty}{a_n}=0\).%
\end{problem}
\begin{problem}{Problem}{The Strong Cauchy Criterion.}{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-27}%
\index{series!Cauchy Criterion!Strong Cauchy criterion} Show that \(\displaystyle\sum_{k=1}^\infty a_k\) converges if and only if \(\limit{n}{\infty}{\sum_{k=n+1}^\infty a_k}=0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-27-3}{}\quad{}The hardest part of this problem is recognizing that it is really about the limit of a sequence as in \hyperref[Convergence]{Chapter~{\xreffont\ref{Convergence}}}.%
\end{problem}
You may also recall the Comparison Test from studying series in Calculus: suppose \(0\leq a_n\leq b_n\), if \(\sum
b_n\) converges then \(\sum a_n\) converges.  This result follows from the fact that the partial sums of \(\sum a_n\) form an increasing sequence which is bounded above by \(\sum
b_n\).  (See \hyperref[cor_IncBoundedConverge]{Corollary~{\xreffont\ref{cor_IncBoundedConverge}}} of \hyperref[IVTandEVT]{Chapter~{\xreffont\ref{IVTandEVT}}}.)  The Cauchy Criterion allows us to extend this to the case where the terms \(a_n\) could be negative as well.  This can be seen in the following theorem.%
\begin{theorem}{Theorem}{}{}{thm_ComparisonTest}%
\alert{Comparison Test}%
\par
\index{series!Comparison Test} Suppose \(|a_n|\leq b_n\) for all \(n\).  If \(\sum
b_n\) converges then \(\sum a_n\) also converges.%
\end{theorem}
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-30}%
Prove \hyperref[thm_ComparisonTest]{Theorem~{\xreffont\ref{thm_ComparisonTest}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-30-4}{}\quad{}Use the Cauchy criterion with the fact that \(\abs{\sum_{k=n+1}^ma_k}\leq\sum_{k=n+1}^m\abs{a_k}\).%
\end{problem}
The following definition is of marked importance in the study of series.%
\begin{definition}{Definition}{Absolute Convergence.}{AbsoluteConvergence}%
\index{Absolute Convergence}%
\index{convergence!of a series!absolute} Given a series \(\sum a_n\), the series \(\sum|a_n|\) is called the \terminology{absolute series} of \(\sum a_{n}\) and if \(\sum|a_n|\) converges then we say that \(\sum a_{n}\) \terminology{converges absolutely.}%
\end{definition}
The significance of this definition comes from the following result.%
\begin{corollary}{Corollary}{}{}{cor_AbsConv-_Conv}%
If \(\sum a_n\) converges absolutely, then \(\sum a_n\) converges.%
\end{corollary}
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-35}%
\index{convergence!of a series!absolute convergence implies convergence} Show that \hyperref[cor_AbsConv-_Conv]{Corollary~{\xreffont\ref{cor_AbsConv-_Conv}}} is a direct consequence of \hyperref[thm_ComparisonTest]{Theorem~{\xreffont\ref{thm_ComparisonTest}}}.%
\end{problem}
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-36}%
\index{series!absolute convergence of!vs. the absolute value of a series} If \(\sum_{n=0}^\infty|a_n|=s\), then does it follow that \(s=|\sum_{n=0}^\infty a_n|\)?  Justify your answer. What can be said?%
\end{problem}
The converse of \hyperref[cor_AbsConv-_Conv]{Corollary~{\xreffont\ref{cor_AbsConv-_Conv}}} is not true as evidenced by the series \(\sum_{n=0}^\infty\frac{(-1)^n}{n+1}\).  As we noted in \hyperref[PowerSeriesQuestions]{Chapter~{\xreffont\ref{PowerSeriesQuestions}}}, this series converges to ln \(2\).  However, its absolute series is the Harmonic \index{series!Harmonic Series} Series which diverges.  Any such series which converges, but not absolutely, is said to \alert{converge conditionally}. Recall also that in \hyperref[PowerSeriesQuestions]{Chapter~{\xreffont\ref{PowerSeriesQuestions}}}, we showed that we could rearrange the terms of the series \(\sum_{n=0}^\infty\frac{(-1)^n}{n+1}\) to make it converge to any number we wished.  We noted further that all rearrangements of the series \(\sum_{n=0}^\infty\frac{(-1)^n}{\left(n+1\right)^2}\) converged to the same value.  The difference between the two series is that the latter converges absolutely whereas the former does not.  Specifically, we have the following result.%
\begin{theorem}{Theorem}{}{}{thm_RearrageAbsConv}%
\index{series!rearrangements}\index{series!absolute convergence of!rearrangements} Suppose \(\sum a_n\) converges absolutely and let \(s=\sum_{n=0}^\infty a_n\).  Then any rearrangement of \(\sum a_n\) must converge to \(s\).%
\end{theorem}
\begin{proof}{Proof}{Sketch of Proof.}{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-39}
We will first show that this result is true in the case where \(a_n\geq 0\).  If \(\sum b_n\) represents a rearrangement of \(\sum a_n\), then notice that the sequence of partial sums \(\left(\sum_{k=0}^nb_k\right)_{n=0}^\infty\)is an increasing sequence which is bounded by \(s\).  By \hyperref[cor_IncBoundedConverge]{Corollary~{\xreffont\ref{cor_IncBoundedConverge}}} of \hyperref[IVTandEVT]{Chapter~{\xreffont\ref{IVTandEVT}}}, this sequence must converge to some number \(t\) and \(t\leq s\).  Furthermore \(\sum a_n\) is also a rearrangement of \(\sum b_n\). Thus the result holds for this special case. (Why?) For the general case, notice that \(a_n=\frac{|a_n\mathopen|+a_n}{2}-\frac{|a_n\mathopen|-a_n}{2}\) and that \(\sum\frac{|a_n\mathopen|+a_n}{2}\) and \(\sum\frac{|a_n\mathopen|-a_n}{2}\) are both convergent series with nonnegative terms.  By the special case \(\sum\frac{|b_n\mathopen|+b_n}{2}=\) \(\sum\frac{|a_n\mathopen|+a_n}{2}\) and \(\sum\frac{|b_n\mathopen|-b_n}{2}=\) \(\sum\frac{|a_n\mathopen|-a_n}{2}\).%
\end{proof}
\begin{problem}{Problem}{}{PowerSeriesRedux-UnifConv-IntsAndDerivs-3-40}%
\index{series!absolute convergence of!rearrangements} Fill in the details and provide a formal proof of \hyperref[thm_RearrageAbsConv]{Theorem~{\xreffont\ref{thm_RearrageAbsConv}}}.%
\end{problem}
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 11.3 Radius of Convergence of a Power Series}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Radius of Convergence of a Power Series}{}{Radius of Convergence of a Power Series}{}{}{PowerSeriesRedux-RadiusOfConv}
We've developed enough machinery to look at the convergence of power series.  The fundamental result is the following theorem due to Abel.%
\begin{theorem}{Theorem}{}{}{thm_RadiusOfConvergence}%
\index{power series!converge inside radius of convergence} Suppose \(\sum_{n=0}^\infty a_nc^n\) converges for some nonzero real number \(c\).  Then \(\sum_{n=0}^\infty
a_nx^n\) converges absolutely for all \(x\) such that \(|x|\lt |c|\).%
\end{theorem}
To prove \hyperref[thm_RadiusOfConvergence]{Theorem~{\xreffont\ref{thm_RadiusOfConvergence}}} first note that by \hyperref[prob_NthTermTest]{Problem~{\xreffont\ref{prob_NthTermTest}}}, \(\limit{n}{\infty}{a_nc^n}=0\).  Thus \(\left(a_nc^n\right)\) is a bounded sequence.  Let \(B\) be a bound: \(\abs{a_nc^n}\le B\).  Then%
\begin{equation*}
\abs{a_nx^n}=\abs{a_nc^n\cdot\left(\frac{x}{c}\right)^n}\leq B\abs{\frac{x}{c}}^n\text{.}
\end{equation*}
%
\par
We can now use the comparison test.%
\begin{problem}{Problem}{}{PowerSeriesRedux-RadiusOfConv-6}%
\index{power series!the radius of convergence}\index{convergence!the radius of convergence of a power series} Prove \hyperref[thm_RadiusOfConvergence]{Theorem~{\xreffont\ref{thm_RadiusOfConvergence}}}.%
\end{problem}
\begin{corollary}{Corollary}{}{}{cor_RadiusOfDivergence}%
Suppose \(\sum_{n=0}^\infty a_nc^n\) diverges for some real number \(c\).  Then \(\sum_{n=0}^\infty a_nx^n\) diverges for all \(x\) such that \(|x|>|c|\).%
\end{corollary}
\begin{problem}{Problem}{}{PowerSeriesRedux-RadiusOfConv-8}%
\index{power series!the radius of convergence}\index{power series!a power series diverges outside it's radius of convergence} Prove \hyperref[cor_RadiusOfDivergence]{Corollary~{\xreffont\ref{cor_RadiusOfDivergence}}}.%
\end{problem}
As a result of \hyperref[thm_RadiusOfConvergence]{Theorem~{\xreffont\ref{thm_RadiusOfConvergence}}} and \hyperref[cor_RadiusOfDivergence]{Corollary~{\xreffont\ref{cor_RadiusOfDivergence}}}, we have the following: either \(\sum_{n=0}^\infty a_nx^n\) converges absolutely for all \(x\) or there exists some nonnegative real number \(r\) such that \(\sum_{n=0}^\infty a_nx^n\) converges absolutely when \(|x|\lt r\) and diverges when \(|x|>r\).  In the latter case, we call \(r\) the \emph{radius of convergence} of the power series \(\sum_{n=0}^{\infty}a_{n} x^{n}\).  In the former case, we say that the radius of convergence of \(\sum_{n=0}^\infty
a_nx^n\) is \(\infty\).  Though we can say that \(\sum_{n=0}^\infty a_nx^n\) converges absolutely when \(|x|\lt r\), we cannot say that the convergence is uniform. However, we can come close.  We can show that the convergence is uniform for \(|x|\leq b\lt r\).  To see this we will use the following result%
\begin{theorem}{Theorem}{The Weierstrass-\(M\) Test.}{}{thm_WeierstrassM}%
\index{Weierstrass-\(M\) Test} Let \(\left(f_n\right)_{n=1}^\infty\) be a sequence of functions defined on \(S\subseteq\RR\) and suppose that \(\left(M_n\right)_{n=1}^\infty\) is a sequence of nonnegative real numbers such that%
\begin{equation*}
\abs{f_n(x)}\leq M_n,\,\, \forall x\in S,\,\, n=1, 2, 3, \ldots\text{.}
\end{equation*}
%
\par
If \(\sum_{n=1}^\infty M_n\) converges then \(\sum_{n=1}^\infty f_n(x)\) converges uniformly on \(S\) to some function (which we will denote by \(f(x)\)).%
\end{theorem}
\begin{proof}{Proof}{Sketch of Proof.}{PowerSeriesRedux-RadiusOfConv-11}
Since the crucial feature of the theorem is the function \(f(x)\) that our series converges to, our plan of attack is to first define \(f(x)\) and then show that our series, \(\sum_{n=1}^\infty f_n(x)\), converges to it uniformly.%
\par
First observe that for any \(x\in S\), \(\sum_{n=1}^\infty
f_n(x)\) converges by the Comparison Test (in fact it converges absolutely) to some number we will denote by \(f(x)\).  This actually defines the function \(f(x)\) for all \(x\in S\).  It follows that \(\sum_{n=1}^\infty
f_n(x)\) converges pointwise to \(f(x)\).%
\par
Next, let \(\eps>0\) be given.  Notice that since \(\sum_{n=1}^\infty M_n\) converges, say to \(M\), then there is a real number, \(N\), such that if \(n>N\), then%
\begin{equation*}
\sum_{k=n+1}^\infty M_k = \abs{\sum_{k=n+1}^\infty M_k} = \abs{M-\sum_{k=1}^n M_k}\lt \eps\text{.}
\end{equation*}
%
\par
You should be able to use this to show that if \(n>N\), then%
\begin{equation*}
\abs{f(x) - \sum_{k=1}^n f_k(x)}\lt  \eps, \, \, \forall x\in S\text{.}
\end{equation*}
%
\end{proof}
\begin{problem}{Problem}{}{PowerSeriesRedux-RadiusOfConv-12}%
\index{Weierstrass-\(M\) Test} Use the ideas above to provide a formal proof of \hyperref[thm_WeierstrassM]{Theorem~{\xreffont\ref{thm_WeierstrassM}}}.%
\end{problem}
\begin{problem}{Problem}{}{PowerSeriesRedux-RadiusOfConv-13}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Referring back to \hyperref[PDE_sol]{equation~({\xreffont\ref{PDE_sol}})}, show that the Fourier series%
\begin{equation*}
\sum_{k=0}^\infty\frac{(-1)^k}{(2k+1)^2}\sin\left((2k+1)\pi x\right)
\end{equation*}
converges uniformly on \(\RR\).%
\item{}Does its differentiated series converge uniformly on \(\RR?\) Explain.%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{PowerSeriesRedux-RadiusOfConv-14}%
Observe that for all \(x \in [-1,1]\) \(\abs{x}\le
1\).  Identify which of the following series converges pointwise and which converges uniformly on the interval \([-1,1]\).  In every case identify the limit function.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\displaystyle \sum_{n=1}^\infty\left(x^n-x^{n-1}\right)\)%
\item{}\(\displaystyle \sum_{n=1}^\infty\frac{\left(x^n-x^{n-1}\right)}{n}\)%
\item{}\(\sum_{n=1}^\infty\frac{\left(x^n-x^{n-1}\right)}{n^2}\)%
\end{enumerate}%
\end{problem}
Using the Weierstrass-\(M\) test, we can prove the following result.%
\begin{theorem}{Theorem}{}{}{thm_PowerSeriesConvergeUniformly}%
\index{power series!converge uniformly on their interval of convergence} Suppose \(\sum_{n=0}^\infty a_nx^n\) has radius of convergence \(r\) (where \(r\) could be \(\infty\) as well).  Let \(b\) be any nonnegative real number with \(b\lt r\).  Then \(\sum_{n=0}^\infty a_nx^n\) converges uniformly on \([-b,b]\).%
\end{theorem}
\begin{problem}{Problem}{}{PowerSeriesRedux-RadiusOfConv-17}%
Prove \hyperref[thm_PowerSeriesConvergeUniformly]{Theorem~{\xreffont\ref{thm_PowerSeriesConvergeUniformly}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesRedux-RadiusOfConv-17-3}{}\quad{}We know that \(\sum_{n=0}^\infty|a_nb^n|\) converges. This should be all set for the Weierstrass-\(M\) test.%
\end{problem}
To finish the story on differentiating and integrating power series, all we need to do is show that the power series, its integrated series, and its differentiated series all have the same radius of convergence.  You might not realize it, but we already know that the integrated series has a radius of convergence at least as big as the radius of convergence of the original series.  Specifically, suppose \(f(x)=\sum_{n=0}^\infty a_nx^n\)has a radius of convergence \(r\) and let \(|x|\lt r\).  We know that \(\,\sum_{n=0}^\infty a_nx^n\) converges uniformly on an interval containing \(0\)and \(x\), and so by \hyperref[cor_IntConvUni]{Corollary~{\xreffont\ref{cor_IntConvUni}}}, \(\int_{t=0}^xf(t)\dx{
t}=\sum_{n=0}^\infty\left(\frac{a_n}{n+1}x^{n+1}\right)\).  In other words, the integrated series converges for any \(x\) with\(\,|x|\lt r\).  This says that the radius of convergence of the integated series must be at least \(r\).%
\par
To show that the radii of convergence are the same, all we need to show is that the radius of convergence of the differentiated series is at least as big as \(r\) as well.  Indeed, since the differentiated series of the integrated series is the original, then this would say that the original series and the integrated series have the same radii of convergence.  Putting the differentiated series into the role of the original series, the original series is now the integrated series and so these would have the same radii of convergence as well.  With this in mind, we want to show that if \(|x|\lt r\), then \(\sum_{n=0}^\infty a_nnx^{n-1}\) converges.  The strategy is to mimic what we did in \hyperref[thm_RadiusOfConvergence]{Theorem~{\xreffont\ref{thm_RadiusOfConvergence}}}, where we essentially compared our series with a converging geometric series.  Only this time we need to start with the differentiated geometric series.%
\begin{problem}{Problem}{}{prob_PwrSeriesDiffConv}%
Show that \(\sum_{n=1}^\infty nx^{n-1}\) converges for \(|x|\lt 1\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{prob_PwrSeriesDiffConv-4}{}\quad{}We know that \(\sum_{k=0}^nx^k=\frac{x^{n+1}-1}{x-1}\). Differentiate both sides and take the limit as \(n\) approaches infinity.%
\end{problem}
\begin{theorem}{Theorem}{}{}{thm_SeriesConv-_DerivConv}%
\index{power series!term by term derivative of} Suppose \(\sum_{n=0}^\infty a_nx^n\) has a radius of convergence \(r\) and let \(|x|\lt r\).  Then \(\sum_{n=1}^\infty a_nnx^{n-1}\) converges.%
\end{theorem}
\begin{problem}{Problem}{}{PowerSeriesRedux-RadiusOfConv-22}%
Prove \hyperref[thm_SeriesConv-_DerivConv]{Theorem~{\xreffont\ref{thm_SeriesConv-_DerivConv}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesRedux-RadiusOfConv-22-3}{}\quad{}Let \(b\) be a number with \(\abs{x}\lt b\lt r\) and consider \(\abs{a_nnx^{n-1}}
=\abs{a_nb^n\cdot\frac{1}{b}\cdot
n\left(\frac{x}{b}\right)^{n-1}}\).  You should be able to use the Comparison Test and \hyperref[prob_PwrSeriesDiffConv]{Problem~{\xreffont\ref{prob_PwrSeriesDiffConv}}}.%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 11.4 Boundary Issues and Abel's Theorem}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Boundary Issues and Abel's Theorem}{}{Boundary Issues and Abel's Theorem}{}{}{PowerSeriesRedux-AbelsThm}
Summarizing our results, we see that any power series \(\sum
a_nx^n\) has a radius of convergence \(r\) such that \(\sum a_nx^n\) converges absolutely when \(|x|\lt r\) and diverges when \(|x|>r\).  Furthermore, the convergence is uniform on any closed interval \([-b,b]\subset(-r,r)\) which tells us that whatever the power series converges to must be a continuous function on \((-r,r)\).  Lastly, if \(f(x)=\sum_{n=0}^\infty a_nx^n\) for \(x\in(-r,r)\), then \(f^\prime(x)=\sum_{n=1}^\infty a_nnx^{n-1}\) for \(x\in(-r,r)\) and \(\int_{t=0}^xf(t)\dx{ t} =
\sum_{n=0}^\infty a_n\frac{x^{n+1}}{n+1}\) for \(x\in(-r,r)\).%
\par
Thus power series are very well behaved within their interval of convergence, and our cavalier approach from \hyperref[CalcIn17th18thCentury]{Chapter~{\xreffont\ref{CalcIn17th18thCentury}}} is justified, \alert{EXCEPT} for one issue.  If you go back to \hyperref[prob_alternating_harmonic_series]{Problem~{\xreffont\ref{prob_alternating_harmonic_series}}} of \hyperref[CalcIn17th18thCentury]{Chapter~{\xreffont\ref{CalcIn17th18thCentury}}}, you see that we used the geometric series to obtain the series, \(\arctan x
=\sum_{n=0}^\infty(-1)^n\frac{1}{2n+1}x^{2n+1}\).  We substituted \(x=1\) into this to obtain \(\frac{\pi}{4}=\sum_{n=0}^\infty(-1)^n\frac{1}{2n+1}\). Unfortunately, our integration was only guaranteed on a closed subinterval of the interval \((-1,1)\) where the convergence was uniform and we substituted in \(x=1\).  We ``danced on the boundary'' in other places as well, including when we said that%
\begin{equation*}
\frac{\pi}{4}=\int_{x=0}^1\sqrt{1-x^2}\dx{x}=1+\sum_{n=1}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\text{ } \right)\left(\frac{\left(-1\right)^n}{2n+1}\right)\text{.}
\end{equation*}
%
\par
The fact is that for a power series \(\sum a_nx^n\) with radius of convergence \(r\), we know what happens when \(|x|\lt r\) and when \(|x|>r\). But we've never talked about what happens when \(|x|=r\).  That is because there is no systematic approach to this boundary problem.  For example, consider the three series%
\begin{equation*}
\sum_{n=0}^\infty x^n,\sum_{n=0}^\infty\frac{x^{n+1}}{n+1}, \sum_{n=0}^\infty\frac{x^{n+2}}{(n+1)(n+2)}\text{.}
\end{equation*}
%
\par
They are all related in that we started with the geometric series and integrated twice, thus they all have radius of convergence equal to 1.  Their behavior on the boundary, i.e., when \(x=\pm 1\), is another story.  The first series diverges when \(x=\pm 1\), the third series converges when \(x=\pm 1\).  The second series converges when \(x=-1\) and diverges when \(x=1\).%
\par
Even with the unpredictability of a power series at the endpoints of its interval of convergence, the Weierstrass-\(M\) test does give us some hope of uniform convergence.%
\begin{problem}{Problem}{}{PowerSeriesRedux-AbelsThm-7}%
Suppose the power series \(\sum a_nx^n\) has radius of convergence \(r\) and the series \(\sum a_nr^n\) converges absolutely.  Then \(\sum a_nx^n\) converges uniformly on \([-r,r]\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesRedux-AbelsThm-7-4}{}\quad{}For \(|x|\leq r\), \(|a_nx^n|\leq |a_nr^n|\).%
\end{problem}
Unfortunately, this result doesn't apply to the integrals we mentioned as the convergence at the endpoints is not absolute. Nonetheless, the integrations we performed in \hyperref[CalcIn17th18thCentury]{Chapter~{\xreffont\ref{CalcIn17th18thCentury}}} are still legitimate. This is due to the following theorem by Abel which extends uniform convergence to the endpoints of the interval of convergence even if the convergence at an endpoint is only conditional.  Abel did not use the term uniform convergence, as it hadn't been defined yet, but the ideas involved are his.%
\begin{theorem}{Theorem}{}{}{AbelsTheorem}%
\alert{Abel's Theorem}%
\par
\index{Abel, Niels Henrik!Abel's Theorem} Suppose the power series \(\sum a_nx^n\) has radius of convergence \(r\) and the series \(\sum a_nr^n\) converges.  Then \(\sum a_nx^n\) converges uniformly on \([0, r]\).%
\end{theorem}
The proof of this is not intuitive, but involves a clever technique known as Abel's Partial Summation Formula.%
\begin{lemma}{Lemma}{}{}{lemma_AbelsPartialSummationFormula}%
Let%
\begin{equation*}
a_1,a_2,\,\ldots,\,a_n,\,b_1,b_2,\,\ldots\,,\,b_n
\end{equation*}
be real numbers and let \(A_m=\sum_{k=1}^ma_k\).  Then%
\begin{equation*}
a_1b_1+a_2b_2+\cdots+a_nb_n=\sum_{j=1}^{n-1}A_j\left(b_j-b_{j+1}\text{ } \right)+A_nb_n\text{.}
\end{equation*}
%
\end{lemma}
\begin{problem}{Problem}{}{PowerSeriesRedux-AbelsThm-12}%
Prove \hyperref[lemma_AbelsPartialSummationFormula]{Lemma~{\xreffont\ref{lemma_AbelsPartialSummationFormula}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesRedux-AbelsThm-12-3}{}\quad{}For \(j>1\), \(a_j=A_j-A_{j-1}\).%
\end{problem}
\begin{lemma}{Lemma}{}{}{lemma_AbelsLemma}%
\alert{Abel's Lemma}%
\par
Let \(a_1,a_2,\,\ldots,\,a_n,\,b_1,b_2,\,\ldots\,,\,b_n\) be real numbers with \(\,b_1\geq
b_2\geq\,\ldots\geq\,b_n\geq 0\) and let \(A_m=\sum_{k=1}^ma_k\).  Suppose \(|A_m|\leq B\) for all \(m\).  Then \(|\sum_{j=1}^na_jb_j|\leq B\cdot
b_1\).%
\end{lemma}
\begin{problem}{Problem}{}{PowerSeriesRedux-AbelsThm-14}%
\index{Abel, Niels Henrik!Abel's Lemma} Prove \hyperref[lemma_AbelsLemma]{Lemma~{\xreffont\ref{lemma_AbelsLemma}}}.%
\end{problem}
\begin{problem}{Problem}{}{PowerSeriesRedux-AbelsThm-15}%
Prove \hyperref[AbelsTheorem]{Theorem~{\xreffont\ref{AbelsTheorem}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesRedux-AbelsThm-15-3}{}\quad{}Let \(\eps\gt0\).  Since \(\sum_{n=0}^\infty
a_nr^n\) converges then by the Cauchy Criterion, there exists \(N\) such that if \(m>n>N\) then \(\abs{\sum_{k=n+1}^ma_kr^k}\lt \frac{\eps }{2}\). Let \(0\leq x\leq r\). By \hyperref[lemma_AbelsLemma]{Lemma~{\xreffont\ref{lemma_AbelsLemma}}},%
\begin{equation*}
\abs{\sum_{k=n+1}^ma_kx^k}=\abs{\sum_{k=n+1}^ma_kr^k\left(\frac{x}{r}\right)^k}\leq \left(\frac{\eps }{2}\right)\left(\frac{x}{r}\right)^{n+1}\leq\frac{\epsilon}{2}\text{.}
\end{equation*}
%
\par
Thus for \(0\leq x\leq r\), \(n>N\),%
\begin{equation*}
\abs{\sum_{k=n+1}^\infty a_kx^k}=\lim_{m\rightarrow\infty}\abs{\sum_{k=n+1}^ma_kx^k}\leq\frac{\eps }{2}\lt \epsilon.\rbrack{}
\end{equation*}
%
\end{problem}
\begin{corollary}{Corollary}{}{}{cor_PowerSeriesConvUnif}%
Suppose the power series \(\sum a_nx^n\) has radius of convergence \(r\) and the series \(\sum
a_n\left(-r\right)^n\) converges.  Then \(\sum a_nx^n\) converges uniformly on \([-r,0]\).%
\end{corollary}
\begin{problem}{Problem}{}{PowerSeriesRedux-AbelsThm-17}%
Prove \hyperref[cor_PowerSeriesConvUnif]{Corollary~{\xreffont\ref{cor_PowerSeriesConvUnif}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{PowerSeriesRedux-AbelsThm-17-5}{}\quad{}Consider \(\sum a_n\left(-x\right)^n\).%
\end{problem}
\end{sectionptx}
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 12 Back to the Real Numbers}
\typeout{************************************************}
%
\begin{chapterptx}{Chapter}{Back to the Real Numbers}{}{Back to the Real Numbers}{}{}{BackToFourier}
\renewcommand*{\chaptername}{Chapter}
\begin{introduction}{}%
As we have seen, when they converge, power series are very well behaved.  The fact Fourier (trigonometric) series weren't always well behaved was a puzzle and it made them a lightning rod for mathematical study in the nineteenth century.%
\par
For example, consider the question of uniqueness.  We saw in \hyperref[TaylorSeries]{Chapter~{\xreffont\ref{TaylorSeries}}} that if a function could be represented by a power series, then that series must be the Taylor series.  More precisely, if%
\begin{equation*}
f(x)=\sum_{n=0}^\infty
a_n(x-a)^n \text{,}
\end{equation*}
then%
\begin{equation*}
a_n=\frac{f^{(n)}(a)}{n!}  \text{.}
\end{equation*}
%
\par
But what can be said about the uniqueness of a trigonometric series?  If we can represent a function \(f\) as a general trigonometric series%
\begin{equation*}
f(x)=\sum_{n=0}^\infty(a_n\cos n\pi
x\,+b_n\sin n\pi x) \text{,}
\end{equation*}
then must this be the Fourier series with the coefficients as determined by Fourier? \index{Fourier, Jean Baptiste Joseph}%
\par
For example, if \(\sum_{n=0}^\infty(a_n\cos n\pi x\,+b_n\sin
n\pi x)\) converges to \(f\) uniformly on the interval \((0,1)\), then because of the uniform convergence, Fourier's term\textendash{}by\textendash{}term integration which we saw in \hyperref[Interregnum]{Part~{\xreffont\ref{Interregnum}}} is perfectly legitimate and the coefficients are, of necessity, the coefficients he computed. However we have seen that the convergence of a Fourier series need not be uniform.  This does not mean that we cannot integrate term\textendash{}by\textendash{}term, but it does say that we can't be sure that term\textendash{}by\textendash{}term integration of a Fourier series will yield the integral of the associated function.%
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Cantor/}{Georg Cantor}\protect\footnotemark{}}{BackToFourier-2-5}{}%
\index{Cantor, Georg!portrait of}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Cantor.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[33]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Cantor/}\label{BackToFourier-2-5-1-2}}%
\index{Lebesgue, Henri}\index{Cantor, Georg} This led to a generalization of the integral by Henri Lebesgue in 1905.  Lebesgue's profound work settled the issue of whether or not a bounded pointwise converging trigonometric series is the Fourier series of a function, We will briefly describe Lebesgue's work later, but this actually needs some build up beyond what we talked about with regard to integration back in Chapter 10.  We will begin this build up by focusing on the work of \href{https://mathshistory.st-andrews.ac.uk/Biographies/Cantor/}{Georg Cantor}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Cantor/}\label{BackToFourier-2-6-5}} (1845\textendash{}1918) in the late 19th century. Cantor's work was profound and in many ways set the stage for how mathematics has been approached throughout the 20th century and currently.     %
\begin{aside}{Aside}{Comment.}{BackToFourier-2-7}%
`Weird' does not mean false.  It simply means that some of Cantor's results can be hard to accept, even after you have seen the proof and verified their validity.%
\end{aside}
To begin, let's suppress the underlying function and suppose we have%
\begin{equation*}
\sum_{n=0}^\infty(a_n\cos n\pi x\,+b_n\sin n\pi x) = \sum_{n=0}^\infty(a^\prime_n\cos n\pi x\,+b^\prime_n\sin n\pi x)\text{.}
\end{equation*}
%
\par
We ask: If these two series are equal must it be true that \(a_n=a^\prime_n\) and \(b_n=b^\prime_n\)? We can reformulate this uniqueness question as follows: Suppose%
\begin{equation*}
\displaystyle\sum_{n=0}^\infty\left((a_n-a^\prime_n)\cos n\pi x\,+(b_n-b^\prime_n)\sin n\pi x\right) = 0\text{.}
\end{equation*}
If we let \(c_n = a_n-a^\prime_n\) and \(d_n =
b_n-b^\prime_n\), then the question becomes: If%
\begin{equation*}
\sum_{n=0}^\infty\left(c_n\cos n\pi x\,+d_n\sin n\pi x\right)
= 0\text{,}
\end{equation*}
is it necessarily true that \(c_n=d_n=0\)?%
\par
Intuitively, it certainly seems reasonable to suppose so, but at this point we have enough experience with infinite sums to know that we need to be \emph{very} careful about relying on the intuition we have honed on finite sums.%
\par
\index{Cantor, Georg!and the modern view of mathematics} The converse of the question above: If \(c_n=d_n=0\), is it necessarily true that%
\begin{equation*}
\sum_{n=0}^\infty\left(c_n\cos n\pi
x\,+d_n\sin n\pi x\right) = 0\text{,}
\end{equation*}
is even more interesting, as we will soon see.  The answer to this seemingly basic question leads to some very profound results.%
\par
In particular, answering this question led Cantor to study the makeup of the real number system.  This in turn opened the door to the modern view of mathematics of the twentieth century.  In particular, Cantor proved the following result in 1871~(\hyperlink{jahnke03__histor_analy}{[{\xreffont 6}]}, p. 305).%
\begin{theorem}{Theorem}{Cantor.}{}{thm_Cantor-uniqueness-of-trig-series}%
\index{Cantor, Georg}\index{Cantor, Georg!uniqueness of Fourier series!first theorem on}\index{Fourier Series!Cantor's first theorem on uniqueness} If the trigonometric series%
\begin{equation*}
\sum_{n=0}^\infty\left(c_n\cos n\pi  x\,+d_n\sin n\pi x\right) = 0\text{,}
\end{equation*}
%
\par
``with the exception of certain values of \(x\),'' then all of its coefficients vanish.%
\end{theorem}
In his attempts to nail down precisely which ``certain values'' could be exceptional, Cantor was led to examine the nature of subsets of real numbers and ultimately to give a precise definition of the concept of infinite sets and to define an arithmetic of ``infinite numbers.''%
\begin{aside}{Aside}{Transfinite Numbers.}{BackToFourier-2-15}%
Actually, he called them \terminology{transfinite numbers} because, by definition, numbers are finite.%
\end{aside}
As a first step toward identifying those ``certain values,'' Cantor proved the following theorem, which we will state but not prove.%
\begin{theorem}{Theorem}{Cantor, (1870).}{}{thm_Cantor_1870}%
\index{Cantor, Georg!uniqueness of Fourier series!second theorem}%
\index{Fourier Series!Cantor's second theorem on uniqueness}%
If the trigonometric series%
\begin{equation*}
\sum_{n=0}^\infty\left(c_n\cos n\pi  x\,+d_n\sin n\pi x\right) = 0\text{,}
\end{equation*}
for all \(x\in\RR\) then all of its coefficients vanish.%
\end{theorem}
He then extended this to the following:%
\begin{theorem}{Theorem}{Cantor, (1871).}{}{thm_Cantor-1871-b}%
\index{Cantor, Georg!uniqueness of Fourier series!third theorem on}%
\index{Fourier Series!Cantor's third theorem on uniqueness}%
If the trigonometric series%
\begin{equation*}
\sum_{n=0}^\infty\left(c_n\cos n\pi x\,+d_n\sin n\pi
x\right) = 0 \text{,}
\end{equation*}
for all but finitely many \(x\in\RR\) then all of its coefficients vanish.%
\end{theorem}
Observe that this is not a trivial generalization.  Although the exceptional points are constrained to be finite in number, this number could still be extraordinarily large.  That is, even if the series given above differed from zero on \(10^{\left(10^{100000}\right)}\) distinct points in the interval \(\left(0, 10^{-\left(10^{100000}\right)}\right)\) the coefficients still vanish.  This remains true even if at each of these \(10^{\left(10^{100000}\right)}\) points the series converges to \(10^{\left(10^{100000}\right)}\).  This is truly remarkable when you think of it this way.%
\par
At this point Cantor became more interested in these exceptional points than in the Fourier series problem that he'd started with.  The next task he set for himself was to see just how general the set of exceptional points could be.  Following Cantor's lead we make the following definitions.%
\begin{definition}{Definition}{}{def_accumulation-point}%
\index{limit!accumulation points}%
\index{sets!accumulation points}%
Let \(S\subseteq \RR\) and let \(a\) be a real number. We say that \(a\) is a \terminology{limit point} (or an \terminology{accumulation point}) of \(S\) if there is a sequence \((a_n)\) with \(a_n\in S-\left\{a\right\}\) which converges to \(a\).%
\end{definition}
\begin{problem}{Problem}{}{PROBLEMAccumulationPoint}%
Let \(S\subseteq\RR\) and let \(a\) be a real number. Prove that \(a\) is a limit point of \(S\) if and only if for every \(\eps>0\) the intersection of the interval \((a-\eps, a+\eps)\) with \(S\) contains more than the single point \({a}\).%
\par
That is,%
\begin{equation*}
(a-\eps, a+\eps) \cap S -\left\{a\right\} \neq \emptyset. 
\end{equation*}
%
\end{problem}
The following definition gets to the heart of the matter.%
\begin{definition}{Definition}{}{def_derived-set}%
\index{sets!derived sets}%
Let \(S\subseteq\RR\).  The set of all limit points of \(S\) is called the \terminology{derived set} of \(S\). The derived set is denoted \(S^{\prime}\).%
\end{definition}
Don't confuse the derived set of a set with the derivative of a function.  They are completely different objects despite the similarity of both the language and notation.  The only thing that they have in common is that they were somehow ``derived'' from something else.%
\begin{problem}{Problem}{}{BackToFourier-2-27}%
Determine the derived set \(S^\prime\), of each of the following sets.%
\begin{multicols}{2}
\begin{enumerate}[label={(\alph*)}]
\item{}\(\displaystyle S=\left\{\frac11, \frac12, \frac13, \ldots\right\}\)%
\item{}\(\displaystyle S=\left\{0,\frac11, \frac12, \frac13, \ldots\right\}\)%
\item{}\(\displaystyle S=(0,1]\)%
\item{}\(\displaystyle S=\left[\left.0,1/2\right)\right.\cup\left.\left(1/2,1\right.\right]\)%
\item{}\(\displaystyle S=\QQ\)%
\item{}\(\displaystyle S=\RR-\QQ\)%
\item{}\(\displaystyle S=\ZZ\)%
\item{}Any finite set \(S\).%
\end{enumerate}
\end{multicols}
%
\end{problem}
\begin{problem}{Problem}{}{BackToFourier-2-28}%
\index{derived sets} Let \(S\subseteq\RR\).%
\begin{enumerate}[label={(\alph*)}]
\item{}Prove that \(\left(S^{\,\prime}\right)^{\,\prime}\subseteq
S^{\,\prime}\).%
\item{}Give an example where these two sets are equal.%
\item{}Give an example where these two sets are not equal.%
\end{enumerate}
%
\end{problem}
The notion of the derived set forms the foundation of Cantor's exceptional set of values.  Specifically, let \(S\) again be a set of real numbers and consider the following sequence of sets:%
\begin{equation*}
S^{\,\prime}\supseteq \left(S^\prime\right)^\prime\supseteq \left(\left(S^\prime\right)^\prime\right)^\prime\supseteq \cdots\text{.}
\end{equation*}
%
\par
\index{Cantor, Georg} Cantor showed that if, at some point, one of these derived sets is empty, then the uniqueness property still holds. Specifically, we have:%
\begin{theorem}{Theorem}{}{}{thm_Cantor-1871}%
\index{Cantor, Georg!fourth theorem on the uniqueness of Fourier series}%
Let \(S\) be a subset of the real numbers with the property that one of its derived sets is empty.  Then if the trigonometric series \(\sum_{n=0}^\infty\left(c_n\cos n\pi
x\,+d_n\sin n\pi x\right)\) is zero for all \(x\in\RR-S\), then all of the coefficients of the series vanish.%
\end{theorem}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Section 12.1 The Rise of Set Theory}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{The Rise of Set Theory}{}{The Rise of Set Theory}{}{}{SECTIONRiseSetTheory}
As much as any other mathematician, Cantor's work led an examination of the foundations of mathematics utilizing sets as the building blocks.  Mathematical ideas were being recast in the language of sets.  We do this nowadays naturally, so it doesn't seem profound, but it really shaped our modern approach. In analysis, we already saw this in \hyperref[def_accumulation-point]{Definition~{\xreffont\ref{def_accumulation-point}}}, \hyperref[PROBLEMAccumulationPoint]{Problem~{\xreffont\ref{PROBLEMAccumulationPoint}}}, and \hyperref[def_derived-set]{Definition~{\xreffont\ref{def_derived-set}}} where we recast Cantor's definition of a limit point into set theoretic terms.%
\par
It turns out that we can rewrite concepts such as limits and continuity in terms of sets.  This led to a subject known as point set topology.  We will not present all of the ideas from topology as this would require an entire new course with an entire new book.  But we can give you a look into how analysis concepts are recast in this set theoretic manner.%
\par
For example, a set of real numbers \(S\) is called closed if it contains all of its limit points.  In other words, \(S\) is closed if and only if \(S'\subset S.\) If this sounds the same as our notion of a closed interval \([a,b]\), it is meant to since the idea is to recast familiar notions in terms of sets.  In fact, this closed interval contains all of its limit points, so it really is closed.  To see this, we will show that any point not in \(S=[a,b]\) is not a limit point of \(S\).  First, suppose \(c\notin [a,b]\).  If \(c\lt a\), then if we let \(\eps =a-c\gt0\), then%
\begin{equation*}
\left(c-\eps
,c+\eps \right)\cap
\left[a,b\right]-\left\{c\right\}=\emptyset 
\end{equation*}
so \(c\notin S^\prime\).%
\begin{problem}{Problem}{}{SECTIONRiseSetTheory-5}%
Prove that if \(b\lt c\), then \(c\) is not a limit point of \([a,b]\).  This will complete the proof that \([a,b]\) is closed.%
\end{problem}
It is true that the term closed was modeled after closed intervals, but these are not the only closed sets.  Each of the following are closed sets.%
\begin{multicols}{2}
\begin{itemize}[label=\textbullet]
\item{}\(\RR\),%
\item{}\(\emptyset\),%
\item{}\(\left[a,\infty \right)\),%
\item{}\(\left(-\infty ,b\right]\),%
\item{}any finite set,%
\item{}\(\displaystyle \left\{\frac{1}{n},\ n\in \mathbb{N}\right\}\cup
\left\{0\right\}\)%
\item{}\(\displaystyle \ZZ\)%
\item{}the union of two closed sets%
\end{itemize}
\end{multicols}
%
\begin{problem}{Problem}{}{SECTIONRiseSetTheory-7}%
Convince yourself that each of the above sets is closed.%
\end{problem}
At this point you can guess that an open set is modeled after and open interval \((a,b)\).  It is, and this can be defined succinctly as%
\begin{definition}{Definition}{Open Sets.}{DEFINITIONOpenSet}%
A set \(S\) of real numbers is open if its complement \(S^c=\RR-S\) is closed.%
\end{definition}
\begin{problem}{Problem}{}{SECTIONRiseSetTheory-10}%
Show that a set \(S\) is open if and only if for all \(c\in S\) there is an \(\eps >0\) such that \(\left(c-\eps ,c+\eps \right)\subset S\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{SECTIONRiseSetTheory-10-2}{}\quad{}Look back at the proof that showed that a closed interval is actually closed.%
\end{problem}
\begin{problem}{Problem}{}{SECTIONRiseSetTheory-11}%
Open \emph{intervals} are not the only open sets. Convince yourself that each of the above sets that the following are also open sets.%
\begin{itemize}[label=\textbullet]
\item{}\(\displaystyle \RR \)%
\item{}\(\displaystyle \emptyset\)%
\item{}\(\bigcup^{\infty }_{n=1}{S_n}\) where each \(S_n\) is an open set.%
\end{itemize}
%
\end{problem}
\begin{problem}{Problem}{}{SECTIONRiseSetTheory-12}%
Of course, there are sets which are neither open nor closed.%
\par
Convince yourself that these sets are neither open nor closed.%
\begin{itemize}[label=\textbullet]
\item{}\(\left[a,b\right)\),%
\item{}\(\left(a,b\right]\),%
\item{}\(\displaystyle \left\{\frac{1}{n},\ n\in \NN\right\}\)%
\end{itemize}
%
\end{problem}
Again, we could spend an entire course delving into these concepts and more, but for our introduction we will show how the concept of continuity on a set can be repackaged into a statement involving sets.  To transition into talking about functions and sets, we have the following two definitions.  So we don't have to get into issues dealing with endpoints on intervals, etc., we will restrict ourselves to talking about functions \(f(x)\) whose domain is an open set \(D\) of real numbers.%
\begin{definition}{Definition}{}{DEFINITIONOpenByPreImage}%
Let \(S\subset D\), we define the image of \(S\) by%
\begin{equation*}
f\left(S\right)=\left\{f\left(x\right)\right|x\in S\}
\end{equation*}
%
\par
For any \(T\subset \mathbb{R}\), we define the preimage of \(T\) by%
\begin{equation*}
f^{-1}\left(T\right)=\left\{x\in D\right|f\left(x\right)\in T\}
\end{equation*}
%
\end{definition}
With these definitions we have the following result%
\begin{problem}{Problem}{}{SECTIONRiseSetTheory-16}%
Show that \(f\left(x\right)\) is continuous at \(x=a\) if and only if for all open sets \(V\) containing \(f(a)\), there is an open set \(U\) containing \(a\) with \(f\left(U\right)\subset V\).%
\end{problem}
Notice that in the previous problem, \(f\left(U\right)\subset
V\) can be written as \(U\subset f^{-1}(V)\).  Based on this observation and the previous problem, continuity of \(f\) on all of its domain \(D\) has an even easier condition when stated in terms of open sets.%
\begin{problem}{Problem}{}{SECTIONRiseSetTheory-18}%
Show that \(f\) is continuous on \(D\) if and only if \(f\) satisfies the following condition: for any open set of real numbers \(V\), \(f^{-1}(V)\) is open.%
\end{problem}
Notice that in the previous problem, \(f\left(U\right)\subset
V\) can be written as \(U\subset f^{-1}(V)\).  Based on this observation and the previous problem, continuity of \(f\) on all of its domain \(D\) has an even easier condition when stated in terms of open sets.%
\begin{problem}{Problem}{}{SECTIONRiseSetTheory-20}%
Show that \(f\) is continuous on \(D\) if and only if \(f\) satisfies the following condition: for any open set of real numbers \(V\), \(f^{-1}(V)\) is open.%
\end{problem}
The value of this reformulation of continuity, and similar definitions for limits in terms of open sets is that it can be generalized beyond the realm of real numbers to sets where the distance between points is either irrelevant or not even possible to determine.  With this in mind, in 1914, Felix Hausdorff (1868\textendash{}1942) defined what it means for a general set to have a topology based solely on sets and not relying on formulating a distance between its elements. Given an arbitrary nonempty set \(S\), we define a topology on \(S\) to be a collection \(\tau \) of subsets of \(S\) which we designate to be ``open'' sets.  The modern rendition of Hausdorff's definition of a topology says that \(\tau \) must satisfy the following axioms.%
\par
%
\begin{itemize}[label=\textbullet]
\item{}\(S\) and \(\emptyset \) must be in \(\tau \).%
\item{}An arbitrary union of open sets in \(\tau \) must also be in \(\tau \).%
\item{}The intersection of any finite number of open sets in \(\tau \) must also be in \(\tau \).%
\end{itemize}
%
\par
Hausdorff actually added an extra \textasciigrave{}\textasciigrave{}separation'{}'{} axiom.%
\par
%
\begin{itemize}[label=\textbullet]
\item{}For any two elements \(x,y\) of \(S\), there must exist two disjoint open sets \(U_x\) and \(U_y\) with \(x\in U_x\) and \(y\in U_y\).%
\end{itemize}
%
\par
Nowadays, a topological space (that is a set with a topology defined on it) which has this extra property is called a Hausdorff space.%
\par
This relatively simple idea generalizes our notion of open sets of real numbers and can be applied to a wide range of sets. We won't get into this, but we note that if we have a function \(f:S\to T\) from one topological space into another, we can define concepts like continuity by insisting that the preimage of any open set in the topology on \(T\) must be open in the topology on \(S\).  It would be hard to overstate the impact of this on modern mathematics.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 12.2 Infinite Sets}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Infinite Sets}{}{Infinite Sets}{}{}{BackToFourier-InfiniteSets}
Cantor's work with sets of real numbers prompted him to study the sizes of various infinite sets in general.  The following theorem follows directly from our previous work with the \hyperref[NIP]{NIP}  and will be very handy later.  It basically says that a sequence of nested closed intervals will still have a non\textendash{}empty intersection even if their lengths do not converge to zero as in the \hyperref[NIP]{NIP} .%
\begin{theorem}{Theorem}{}{}{thm_WeakNIP}%
\index{Nested Interval Property (NIP)!weak form}%
Let \(\left(\left[a_n, b_n\right]\right)_{n=1}^\infty\) be a sequence of nested intervals such that \(\limit{n}{\infty}{\abs{b_n-a_n}}>0\).  Then there is at least one \(c\in\RR\) such that \(c\in\left[a_n,
b_n\right]\) for all \(n\in\NN\).%
\end{theorem}
\begin{proof}{Proof}{}{BackToFourier-InfiniteSets-4}
By \hyperref[cor_IncBoundedConverge]{Corollary~{\xreffont\ref{cor_IncBoundedConverge}}} of \hyperref[IVTandEVT]{Chapter~{\xreffont\ref{IVTandEVT}}}, we know that a bounded increasing sequence such as \((a_n)\) converges, say to \(c\).  Since \(a_n\leq a_m\leq b_n\) for \(m>n\) and \(\limit{m}{\infty}{a_m}=c\), then for any fixed \(n\), \(a_n\leq c\leq b_n\).  This says \(c\in\left[a_n,
b_n\right]\) for all \(n\in\NN\).%
\end{proof}
\begin{problem}{Problem}{}{BackToFourier-InfiniteSets-5}%
Suppose \(\limit{n}{\infty}{\abs{b_n-a_n}}>0\).  Show that there are at least two points, \(c\) and \(d\), such that \(c\in[a_n, b_n]\) and \(d\in[a_n, b_n]\) for all \(n\in\NN\).%
\end{problem}
Our next theorem says that in a certain, very technical sense there are more real numbers than there are counting numbers \hyperlink{franks10__cantor_other_proof_rr_uncoun}{[{\xreffont 3}]}. This probably does not seem terribly significant.  After all, there are real numbers which are not counting numbers.  What will make this so startling is that the same cannot be said about all sets which strictly contain the counting numbers.  We will get into the details of this after the theorem is proved.%
\begin{theorem}{Theorem}{Cantor, (1874).}{}{thm_NoSeriesContainsAllReals}%
\index{Cantor, Georg!first proof that \(\RR\) is uncountable}%
\index{\(\RR\)!is uncountable!Cantor's first proof}%
\alert{}%
\par
Let \(S=\left(s_n\right)_{n=1}^\infty\) be a sequence of real numbers.  There is a real number \(c\), which is not in \(S\).%
\begin{aside}{Aside}{Notation.}{thm_NoSeriesContainsAllReals-4-3}%
To streamline things, we are abusing notation here as we are letting \(S\) denote both the sequence (which is ordered) and the underlying (unordered) set of entries in the sequence.%
\end{aside}
\end{theorem}
\begin{proof}{Proof}{}{BackToFourier-InfiniteSets-8}
For the sake of obtaining a contradiction assume that the sequence \(S\) contains every real number; that is, \(S=\RR\).  As usual we will build a sequence of nested intervals \(\left(\left[x_i,
y_i\right]\right)_{i=1}^\infty\).%
\par
Let \(x_1\) be the smaller of the first two distinct elements of \(S\), let \(y_1\) be the larger and take \(\left[x_1,y_1\right]\) to be the first interval.%
\par
Next we assume that \(\left[x_{n-1}, y_{n-1}\right]\) has been constructed and build \(\left[x_n, y_n\right]\) as follows.  Observe that there are infinitely many elements of \(S\) in \(\left(x_{n-1}, y_{n-1}\right)\) since \(S=\RR\).  Let \(s_m\) and \(s_k\) be the \emph{first} two distinct elements of \(S\) such that%
\begin{equation*}
s_m, s_k \in \left(x_{n-1}, y_{n-1}\right) \text{.}
\end{equation*}
%
\par
Take \(x_n\) to be the smaller and \(y_n\) to be the larger of \(s_m\) and \(s_k\).  Then \(\left[x_n,
y_n\right]\) is the \(n\)th interval.%
\par
From the way we constructed them it is clear that%
\begin{equation*}
\left[x_1, y_1\right] \supseteq \left[x_2, y_2\right]
\supseteq \left[x_3, y_3\right] \supseteq \ldots \text{.}
\end{equation*}
%
\par
Therefore by \hyperref[thm_WeakNIP]{Theorem~{\xreffont\ref{thm_WeakNIP}}} there is a real number, say \(c\), such that%
\begin{equation*}
c\in\left[x_n,
y_n\right] \text{ for all } n\in\NN \text{.}
\end{equation*}
%
\par
In fact, since \(x_1\lt x_2\lt x_3\ldots\lt y_3\lt y_2\lt
y_1\) it is clear that%
\begin{equation}
x_n\lt c\lt y_n, \ \forall n\text{.}\label{eq_NIP-interval}
\end{equation}
%
\par
We will show that \(c\) is the number we seek.  That the inequalities in \hyperref[eq_NIP-interval]{formula~({\xreffont\ref{eq_NIP-interval}})} are strict will play a crucial role.%
\par
To see that \(c\not\in S\) we suppose that \(c\in S\) and derive a contradiction.%
\par
So, suppose that \(c=s_p\) for some \(p\in\NN\).  Then only \(\left\{s_1, s_2,\ldots, s_{p-1}\right\}\) appear before \(s_p\) in the sequence \(S\).  Since each \(x_n\) is taken from \(S\) it follows that only finitely many elements of the sequence \((x_n)\) appear before \(s_p=c\) in the sequence as well.%
\par
Let \(x_l\) be the \emph{last} element of \((x_n)\) which appears before \(c=s_p\) in the sequence and consider \(x_{l+1}\).  The way it was constructed, \(x_{l+1}\) was one of the first two distinct terms in the sequence \(S\) strictly between \(x_l\) and \(y_l\), the other being \(y_{l+1}\).  Since \(x_{l+1}\) does not appear before \(c=s_p\) in the sequence and \(x_l\lt c\lt
y_l\), it follows that either \(c=x_{l+1}\) or \(c=y_{l+1}\).  However, this gives us a contradiction as we know from \hyperref[eq_NIP-interval]{formula~({\xreffont\ref{eq_NIP-interval}})} that \(x_{l+1}\lt c\lt y_{l+1}\).%
\par
Thus \(c\) is not an element of \(S\).%
\end{proof}
So how does this theorem show that there are ``more'' real numbers than counting numbers?  Before we address that question we need to be very careful about the meaning of the word ``more'' when we're talking about infinite sets.%
\par
First let's consider two finite sets, say \(A=\left\{\alpha,\beta,\gamma,\delta\right\}\) and \(B=\left\{a,b,c,d,e\right\}\).  How do we know that \(B\) is the bigger set? (It obviously is.) Clearly we can just count the number of elements in both \(A\) and \(B\).  Since \(\abs{A}=4\) and \(\abs{B}=5\) and \(4\lt 5\) \(B\) is clearly bigger.  But we're looking for a way to determine the relative size of two sets without counting them because we have no way of counting the number of elements of an infinite set. Indeed, it isn't even clear what the phrase ``the number of elements'' might \emph{mean} when applied to the elements of an infinite set.%
\par
When we count the number of elements in a finite set what we're really doing is matching up the elements of the set with a set of consecutive positive integers, starting at \(1\).  Thus since%
\begin{align*}
1\amp \leftrightarrow\alpha\\
2\amp \leftrightarrow\beta\\
3\amp \leftrightarrow\gamma\\
4\amp \leftrightarrow\delta
\end{align*}
we see that \(\abs{A}=4\).  Moreover, the order of the match\textendash{}up is unimportant.  Thus since%
\begin{align*}
2\amp \leftrightarrow e\\
3\amp \leftrightarrow a\\
5\amp \leftrightarrow b\\
4\amp \leftrightarrow d\\
1\amp \leftrightarrow c
\end{align*}
it is clear that the elements of \(B\) and the set \(\left\{1,2,3,4,5\right\}\) can be matched up as well.  And it doesn't matter what order either set is in.  They both have \(5\) elements.%
\par
Such a match\textendash{}up is called a one\textendash{}to\textendash{}one correspondence.  In general, if two sets can be put in one\textendash{}to\textendash{}one correspondence then they are the same ``size.'' Of course the word ``size'' has lots of connotations that will begin to get in the way when we talk about infinite sets, so instead we will say that the two sets have the same \emph{cardinality.} Speaking loosely, this just means that they are the same size.%
\par
More precisely, if a given set \(S\) can be put in one\textendash{}to\textendash{}-one correspondence with a finite set of consecutive integers, say \(\left\{1,2,3,\ldots, N\right\}\), then we say that the cardinality of the set is \(N\).  But this just means that both sets have the same cardinality.  It is this notion of one\textendash{}to\textendash{}one correspondence, along with the next two definitions, which will allow us to compare the sizes (cardinalities) of infinite sets.%
\begin{definition}{Definition}{}{def_CountableSets}%
\index{cardinality!countable sets}%
\index{countable sets!defintion of}%
Any set which can be put into one\textendash{}to\textendash{}one correspondence with \(\NN=\left\{1,2,3,\ldots\right\}\) is called a \terminology{countably infinite} set.  Any set which is either finite or countably infinite is said to be \terminology{countable}.%
\end{definition}
\index{Cantor, Georg} Since \(\NN\) is an infinite set, we have no symbol to designate its cardinality so we have to invent one.  The symbol used by Cantor and adopted by mathematicians ever since is \(\aleph_0\).  Thus the cardinality of any countably infinite set is \(\aleph_0\).%
\begin{aside}{Aside}{Notation.}{BackToFourier-InfiniteSets-16}%
\(\aleph{}\) is the first letter of the Hebrew alphabet and is pronounced ``aleph.'' \(\aleph_0\) is pronounced ``aleph null.''%
\end{aside}
We have already given the following definition informally.  We include it formally here for later reference.%
\begin{definition}{Definition}{}{def_EqualCardinality}%
\index{cardinality!equal cardinality}%
If two sets can be put into one\textendash{}to\textendash{}one correspondence then they are said to have the \alert{same cardinality}.%
\end{definition}
With these two definitions in place we can see that \hyperref[thm_NoSeriesContainsAllReals]{Theorem~{\xreffont\ref{thm_NoSeriesContainsAllReals}}} is nothing less than the statement that the real numbers are not countably infinite.  Since it is certainly not finite, then we say that the set of real numbers is uncountable and therefore ``bigger'' than the natural numbers!%
\par
To see this let us suppose first that each real number appears in the sequence \((s_n)_{n=1}^\infty\) exactly once.  In that case the indexing of our sequence is really just a one\textendash{}to\textendash{}one correspondence between the elements of the sequence and \(\NN:\)%
\begin{align*}
1\amp \leftrightarrow s_1\\
2\amp \leftrightarrow s_2\\
3\amp \leftrightarrow s_3\\
4\amp \leftrightarrow s_4\\
\amp \ \ \ \vdots
\end{align*}
%
\par
If some real numbers are repeated in our sequence then all of the real numbers are a subset of our sequence and will therefore also be countable (see \hyperref[prob_countable_sets_unions_of]{Problem~{\xreffont\ref{prob_countable_sets_unions_of}}}, part a).%
\par
In either case, every sequence is countable.  But our theorem says that \emph{no} sequence in \(\RR\) includes all of \(\RR\).  Therefore \(\RR\) is uncountable.%
\par
Most of the sets you have encountered so far in your life have been countable.%
\begin{problem}{Problem}{}{BackToFourier-InfiniteSets-24}%
Show that each of the following sets is countable.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(\left\{2,3,4,5,\ldots\right\}=\left\{n\right\}_{n=2}^\infty\)%
\item{}\(\left\{0,1,2,3,\ldots\right\}=\left\{n\right\}_{n=0}^\infty\)%
\item{}\(\left\{1,4,9,16,\ldots,n^2,\ldots\right\}=\left\{n^2\right\}_{n=1}^\infty\)%
\item{}The set of prime numbers%
\item{}\(\ZZ\)%
\end{enumerate}%
\end{problem}
In fact, if we start with a countable set it is rather difficult to use it to build anything but another countable set.%
\begin{problem}{Problem}{}{prob_countable_sets_unions_of}%
Let \(\left\{A_1, A_2, A_3, \ldots \right\}\) be a collection of countable sets.  Show that each of the following sets is also countable:%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Any subset of \(A_1\).%
\item{}\(A_1\cup A_2\)%
\item{}\(A_1\cup A_2 \cup A_3\)%
\item{}\(\displaystyle\bigcup_{i=1}^nA_i\)%
\item{}\(\displaystyle\bigcup_{i=1}^\infty A_i\)%
\end{enumerate}%
\end{problem}
It seems that no matter what we do the only example of an uncountably infinite set is \(\RR\).  But wait!  Remember the rational numbers?  They were similar to the real numbers in many ways.  Perhaps they are uncountably infinite too?%
\par
Alas, no.  The rational numbers turn out to be countable too.%
\begin{theorem}{Theorem}{}{}{thm_QisCountable}%
\index{\(\QQ\)!is countable}%
\(\QQ\) is countable.%
\end{theorem}
\begin{proof}{Proof}{Sketch of Proof.}{BackToFourier-InfiniteSets-30}
First explain how you know that all of the non\textendash{}negative rational numbers are in this list:%
\begin{equation*}
\frac{0}{1},\frac{0}{2},\frac{1}{1},\frac{0}{3},
\frac{1}{2},\frac{2}{1},\frac{0}{4},\frac{1}{3}, \frac{2}{2},
\frac{3}{1}, \cdots \text{.}
\end{equation*}
%
\par
However there is clearly some duplication.  To handle this, apply part (a) of \hyperref[prob_countable_sets_unions_of]{Problem~{\xreffont\ref{prob_countable_sets_unions_of}}}.  Does this complete the proof or is there more to do?%
\end{proof}
\begin{problem}{Problem}{}{BackToFourier-InfiniteSets-31}%
Prove \hyperref[thm_QisCountable]{Theorem~{\xreffont\ref{thm_QisCountable}}}.%
\end{problem}
\index{Leibniz, Gottfried Wilhelm} Notice how this idea hearkens back to the discussion of Leibniz'  approach to the Product Rule (\hyperref[eq_LeibnizProductRule]{equation~({\xreffont\ref{eq_LeibnizProductRule}})}).  He simply tossed aside the expression \(\dx{ x}\dx{ y}\) because it was \emph{infinitely small} compared to either \(x\dx{ y}\) or \(y\dx{ x}\).  Although this isn't quite the same thing we are discussing here it is similar and it is clear that Leibniz' insight and intuition were extremely acute.  They were moving him in the right direction, at least.%
\par
All of our efforts to build an uncountable set from a countable one have come to nothing.  In fact many sets that at first ``feel'' like they should be uncountable are in fact countable. This makes the uncountability of \(\RR\) all the more remarkable.%
\begin{aside}{Aside}{Comment.}{BackToFourier-InfiniteSets-34}%
The failure is in the methods we've used so far.  It is possible to build an uncountable set using just two symbols if we're clever enough, but this would take us too far away from our main topic.%
\end{aside}
However if we \emph{start} with an uncountable set it is relatively easy to build others from it.%
\begin{problem}{Problem}{}{BackToFourier-InfiniteSets-36}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Let \((a,b)\) and \((c,d)\) be two open intervals of real numbers.  Show that these two sets have the same cardinality by constructing a one\textendash{}to\textendash{}one onto function between them.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{BackToFourier-InfiniteSets-36-2-2}{}\quad{}A linear function should do the trick.%
\item{}Show that any open interval of real numbers has the same cardinality as \(\RR\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{BackToFourier-InfiniteSets-36-3-2}{}\quad{}Consider the interval \((-\pi/2,\pi/2)\).%
\item{}Show that \((0,1]\) and \((0,1)\) have the same cardinality.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{BackToFourier-InfiniteSets-36-4-2}{}\quad{}Note that \(\left\{1,1/2,1/3,\ldots\right\}\) and \(\left\{1/2, 1/3, \ldots\right\}\) have the same cardinality.%
\item{}Show that \([0,1]\) and \((0,1)\) have the same cardinality.%
\end{enumerate}%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 12.3 Lebesgue Measure and Integration}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Lebesgue Measure and Integration}{}{Lebesgue Measure and Integration}{}{}{SECTIONLebesgMeas}
Recall that the Dirichlet Function%
\begin{equation*}
f(x)=
\begin{cases}
0, \amp \text{ if }  x \text{ is rational}\\
1, \amp \text{ if }  x \text{ is irrational}
\end{cases}
\end{equation*}
is not Darboux (Riemann) integrable.  The existence of non\textendash{}integrable functions lead mathematicians to generalize the idea of an integral.%
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Stieltjes/}{Thomas Jan Stieltjes}\protect\footnotemark{} (1856-1894)}{FIGUREStieltjes}{}%
\begin{image}{0.35}{0.3}{0.35}{}%
\includegraphics[width=\linewidth]{external/images/Stieltjes.jpg}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[35]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Stieltjes/}\label{FIGUREStieltjes-1-2}}%
Of note is the work of Thomas Jan Stieltjes who was working on a problem called the moment problem, that is, the problem of determining the mass distribution of an object given that the moments of all orders of a body are given.  This was published in his work \textit{Recherches sur les fractions continues} (Research on Continued Fractions) in 1894.  If you are acquainted with probability theory, the first moment of a probability density function is the expected value, the second moment (about the mean) is the variance, etc.  The problem is to reconstruct the probability density function from these moments.  As intimidating as the problem sounds, you are acquainted with the idea of a Stieltjes integral, but not by this name.  Consider for example,%
\begin{equation*}
\int^1_{x=0}{\sin\left(x^2\right)2x\dx{x}}
\end{equation*}
In trying to compute this integral, you can quickly recognize that this is the integral%
\begin{equation*}
\int^1_{x=0}{\sin(x^2)} \dx(x^2)
\end{equation*}
We are writing the integral this way to emphasize the differential \(\dx{(x^2)}\). In the Riemann integral%
\begin{equation*}
\int^1_{u=0}{{\mathrm{sin}
\left(u\right)\ }\ du}
\end{equation*}
the differential \(\dx{u}\) represents the fact that the interval is being partitioned into subintervals whose lengths are being measured.  The differential \(d\left(x^2\right)=2xdx\) represents the idea that points closer to one carry more weight or are denser than those closer to zero:%
\begin{align*}
\mineval{2x\dx{x}}{x=0}\amp{}=0\dx{x}\\
\amp{}=\mineval{2x\dx{x}}{x=\frac12}\\
\amp{}=1 \dx{x}\\
\amp{}=\mineval{2x\dx{x}}{x=1}\\
\amp{}=2 \dx{x}.
\end{align*}
This says that the ``lengths'' of the subintervals in our partition are being measured in different ways.  In general, given two functions \(f\left(x\right),\ g(x)\), the Stieltjes integral is defined by%
\begin{equation*}
\int^b_{x=a}{f\left(x\right)dg}
\end{equation*}
%
\par
You can see that the Riemann integral is just the Stieltjes integral with \(g\left(x\right)=x\).  We won't go further with the Stieltjes integral except to say that it is useful in probability and physics in places where \(f\) has jumps. With the proper density function, these discontinuities can be minimized.  It is also generalizable to functional analysis. It also sets the stage for what we will focus on, which is to find a way to integrate a function such as the Dirichlet function.  The trick is to not focus on the function but on the interval on which it is integrated and how to measure the subsets in a partition.  This generalization comes from \href{https://mathshistory.st-andrews.ac.uk/Biographies/Lebesgue/}{Henri Lebesgue}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Lebesgue/}\label{SECTIONLebesgMeas-5-4}} (1875-1941) in 1901.%
\begin{figureptx}{Figure}{Henri Lebesgue}{FIGURELebesgue}{}%
\begin{image}{0.3}{0.4}{0.3}{}%
\includegraphics[width=\linewidth]{external/images/Lebesgue.png}
\end{image}%
\tcblower
\end{figureptx}%
Given Cantor's results that the set of rational numbers is countable while the set of irrational numbers is uncountable, Lebesgue thought that the value of \(f\left(x\right)\) on a relatively \textasciigrave{}\textasciigrave{}small'{}'{} set such as the rational numbers should have no bearing on the value of an integral and he focused more on measuring the \textasciigrave{}\textasciigrave{}sizes'{}'{} of sets to take that into account.  The basis of this idea comes from the following.%
\begin{theorem}{Theorem}{}{}{THEOREMMeasureZero}%
Let \(S=\{s_1,\ s_2,\ s_3,\ \dots \}\) be countably infinite set of real numbers.  Let \(\eps >0\).  There is a collection of open intervals \((a_n,\ b_n)\) with%
\begin{equation*}
S\subset \bigcup^{\infty }_{n=1}{\left(a_n,\ b_n\right)}
\end{equation*}
and%
\begin{equation*}
\sum^{\infty }_{n=1}{\left(b_n-a_n\right)}\lt\eps 
\end{equation*}
%
\end{theorem}
\begin{problem}{Problem}{}{SECTIONLebesgMeas-9}%
Prove Theorem \hyperref[THEOREMMeasureZero]{Theorem~{\xreffont\ref{THEOREMMeasureZero}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{SECTIONLebesgMeas-9-2}{}\quad{}Consider the interval \(\left(s_n-\frac{\eps  }{2^{n+1}},\ s_n+\frac{\eps }{2^{n+1}}\right)\).]%
\end{problem}
The collection of intervals \((a_n,b_n)\) with \(S\subset
\bigcup^{\infty }_{n=1}{\left(a_n,\ b_n\right)}\) is called an open cover of \(S\).  Lebesgue defined the (outer) measure of a set to be the infimum of the sums \(\sum^{\infty
}_{n=1}{\left(b_n-a_n\right)}\) over all such open covers. In symbols, the (outer) measure of a set \(S\) \(\left(\lambda \left(S\right)\right)\) is given by%
\begin{equation*}
\lambda(S)=\inf_{S\subset \bigcup^{\infty}_{n=1}{(a_n,\ b_n)}} \left(\sum^{\infty }_{n=1}{\left(b_n-a_n\right)}\right) 
\end{equation*}
%
\par
Given the result of \hyperref[THEOREMMeasureZero]{Theorem~{\xreffont\ref{THEOREMMeasureZero}}}, any countable set would have Lebesgue measure zero.  Since the rational numbers are countably infinite, then they have measure zero.  We won't get into the details of a Lebesgue integral, but it should not be surprising that if two (Lebesgue integrable) functions, \(f\) and \(g\), differ from each other on set of measure zero, then they should have the same value for their Lebesgue integrals.  Since the Dirichlet function differs from the constantly one function on a set of measure zero (namely \(\mathbb{Q}\)), then it should be Lebesgue integrable (as a constant function should be) and its integral would be%
\begin{equation*}
\int^b_{x=a}{f\left(x\right)d\lambda =\left(b-a\right)}
\end{equation*}
%
\par
You may have noticed that we used the term outer measure before. For a measurable set, its measure is this outer measure.  However, there are examples of non\textendash{}measurable sets.  Any set would have an outer measure, but assigning this as the measure of one of these non\textendash{}measurable sets results in a number of contradictions on how a measure should behave.%
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Vitali/}{Giuseppe Vitali}\protect\footnotemark{} (1875-1932)}{FIGUREGiuseppeVitali}{}%
\begin{image}{0.3}{0.4}{0.3}{}%
\includegraphics[width=\linewidth]{external/images/Vitali.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[37]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Vitali/}\label{FIGUREGiuseppeVitali-1-2}}%
The first non\textendash{}measurable set was described by Giuseppe Vitali  in 1905.  Creating such a set requires the use of something called the Axiom of Choice and delves deeply into a careful study of the theory of sets.  We will see how complicated sets can be in the next section.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 12.4 Cantor's Theorem and Its Consequences}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Cantor's Theorem and Its Consequences}{}{Cantor's Theorem and Its Consequences}{}{}{BackToFourier-CantorsTheorem}
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Dedekind/}{Richard Dedekind}\protect\footnotemark{}}{BackToFourier-CantorsTheorem-2}{}%
\index{Dedekind, Richard!portrait of}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Dedekind.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[38]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Dedekind/}\label{BackToFourier-CantorsTheorem-2-1-2}}%
After employing his ideas on infinite sets of real numbers to study trigonometric series, Cantor gravitated toward applying his ideas to sets in general.  For example, Once Cantor showed that there were two types of infinity (countable and uncountable), the following question was natural, ``Do all uncountable sets have the same cardinality?''%
\par
Just like not all ``non\textendash{}dogs'' are cats, there is, offhand, no reason to believe that all uncountable sets should be the same size.  However constructing uncountable sets of different sizes is not as easy as it sounds.%
\par
\index{Dedekind, Richard}\index{Cantor, Georg!unit interval and unit square have equal cardinalty} For example, what about the line segment represented by the interval \([0,1]\) and the square represented by the set \([0,1]\times[0,1]=\left\{(x,y)|0\leq x,y\leq 1\right\}\). Certainly the two dimensional square must be a larger infinite set than the one dimensional line segment.  Remarkably, Cantor showed that these two sets were the same cardinality.  In his 1877 correspondence of this result to his friend and fellow mathematician, Richard Dedekind, even Cantor remarked, ``I see it, but I don't believe it!''%
\par
The following gives the original idea of Cantor's proof.  Cantor devised the following function \(f:[0,1]\times[0,1]\rightarrow
[0,1]\).  First, we represent the coordinates of any point \((x,y)\in [0,1]\times[0,1]\) by their decimal representations \(x=0.a_1 a_2 a_3\ldots\) and \(y=0.b_1 b_2
b_3\ldots\).  Even terminating decimals can be written this way as we could write \(0.5=0.5000\ldots\).  We can then define \(f(x,y)\) by%
\begin{equation*}
f((0.a_1 a_2 a_3\ldots ,0.b_1 b_2
b_3\ldots))=0.a_1 b_1 a_2 b_2 a_3 b_3\ldots \text{.}
\end{equation*}
%
\par
This relatively simple idea has some technical difficulties in it related to the following result.%
\begin{problem}{Problem}{}{BackToFourier-CantorsTheorem-8}%
Consider the sequence \((0.9,0.99,0.999,\ldots)\). Determine that this sequence converges and, in fact, it converges to \(1\).  This suggests that \(0.999\ldots=1\).%
\end{problem}
Similarly, we have \(0.04999\ldots=0.05000\ldots\), etc.  To make the decimal representation of a real number in \([0,1]\) unique, we must make a consistent choice of writing a terminating decimal as one that ends in an infinite string of zeros or an infinite string of nines [with the one exception \(0=0.000\ldots\) ]. No matter which choice we make, we could never make this function onto.  For example, \(109/1100=0.09909090\ldots\) would have as its pre-image \((0.0999\ldots,0.9000\ldots)\) which would be a mix of the two conventions.%
\par
Cantor was able to overcome this technicality to demonstrate a one to one correspondence, but instead we will note that in either convention, the function is one\textendash{}to\textendash{}one, so this says that the set \([0,1]\times[0,1]\) is the same cardinality as some (uncountable) subset of \(\RR\).  The fact that this has the same cardinality as \(\RR\) is something we will come back to.  But first we'll try construct an uncountable set which does not have the same cardinality as \(\RR\).  To address this issue, Cantor proved the following in 1891.%
\begin{theorem}{Theorem}{Cantor's Theorem.}{}{thm_CantorsTheorem}%
\index{Cantor, Georg!Cantor's Theorem}%
Let \(S\) be any set.  Then there is no one\textendash{}to\textendash{}one correspondence between \(S\) and \(P(S)\), the set of all subsets of \(S\).%
\end{theorem}
Since \(S\) can be put into one\textendash{}to\textendash{}one correspondence with a subset of \(P(S)\) (\(a\rightarrow \left\{a\right\}\)), then this says that \(P(S)\) is at least as large as S. In the finite case \(\abs{P(S)}\) is strictly greater than \(\abs{S}\) as the following problem shows.  It also demonstrates why \(P(S)\) is called the power set of \(S\).%
\begin{problem}{Problem}{}{prob_PowerSet}%
Prove: If \(\abs{S}=n\), then \(\abs{ P(S)}=2^n\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{prob_PowerSet-5}{}\quad{}Let \(S=\left\{a_1,a_2,\ldots,a_n\right\}\).  Consider the following correspondence between the elements of \(P(S)\) and the set \(T\) of all \(n\)-tuples of yes (Y) or no (N):%
\begin{align*}
\{ \}  \amp \leftrightarrow \{N,N,N,\ldots,N\}\\
\{a_1\}\amp \leftrightarrow \{Y,N,N,\ldots ,N\}\\
\{a_2\}\amp \leftrightarrow \{N,Y,N,\ldots,N\}\\
\amp \vdots\\
S\amp \leftrightarrow \{Y,Y,Y,\ldots,Y\}
\end{align*}
%
\par
How many elements are in \(T?\)%
\end{problem}
\begin{problem}{Problem}{}{BackToFourier-CantorsTheorem-14}%
Remarkably, Cantor's Theorem holds for infinite sets as well.%
\par
Prove \hyperref[thm_CantorsTheorem]{Cantor's Theorem}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{BackToFourier-CantorsTheorem-14-3}{}\quad{}Assume for contradiction, that there is a one\textendash{}to\textendash{}one correspondence \(f:S\rightarrow P(S)\).  Consider \(A=\left\{x\in S|x\not\in f(x)\right\}\).  Since \(f\) is onto, then there is \(a\in A\) such that \(A=f(a)\).  Is \(a\in A\) or is \(a\not\in A?\)%
\end{problem}
Actually it turns out that \(\RR\) and \(P(\NN)\) have the same cardinality.  This can be seen in a roundabout way using some of the above ideas from \hyperref[prob_PowerSet]{Problem~{\xreffont\ref{prob_PowerSet}}}.  Specifically, let \(T\) be the set of all sequences of zeros or ones (you can use \(Y\)s or \(N\)s, if you prefer).  Then it is straightforward to see that \(T\) and \(P(\NN)\) have the same cardinality.%
\par
If we consider \((0,1]\), which has the same cardinality as \(\RR\), then we can see that this has the same cardinality as \(T\) as well.  Specifically, if we think of the numbers in binary, then every real number in \([0,1]\) can be written as \(\sum_{j=1}^\infty \frac{a_j}{2^j} =0.a_1a_2a_3\ldots\) where \(a_j\in\left\{0,1\right\}\).  We have to account for the fact that binary representations such as \(0.0111\ldots\) and \(0.1000\ldots\) represent the same real number (say that no representations will end in an infinite string of zeros), then we can see that \([0,1]\) has the same cardinality as \(T-U\), where \(U\) is the set of all sequences ending in an infinite string of zeros.  It turns out that \(U\) itself is a countable set.%
\begin{problem}{Problem}{}{BackToFourier-CantorsTheorem-17}%
\index{countable sets!countable union of finite sets} Let%
\begin{equation*}
U_n=\left\{(a_1,a_2,a_3,\ldots)\ |\ a_j\in \left\{0,1\right\} \text{
and } a_{n+1}=a_{n+2}=\cdots=0\right\}\text{.}
\end{equation*}
Show that for each \(n\), \(U_n\) is finite and use this to conclude that \(U\) is countably infinite.%
\end{problem}
The above problems say that \(\RR\), \(T-U\), \(T\), and \(P(N)\) all have the same cardinality.  The following two problems show that deleting a countable set from an uncountable set does not change its cardinality.%
\begin{problem}{Problem}{}{BackToFourier-CantorsTheorem-19}%
\index{sets!countably infinite subsets} Let \(S\) be an infinite set.  Prove that \(S\) contains a countably infinite subset.%
\end{problem}
\begin{problem}{Problem}{}{BackToFourier-CantorsTheorem-20}%
Suppose \(X\) is an uncountable set and \(Y\subset X\) is countably infinite.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Prove that \(X\) and \(X-Y\) are both uncountable sets.%
\item{}Prove that \(X\) and \(X-Y\) have the same cardinality.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{BackToFourier-CantorsTheorem-20-4-2}{}\quad{}Let \(Y=Y_0\).  If \(X-Y_0\) is an infinite set, then by the previous problem it contains a countably infinite set \(Y_1\).  Likewise if \(X-(Y_0\cup
Y_1)\) is infinite it also contains an infinite set \(Y_2\).  Again, if \(X-(Y_0\cup Y_1\cup Y_2)\) is an infinite set then it contains an infinite set \(Y_3\), etc.  For \(n=1, 2, 3,\ldots \), let \(f_n:Y_{n-1}\rightarrow Y_n\) be a one\textendash{}to\textendash{}one correspondence and define \(f:X\rightarrow X-Y\) by%
\begin{equation*}
f(x) =
\begin{cases}
f_n(x), \amp \text{ if }  x\in Y_n, n=0,1,2,\ldots\\
x, \amp \text{ if }  x\in X-(\cup_{n=0}^\infty Y_n ) 
\end{cases}\text{.}
\end{equation*}
%
\par
Show that \(f\) is one\textendash{}to\textendash{}one and onto.%
\end{enumerate}%
\end{problem}
As we indicated before, Cantor's \index{Cantor, Georg} work on infinite sets had a profound impact on mathematics in the beginning of the twentieth century.  For example, in examining the proof of Cantor's Theorem, the eminent logician Bertrand Russell devised his famous paradox in 1901.  Before this time, a set was naively thought of as just a collection of objects.  Through the work of Cantor and others, sets were becoming a central object of study in mathematics as many mathematical concepts were being reformulated in terms of sets. The idea was that set theory was to be a unifying theme of mathematics.  This paradox set the mathematical world on its ear.%
\par
\index{Russell's Paradox} \terminology{Russell's Paradox:} Consider the set of all sets which are not elements of themselves.  We call this set \(D\) and ask, ``Is \(D\in D?\)'' Symbolically, this set is%
\begin{equation*}
D=\{S\ |\ S\not \in S\} \text{.}
\end{equation*}
%
\par
If \(D\in D\), then by definition, \(D\not\in D\).  If \(D\not\in D\), then by definition, \(D\in D\).%
\par
If you look back at the proof of Cantor's Theorem, this was basically the idea that gave us the contradiction.  To have such a contradiction occurring at the most basic level of mathematics was scandalous.  It forced a number of mathematicians and logicians to carefully devise the axioms by which sets could be constructed.  To be honest, most mathematicians still approach set theory from a naive point of view as the sets we typically deal with fall under the category of what we would call ``normal sets.'' In fact, such an approach is officially called Naive Set Theory (as opposed to Axiomatic Set Theory). However, attempts to put set theory and logic on solid footing led to the modern study of symbolic logic and ultimately the design of computer (machine) logic.%
\par
Another place where Cantor's work had a profound influence in modern logic comes from something we alluded to before.  We showed before that the unit square \([0,1]\times [0,1]\) had the same cardinality as an uncountable subset of \(\RR\).  In fact, Cantor \index{Cantor, Georg} showed that the unit square had the same cardinality as \(\RR\) itself and was moved to advance the following in 1878.%
\begin{conjecture}{Conjecture}{The Continuum Hypothesis.}{}{BackToFourier-CantorsTheorem-26}%
\index{Continuum Hypothesis!original}%
Every uncountable subset of \(\RR\) has the same cardinality as \(\RR\).%
\end{conjecture}
Cantor was unable to prove or disprove this conjecture (along with every other mathematician).  In fact, proving or disproving the Continuum Hypothesis, was one of Hilbert's famous 23 problems presented as a challenge to mathematicians at the International Congress of Mathematicians in 1900.%
\par
Since \(\RR\) has the same cardinality as \(P(\NN)\), then the Continuum Hypothesis was generalized to the:%
\begin{conjecture}{Conjecture}{The Generalized Continuum Hypothesis.}{}{BackToFourier-CantorsTheorem-29}%
\index{Continuum Hypothesis!generalized}%
Given an infinite set \(S\), there is no infinite set which has a cardinality strictly between that of \(S\) and its power set \(P(S)\).%
\end{conjecture}
\begin{aside}{Aside}{The Zermelo-Fraenkel Axioms.}{BackToFourier-CantorsTheorem-30}%
One of the formal axiomatic approaches to set theory established by Ernst Zermelo in 1908 and revised by Abraham Fraenkel in 1921.%
\end{aside}
Efforts to prove or disprove this were in vain and with good reason.  In 1940, the logician Kurt Gdel showed that the Continuum Hypothesis could not be disproved from the Zermelo-Fraenkel Axioms of set theory.  In 1963, Paul Cohen showed that the Continuum Hypothesis could not be proved using the Zermelo-Fraenkel Axioms.  In other words, the Zermelo-Fraenkel Axioms do not contain enough information to decide the truth of the hypothesis.%
\par
We are willing to bet that at this point your head might be swimming a bit with uncertainty.  If so, then know that these are the same feelings that the mathematical community experienced in the mid twentieth century.  In the past, mathematics was seen as a model of logical certainty.  It is disconcerting to find that there are statements that are ``undecidable.'' In fact, Gdel proved in 1931 that a consistent finite axiom system that contained the axioms of arithmetic would always contain undecidable statements which could neither be proved true nor false with those axioms. Mathematical knowledge would always be incomplete.%
\begin{figureptx}{Figure}{\href{https://mathshistory.st-andrews.ac.uk/Biographies/Godel/}{Kurt Gdel}\protect\footnotemark{}}{BackToFourier-CantorsTheorem-33}{}%
\index{Gdel, Kurt!portrait of}%
\begin{image}{0.325}{0.35}{0.325}{}%
\includegraphics[width=\linewidth]{external/images/Godel.png}
\end{image}%
\tcblower
\end{figureptx}%
\footnotetext[39]{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Godel/}\label{BackToFourier-CantorsTheorem-33-1-2}}%
So by trying to put the foundations of Calculus on solid ground, we have come to a point where we can never obtain mathematical certainty.  Does this mean that we should throw up our hands and concede defeat?  Should we be paralyzed with fear of trying anything?  Certainly not!  As we mentioned before, most mathematicians do well by taking a pragmatic approach: using their mathematics to solve problems that they encounter.  In fact, it is typically the problems that motivate the mathematics.  It is true that mathematicians take chances that don't always pan out, but they still take these chances, often with success.  Even when the successes lead to more questions, as they typically do, tackling those questions usually leads to a deeper understanding.  At the very least, our incomplete understanding means we will always have more questions to answer, more problems to solve.%
\begin{aside}{Aside}{Note from Bob.}{BackToFourier-CantorsTheorem-35}%
I'm not sure how, but it sounds appropriate to mention in the following passage that by essentially ignoring the complexities of axiomatic set theory (but knowing they are there), we are in much the same boat as mathematicians of the 18th century who exploited the infinitely small with great success and a large number of questions to be answered.  That being said, I like the way that this ends the book.%
\par
Any thoughts?%
\end{aside}
What else could a mathematician ask for?%
\end{sectionptx}
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 13 Epilogues}
\typeout{************************************************}
%
\begin{chapterptx}{Chapter}{Epilogues}{}{Epilogues}{}{}{Epilogues}
\renewcommand*{\chaptername}{Chapter}
%
%
\typeout{************************************************}
\typeout{Section 13.1 On the Nature of Numbers: A Dialogue (with Apologies to Galileo)}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{On the Nature of Numbers: A Dialogue (with Apologies to Galileo)}{}{On the Nature of Numbers: A Dialogue (with Apologies to Galileo)}{}{}{Epilogues-NatureNumbers}
\begin{introduction}{}%
\alert{Interlocuters}:\emph{Salviati, Sagredo, and Simplicio; Three Friends of Galileo Galilei}%
\par
\alert{Setting}: Three friends meet in a garden for lunch in Renaissance Italy.  Prior to their meal they discuss the book \emph{How We Got From There to Here: A Story of Real Analysis.} How they obtained a copy is not clear.%
\par
\alert{Salviati}: My good sirs. I have read this very strange volume as I hope you have?%
\par
\alert{Sagredo}: I have and I also found it very strange.%
\par
\alert{Simplicio}: Very strange indeed; at once silly and mystifying.%
\par
\alert{Salviati}: Silly? How so?%
\par
\alert{Simplicio}: These authors begin their tome with the question, ``What is a number?'' This is an unusually silly question, don't you think? Numbers are numbers. Everyone knows what they are.%
\par
\alert{Sagredo}: I thought so as well until I reached the last chapter. But now I am not so certain. What about this quantity \(\aleph_0?\) If this counts the positive integers, isn't it a number? If not, then how can it count anything? If so, then what number is it? These questions plague me 'til I scarcely believe I know anything anymore.%
\par
\alert{Simplicio}: Of course \(\aleph_0\) is not a number! It is simply a new name for the infinite, and infinity is not a number.%
\par
\alert{Sagredo}: But isn't \(\aleph_0\) the cardinality of the set of natural numbers, \(\NN\), in just the same way that the cardinality of the set \(S=\left\{Salviati, Sagredo, Simplicio\right\}\) is \(3?\) If \(3\) is a number, then why isn't \(\aleph_0?\)%
\par
\alert{Simplicio}: Ah, my friend, like our authors you are simply playing with words. You count the elements in the set \(S=\left\{Salviati, Sagredo, Simplicio\right\};\) you see plainly that the \emph{number of elements} it contains is \(3\) and then you change your language. Rather than saying that the \emph{number of elements} in \(S\) is \(3\) you say that the \emph{cardinality} is \(3\). But clearly ``cardinality'' and ``number of elements'' mean the same thing.%
\par
Similarly you use the symbol \(\NN\) to denote the set of positive integers. With your new word and symbol you make the statement ``the \emph{cardinality} (number of elements) of \(\NN\) is \(\aleph_0\).'' This statement has the same grammatical form as the statement ``the \emph{number of elements} (cardinality) of \(S\) is three.'' Since three \emph{is} a number you conclude that \(\aleph_0\) is also a number.%
\par
But this is simply nonsense dressed up to sound sensible. If we unwind our notation and language, your statement is simply, ``The number of positive integers is infinite.'' This is obviously nonsense because infinity is \emph{not} a number.%
\par
Even if we take infinity as an undefined term and try to define it by your statement this is still nonsense since you are using the word ``number'' to define a new ``number'' called infinity. This definition is circular. Thus it is no definition at all. It is nonsense.%
\par
\alert{Salviati}: Your reasoning on this certainly seems sound.%
\par
\alert{Simplicio}: Thank you.%
\par
\alert{Salviati}:  However, there are a couple of small points I would like to examine more closely if you will indulge me?%
\par
\alert{Simplicio}: Of course. What troubles you?%
\par
\alert{Salviati}: You've said that we cannot use the word ``number'' to define numbers because this would be circular reasoning. I entirely agree, but I am not sure this is what our authors are doing.%
\par
Consider the set \(\left\{1, 2, 3\right\}\). Do you agree that it contains three elements?%
\par
\alert{Simplicio}: Obviously.%
\par
\alert{Sagredo}: Ah!  I see your point!  That there are three elements does \emph{not} depend on what those elements are.  Any set with three elements has three elements regardless of the nature of the elements.  Thus saying that the set \(\left\{1, 2, 3\right\}\) contains three elements does not define the word ``number'' in a circular manner because it is irrelevant that the \emph{number} 3 is one of the elements of the set.  Thus to say that three is the cardinality of the set \(\left\{1, 2, 3\right\}\) has the same meaning as saying that there are three elements in the set \(\left\{Salviati, Sagredo, Simplicio\right\}\).  In both cases the number ``\(3\)'' is the name that we give to the totality of the elements of each set.%
\par
\alert{Salviati}:  Precisely. In exactly the same way \(\aleph_0\) is the symbol we use to denote the totality of the set of positive integers.%
\par
Thus \(\aleph_0\) is a number in the same sense that '\(3\)' is a number, is it not?%
\par
\alert{Simplicio}:  I see that we can say in a meaningful way that three is the cardinality of any set with~.~.~.~well,~.~.~.~with three elements (it becomes very difficult to talk about these things) but this is simply a tautology! It is a way of saying that a set which has three elements has three elements!%
\par
This means only that we have counted them and we had to stop at three. In order to do this we must have numbers first. Which, of course, we do. As I said, everyone knows what numbers are.%
\par
\alert{Sagredo}:  I must confess, my friend, that I become more confused as we speak. I am no longer certain that I really know what a number is. Since you seem to have retained your certainty can you clear this up for me? Can you tell me what a number is?%
\par
\alert{Simplicio}:  Certainly. A number is what we have just been discussing. It is what you have when you stop counting. For example, three is the totality (to use your phrase) of the elements of the sets \(\left\{Salviati, Sagredo, Simplicio\right\}\) or \(\left\{1, 2, 3\right\}\) because when I count the elements in either set I have to stop at three. Nothing less, nothing more. Thus three is a number.%
\par
\alert{Salviati}:  But this definition only confuses me! Surely you will allow that fractions are numbers? What is counted when we end with, say \(4/5\) or \(1/5?\)%
\par
\alert{Simplicio}:  This is simplicity itself. \(4/5\) is the number we get when we have divided something into \(5\) equal pieces and we have counted four of these fifths. This is four-fifths. You see? Even the language we use naturally bends itself to our purpose.%
\par
\alert{Salviati}:  But what of one-fifth? In order to count one fifth we must first divide something into fifths. To do this we must know what one-fifth is, musn't we? We seem to be using the word ``number'' to define itself again. Have we not come full circle and gotten nowhere?%
\par
\alert{Simplicio}:  I confess this had not occurred to me before. But your objection is easily answered. To count one-fifth we simply divide our ``something'' into tenths. Then we count two of them. Since two-tenths is the same as one-fifth the problem is solved. Do you see?%
\par
\alert{Sagredo}:  I see your point but it will not suffice at all! It merely replaces the question, ``What is one-fifth?'' with, ``What is one-tenth?'' Nor will it do to say that one-tenth is merely two-twentieths. This simply shifts the question back another level.%
\par
Archimedes said, ``Give me a place to stand and a lever long enough and I will move the earth.'' But of course he never moved the earth because he had nowhere to stand. We seem to find ourselves in Archimedes' predicament: We have no place to stand.%
\par
\alert{Simplicio}:  I confess I don't see a way to answer this right now. However I'm sure an answer can be found if we only think hard enough. In the meantime I cannot accept that \(\aleph_0\) is a number. It is, as I said before, infinity and infinity is not a number! We may as well believe in fairies and leprechauns if we call infinity a number.%
\par
\alert{Sagredo}:  But again we've come full circle. We cannot say definitively that \(\aleph_0\) is or is not a number until we can state with confidence what a number is. And even if we could find solid ground on which to solve the problem of fractions, what of \(\sqrt{2}?\) Or \(\pi?\) Certainly these are numbers but I see no way to count to either of them.%
\par
\alert{Simplicio}:  Alas! I am beset by demons! I am bewitched! I no longer believe what I \emph{know} to be true!%
\par
\alert{Salviati}:  Perhaps things are not quite as bad as that. Let us consider further. You said earlier that we all know what numbers are, and I agree. But perhaps your statement needs to be more precisely formulated. Suppose we say instead that we all know what numbers \emph{need} to be? Or that we know what we \emph{want} numbers to be?%
\par
Even if we cannot say with certainly what numbers \emph{are} surely we can say what we want and need for them to be. Do you agree?%
\par
\alert{Sagredo}:  I do.%
\par
\alert{Simplicio}:  And so do I.%
\par
\alert{Salviati}:  Then let us invent numbers anew, as if we've never seen them before, always keeping in mind those properties we need for numbers to have. If we take this as a starting point then the question we need to address is, ``What do we need numbers to be?''%
\par
\alert{Sagredo}:  This is obvious! We need to be able to add them and we need to be able to multiply them together, and the result should also be a number.%
\par
\alert{Simplicio}:  And subtract and divide too, of course.%
\par
\alert{Sagredo}:  I am not so sure we actually need these. Could we not \emph{define} ``subtract two from three'' to be ``add negative two to three'' and thus dispense with subtraction and division?%
\par
\alert{Simplicio}:  I suppose we can but I see no advantage in doing so. Why not simply have subtraction and division as we've always known them?%
\par
\alert{Sagredo}:  The advantage is parsimony. Two arithmetic operations are easier to keep track of than four. I suggest we go forward with only addition and multiplication for now. If we find we need subtraction or division we can consider them later.%
\par
\alert{Simplicio}:  Agreed. And I now see another advantage. Obviously addition and multiplication must not depend on order. That is, if \(x\) and \(y\) are numbers then \(x+y\) must be equal to \(y+x\) and \(xy\) must be equal to \(yx\). This is not true for subtraction, for \(3-2\) does \emph{not} equal \(2-3\). But if we define subtraction as you suggest then this symmetry is preserved:%
\begin{equation*}
x+(-y) = (-y)+x\text{.}
\end{equation*}
%
\par
\alert{Sagredo}:  Excellent! Another property we will require of numbers occurs to me now. When adding or multiplying more than two numbers it should not matter where we begin. That is, if \(x\), \(y\) and \(z\) are numbers it should be true that%
\begin{equation*}
(x+y)+z = x+(y+z)
\end{equation*}
and%
\begin{equation*}
(x\cdot y)\cdot z = x\cdot(y\cdot z)\text{.}
\end{equation*}
%
\par
\alert{Simplicio}:  Yes! We have it! Any objects which combine in these precise ways can be called numbers.%
\par
\alert{Salviati}:  Certainly these properties are necessary, but I don't think they are yet sufficient to our purpose. For example, the number \(1\) is unique in that it is the only number which, when multiplying another number leaves it unchanged.%
\par
For example:  \(1\cdot3=3\). Or, in general, if \(x\) is a number then \(1\cdot x =x\).%
\par
\alert{Sagredo}:  Yes. Indeed. It occurs to me that the number zero plays a similar role for addition: \(0+x=x\).%
\par
\alert{Salviati}:  It does not seem to me that addition and multiplication, as we have defined them, force \(1\) or \(0\) into existence so I believe we will have to postulate their existence independently.%
\par
\alert{Sagredo}:  Is this everything then? Is this all we require of numbers?%
\par
\alert{Simplicio}:  I don't think we are quite done yet. How shall we get division?%
\par
\alert{Sagredo}:  In the same way that we defined subtraction to be the addition of a negative number, can we not define division to be multiplication by a reciprocal? For example, \(3\) divided by \(2\) can be considered \(3\) \emph{multiplied} by \(1/2\), can it not?%
\par
\alert{Salviati}:  I think it can. But observe that \emph{every} number will need to have a corresponding negative so that we can subtract any amount. And again nothing we've discussed so far forces these negative numbers into existence so we will have to postulate their existence separately.%
\par
\alert{Simplicio}:  And in the same way every number will need a reciprocal so that we can divide by any amount.%
\par
\alert{Sagredo}:  Every number that is, except zero.%
\par
\alert{Simplicio}:  Yes, this is true. Strange is it not, that of them all only this one number needs no reciprocal? Shall we also postulate that zero has no reciprocal?%
\par
\alert{Salviati}: I don't see why we should. Possibly \(\aleph_0\) is the reciprocal of zero. Or possibly not. But I see no need to concern ourselves with things we do not need.%
\par
\alert{Simplicio}: Is this everything then? Have we discovered all that we need for numbers to be?%
\par
\alert{Salviati}: I believe there is only one property missing. We have postulated addition and we have postulated multiplication and we have described the numbers zero and one which play similar roles for addition and multiplication respectively. But we have not described how addition and multiplication work together.%
\par
That is, we need a rule of distribution:  If \(x\), \(y\) and \(z\) are all numbers then%
\begin{equation*}
x\cdot(y+z) = x\cdot y+x\cdot z\text{.}
\end{equation*}
%
\par
With this in place I believe we have everything we need.%
\par
\alert{Simplicio}:  Indeed. We can also see from this that \(\aleph_0\) cannot be a number since, in the first place, it cannot be added to another number and in the second, even if it could be added to a number the result is surely not also a number.%
\par
\alert{Salviati}:  My dear Simplicio, I fear you have missed the point entirely! Our axioms do not declare what a number is, only how it behaves with respect to addition and multiplication with other numbers. Thus it is a mistake to presume that ``numbers'' are only those objects that we have always believed them to be. In fact, it now occurs to me that ``addition'' and ``multiplication'' also needn't be seen as the operations we have always believed them to be.%
\par
For example suppose we have three objects, \(\left\{a, b, c\right\}\) and suppose that we define ``addition'' and ``multiplication'' by the following tables:%
\begin{align*}
\begin{array}{c|ccc} 
+\amp a\amp b\amp c\\\hline 
a\amp a\amp b\amp c\\
b\amp b\amp c\amp a\\ 
c\amp c\amp a\amp b 
\end{array}  
\amp \amp \amp \amp \amp \amp 
\begin{array}{c|ccc} 
\cdot\amp a\amp b\amp c\\\hline 
a\amp a\amp a\amp a\\ 
b\amp a\amp b\amp c\\ 
c\amp a\amp c\amp b 
\end{array} 
\end{align*}
%
\par
I submit that our set along with these definitions satisfy all of our axioms and thus \(a\), \(b\) and \(c\) qualify to be called ``numbers.''%
\par
\alert{Simplicio}:  This cannot be! There is no zero, no one!%
\par
\alert{Sagredo}:  But there is. Do you not see that \(a\) plays the role of zero \textemdash{} if you add it to any number you get that number back. Similarly \(b\) plays the role of one.%
\par
\alert{Simplicio}: This is astonishing! If \(a, b\) and \(c\) can be numbers then I am less sure than ever that I know what numbers are! Why, if we replace \(a, b\), and \(c\) with \emph{Simplicio} \emph{Sagredo,} and \emph{Salviati} then \emph{we} become numbers ourselves!%
\par
\alert{Salviati}:  Perhaps we will have to be content with knowing how numbers behave rather than knowing what they are.%
\par
However I confess that I have a certain affection for the numbers I grew up with. Let us call those the ``real'' numbers. Any other set of numbers, such as our \(\left\{a,b,c\right\}\) above we will call a field of numbers, since they seem to provide us with new ground to explore. Or perhaps just a \emph{number field}?%
\par
As we have been discussing this I have been writing down our axioms.  They are these.  Numbers are any objects which satisfy all of the following properties:%
\par
\alert{AXIOMS OF NUMBERS}%
\par
%
\begin{descriptionlist}
\begin{dlimedium}{Axiom I: Definition of Operations}{Epilogues-NatureNumbers-2-78-1-1}%
They can be combined by two operations, denoted ``\(\cdot\)'' and ``+''.%
\end{dlimedium}%
\begin{dlimedium}{Axiom II: Closure}{Epilogues-NatureNumbers-2-78-1-2}%
If \(x\), \(y\) and \(z\) are numbers then \(x+y\) is also a number. \(x\cdot y\) is also a number.%
\end{dlimedium}%
\begin{dlimedium}{Axiom II: Commutativity}{Epilogues-NatureNumbers-2-78-1-3}%
\(x+y=y+x\) \(x\cdot y=y\cdot x\)%
\end{dlimedium}%
\begin{dlimedium}{Axiom III: Associativity}{Epilogues-NatureNumbers-2-78-1-4}%
\((x+y)+z=x+(y+z)\) \((x\cdot y)\cdot z = x\cdot(y\cdot z)\)%
\end{dlimedium}%
\begin{dlimedium}{Axiom IV: Additive Identity}{Epilogues-NatureNumbers-2-78-1-5}%
There is a number, denoted \(0\), such that for any number, \(x\),%
\begin{equation*}
x+0=x\text{.}
\end{equation*}
%
\end{dlimedium}%
\begin{dlimedium}{Axiom V: Multiplicative Identity}{Epilogues-NatureNumbers-2-78-1-6}%
There is a number, denoted \(1\), such that for any number, \(x\),%
\begin{equation*}
1\cdot x=x\text{.}
\end{equation*}
%
\end{dlimedium}%
\begin{dlimedium}{Axiom VI: Additive Inverses}{Epilogues-NatureNumbers-2-78-1-7}%
Given any number, \(x\), there is a number, denoted \(-x\), with the property that%
\begin{equation*}
x+(-x)=0\text{.}
\end{equation*}
%
\end{dlimedium}%
\begin{dlimedium}{Axiom VII: Multiplicative Inverse}{Epilogues-NatureNumbers-2-78-1-8}%
Given any number, \(x\ne0\), there is a number, denoted \(x^{-1}\), with the property that%
\begin{equation*}
x\cdot x^{-1} =1\text{.}
\end{equation*}
%
\end{dlimedium}%
\begin{dlimedium}{Axiom VIII: The Distributive Property}{Epilogues-NatureNumbers-2-78-1-9}%
If \(x\), \(y\) and \(z\) are numbers then%
\begin{equation*}
x\cdot(y+z) = x\cdot y+x\cdot z\text{.}
\end{equation*}
%
\end{dlimedium}%
\end{descriptionlist}
%
\par
\alert{Sagredo}: My friend, this is a thing of surpassing beauty! All seems clear to me now. Numbers are any group of objects which satisfy our axioms. That is, a number is anything that \emph{acts} like a number.%
\par
\alert{Salviati}:  Yes this seems to be true.%
\par
\alert{Simplicio}:  But wait! We have not settled the question: Is \(\aleph_0\) a number or not?%
\par
\alert{Salviati}:  If everything we have just done is valid then \(\aleph_0\) \emph{could be} a number. And so could \(\aleph_1\),\(\aleph_2 \ldots\) \emph{if} we can find a way to define addition and multiplication on the set \(\left\{\aleph_0, \aleph_1, \aleph_2 \ldots\right\}\) in a manner that agrees with our axioms.%
\par
\alert{Sagredo}:  An arithmetic of infinities! This is a very strange idea. Can such a thing be made sensible?%
\par
\alert{Simplicio}:  Not, I think, before lunch. Shall we retire to our meal?%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  Additional Problems}
\typeout{************************************************}
%
\begin{subsectionptx}{Subsection}{Additional Problems}{}{Additional Problems}{}{}{Epilogues-NatureNumbers-3}
\begin{problem}{Problem}{}{Epilogues-NatureNumbers-3-2}%
Show that \(0\neq 1\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{Epilogues-NatureNumbers-3-2-3}{}\quad{}Show that if \(x\neq0\), then \(0\cdot x \neq x\).%
\end{problem}
\begin{problem}{Problem}{}{Epilogues-NatureNumbers-3-3}%
Consider the set of ordered pairs of integers: \(\left\{(x,y)|x, y \in \ZZ\right\}\), and define addition and multiplication as follows:%
 \par
%
\begin{itemize}[label=\textbullet]
\item{}\lititle{Addition:.}\par%
\(\displaystyle (a,b)+(c,d) = (ad+bc, bd)\)%
\item{}\lititle{Multiplication:.}\par%
\((a,b)\cdot(c,d) = (ac, bd)\).%
\end{itemize}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}If we add the convention that%
\begin{equation*}
(ab, ad) = (b,d)
\end{equation*}
show that this set with these operations forms a number field.%
\item{}Which number field is this?%
\end{enumerate}%
\end{problem}
\begin{problem}{Problem}{}{Epilogues-NatureNumbers-3-4}%
Consider the set of ordered pairs of real numbers, \(\left\{(x,y)|x, y\in\RR\right\}\), and define addition and multiplication as follows:%
 \par
%
\begin{itemize}[label=\textbullet]
\item{}\lititle{Addition:.}\par%
\(\displaystyle (a,b)+(c,d) = (a+c, b+d)\)%
\item{}\lititle{Multiplication:.}\par%
\((a,b)\cdot(c,d) = (ac- bd,ad+bc)\).%
\end{itemize}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Show that this set with these operations forms a number field.%
\item{}Which number field is this?%
\end{enumerate}%
\end{problem}
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 13.2 Building the Real Numbers}
\typeout{************************************************}
%
\begin{sectionptx}{Section}{Building the Real Numbers}{}{Building the Real Numbers}{}{}{Epilogues-BuildRealNumbers}
\begin{introduction}{}%
Contrary to the title of this section we will not be rigorously building the real numbers here.  Instead our goal is to show why such a build is logically necessary, and to give a sense of some of the ways this has been accomplished in the past.  This may seem odd given our uniform emphasis on mathematical rigor, especially in the third part of the text, but there are very good reasons for this.%
\par
One is simple practicality.  The fact is that rigorously building the real numbers and then showing that they have the required properties is extraordinarily detailed work, even for mathematics.  If we want to keep this text to a manageable size (we do), we simply don't have the room.%
\par
The second reason is that there is, as far as we know, very little for you to gain by it.  When we are done we will have the real numbers.  The same real numbers you have been using all of your life.  They have the same properties, and quirks, they've always had.  To be sure, they will not have lost any of their charm. They will be the same delightful mix of the mundane and the bizarre, and they are still well worth exploring and getting to know better.  But nothing we do in the course of building them up logically from simpler ideas will help with that exploration.%
\par
A reasonable question then, is, ``Why bother?'' If the process is overwhelmingly, tediously detailed (it is) and gives us nothing new for our efforts, why do it at all?%
\begin{aside}{Aside}{Andrew Wiles.}{Epilogues-BuildRealNumbers-2-5}%
The man who proved \href{https://mathshistory.st-andrews.ac.uk/HistTopics/Fermat's_last_theorem/}{Fermat's Last Theorem}\footnotemark{}.%
\end{aside}
\footnotetext[40]{\nolinkurl{mathshistory.st-andrews.ac.uk/HistTopics/Fermat's_last_theorem/}\label{Epilogues-BuildRealNumbers-2-5-2-2}}%
\href{https://mathshistory.st-andrews.ac.uk/Biographies/Wiles/}{Andrew Wiles}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Wiles/}\label{Epilogues-BuildRealNumbers-2-6-2}} has compared doing mathematics has been compared to entering a dark room. At first you are lost.  The layout of the room and furniture are unknown so you fumble about for a bit and slowly get a sense of your immediate environs, perhaps a vague notion of the organization of the room as a whole.  Eventually, after much, often tedious exploration, you become quite comfortable in your room.  But always there will be dark corners; hidden areas you have not yet explored.  Such a dark area may hide anything; the latches to unopened doors you didn't know were there; a clamp whose presence explains why you couldn't move that little desk in the corner; even the light switch that would allow you to illuminate an area more clearly than you would have imagined possible.%
\par
But, and this is the point, there is no way to know what you will find there until you walk into that dark corner and begin exploring.  Perhaps nothing.  But perhaps something wonderful.%
\par
This is what happened in the late nineteenth century.  The real numbers had been used since the Pythagoreans learned that \(\sqrt{2}\) was irrational.  But really, most calculuations were (and still are) done with just the rational numbers. Moreover, since \(\QQ\) forms a ``set of measure zero,'' \index{measure zero} it is clear that most of the real numbers had gone completely unused. The set of real numbers was thus one of those ``dark corners'' of mathematics. It had to be explored.%
\begin{aside}{Aside}{Measure Zero Sets.}{Epilogues-BuildRealNumbers-2-9}%
See \hyperref[THEOREMMeasureZero]{Theorem~{\xreffont\ref{THEOREMMeasureZero}}} of \hyperref[BackToFourier]{Chapter~{\xreffont\ref{BackToFourier}}}.%
\end{aside}
``But even if that is true,'' you might ask, ``I have no interest in the logical foundations of the real numbers, especially if such knowledge won't tell me anything I don't already know.  Why do I need to know all of the details of constructing \(\RR\) from \(\QQ?\)''%
\par
The answer to this is very simple: You don't.%
\par
That's the other reason we're not covering all of the details of this material.  We will explain enough to light up, dimly perhaps, this little corner of mathematics. Later, should you need (or want) to come back to this and explore further you will have a foundation to start with. Nothing more.%
\par
Until the nineteenth century the geometry of Euclid, as given in his book \emph{The Elements,} was universally regarded as the touchstone of mathematical perfection.  This belief was so deeply embedded in Western culture that as recently as 1923, Edna St.~Vincent Millay opened one of the poems in her book \emph{The Harp Weaver and Other Poems} with the line ``Euclid alone has looked on beauty bare.''%
\par
Euclid begins his book by stating \(5\) simple axioms and proceeds, step by logical step, to build up his geometry.  Although far from actual perfection, his methods are clean, precise and efficient \textemdash{} he arrives at the Pythagorean Theorem in only \(47\) steps (theorems) \textemdash{} and even today Euclid's \emph{Elements} still sets a very high standard of mathematical exposition and parsimony.%
\par
The goal of starting with what is clear and simple and proceeding logically, rigorously, to what is complex is still a guiding principle of all mathematics for a variety of reasons.  In the late nineteenth century, this principle was brought to bear on the real numbers.  That is, some properties of the real numbers that at first seem simple and intuitively clear turn out on closer examination, as we have seen, to be rather counter-intuitive.  This alone is not really a problem.  We can have counter-intuitive properties in our mathematics \textemdash{} indeed, this is a big part of what makes mathematics interesting \textemdash{} as long as we arrive at them logically, starting from simple assumptions the same way Euclid did.%
\par
Having arrived at a view of the real numbers which is comparable to that of our nineteenth century colleagues, it should now be clear that the real numbers and their properties must be built up from simpler concepts as suggested by our Italian friends in the previous section.%
\par
In addition to those properties we have discovered so far, both \(\QQ\) and \(\RR\) share another property which will be useful. We have used it throughout this text but have not heretofore made it explicit. They are both \emph{linearly ordered.} We will now make this property explicit.%
\begin{definition}{Definition}{Linear Ordering.}{def_NumberField}%
\index{number field!linearly ordered}%
A number field is said to be \emph{linearly ordered} if there is a relation, denoted ``\(\lt \)'', on the elements of the field which satisfies all of the following for all \(x, y\), and \(z\) in the field.%
\par
%
\begin{enumerate}
\item{}For all numbers \(x\) and \(y\) in the field, exactly one of the following holds:%
\par
%
\begin{enumerate}
\item{}\(\displaystyle x\lt y\)%
\item{}\(\displaystyle x=y\)%
\item{}\(\displaystyle y\lt x\)%
\end{enumerate}
%
\item{}If \(x\lt y\), then \(x+z\lt y+z\) for all \(z\) in the field.%
\item{}If \(x\lt y\), and \(0\lt z\), then \(x\cdot z \lt y\cdot z\).%
\item{}If \(x\lt y\) and \(y\lt z\) then \(x\lt z\).%
\end{enumerate}
%
\end{definition}
Any number field with such a relation is called a \terminology{linearly ordered number field} and as the following problem shows, not every number field is linearly ordered.%
\begin{problem}{Problem}{}{Epilogues-BuildRealNumbers-2-20}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}Prove that the following must hold in \emph{any} linearly ordered number field.%
\par
%
\begin{enumerate}
\item{}\(0\lt x\) if and only if \(-x\lt 0\).%
\item{}If \(x\lt y\) and \(z\lt 0\) then \(y\cdot z\lt x\cdot z\).%
\item{}For all \(x\neq 0\), \(0\lt x^2\).%
\item{}\(0\lt 1\).%
\end{enumerate}
%
\item{}Show that the set of complex numbers (\(\CC\)) is not a linearly ordered field.%
\end{enumerate}%
\end{problem}
In a thorough, rigorous presentation we would now assume the existence of the natural numbers \((\NN)\), and their properties and use these to define the integers, \((\ZZ)\).  We would then use the integers to define the rational numbers, \((\QQ)\).  We could then show that the rationals satisfy the field axioms worked out in the previous section, and that they are linearly ordered.%
\par
Then \textemdash{} at last \textemdash{} we would use \(\QQ\) to define the real numbers \((\RR)\), show that these also satisfy the field axioms and also have the other properties we expect: Continuity, the \hyperref[NIP]{Nested Interval Property} , the Least Upper Bound Property, the Bolzano-Weierstrass Theorem, the convergence of all Cauchy sequences, and linear ordering.%
\par
We would start with the natural numbers because they seem to be simple enough that we can simply assume their properties. As \href{https://mathshistory.st-andrews.ac.uk/Biographies/Kronecker/}{Leopold Kronecker}\footnote{\nolinkurl{mathshistory.st-andrews.ac.uk/Biographies/Kronecker/}\label{Epilogues-BuildRealNumbers-2-23-2}} (1823-1891) said: ``God made the natural numbers, all else is the work of man.''%
\par
Unfortunately this is rather a lot to fit into this epilogue so we will have to abbreviate the process rather severely.%
\par
We will assume the existence and properties of the rational numbers.  Building \(\QQ\) from the integers is not especially hard and it is easy to show that they satisfy the axioms worked out by Salviati, Sagredo and Simplicio in the previous section.  But the level of detail required for rigor quickly becomes onerous.%
\par
Even starting at this fairly advanced position in the chain of logic there is still a considerable level of detail needed to complete the process.  Therefore our exposition will necessarily be incomplete.%
\par
Rather than display, in full rigor, how the real numbers can be built up from the rationals we will show, in fairly broad terms, three ways this has been done in the past.  We will give references later in case you'd like to follow up and learn more.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection  The Decimal Expansion}
\typeout{************************************************}
%
\begin{subsectionptx}{Subsection}{The Decimal Expansion}{}{The Decimal Expansion}{}{}{DecimalExpansion}
This is by far the most straightforward method we will examine.  Since we begin with \(\QQ\), we already have some numbers whose decimal expansion is infinite.  For example, \(\frac13= 0.333\ldots\).  We also know that if \(x\in\QQ\) then expressing \(x\) as a decimal gives either a finite or a \emph{repeating infinite} decimal.%
\par
More simply, we can say that \(\QQ\) consists of the set of all decimal expressions which eventually repeat.  (If it eventually repeats zeros then it is what we've called a finite decimal.)%
\par
We then define the real numbers to be the set of all infinite decimals, repeating or not.%
\par
It may feel as if all we have to do is define addition and multiplication in the obvious fashion and we are finished. This set with these definitions obviously satisfy all of the field axioms worked out by our Italian friends in the previous section.  Moreover it seems clear that all of our equivalent completeness axioms are satisfied.%
\par
However, things are not quite as clear cut as they seem.%
\par
The primary difficulty in this approach is that the decimal representation of the real numbers is so familiar that everything we need to show \emph{seems} obvious.  But stop and think for a moment.  Is it really obvious how to define addition and multiplication of \emph{infinite} decimals? Consider the addition algorithm we were all taught in grade school.  That algorithm requires that we line up two numbers at their decimal points:%
\begin{align*}
d_1d_2\amp .d_3d_4\\
+\   \delta_1\delta_2\amp .\delta_3\delta_4\text{.}
\end{align*}
%
\par
We then begin adding in the rightmost column and proceed to the left.  \emph{But if our decimals are infinite we can't get started because there is no rightmost column!}%
\par
A similar problem occurs with multiplication.%
\par
So our first problem is to define addition and multiplication in \(\RR\) in a manner that re-captures addition and multiplication in \(\QQ\).%
\par
This is not a trivial task.%
\par
One way to proceed is to recognize that the decimal notation we've used all of our lives is really shorthand for the sum of an infinite series.  That is, if \(x=0.d_1d_2d_3\ldots\) where \(0\leq d_i\leq 9\) for all \(i\in\NN\) then%
\begin{equation*}
x=\sum_{i=1}^\infty\frac{d_i}{10^i}\text{.}
\end{equation*}
%
\par
Addition is now apparently easy to define: If \(x=\sum_{i=1}^\infty\frac{d_i}{10^i}\) and \(y=\sum_{i=1}^\infty\frac{\delta_i}{10^i}\) then%
\begin{equation*}
x+y= \sum_{i=1}^\infty\frac{e_i}{10^i} \text{ where \(e_i =d_i+\delta_i\). }
\end{equation*}
%
\par
But there is a problem.  Suppose for some \(j\in\NN\), \(e_j=d_j+\delta_j>10\).  In that case our sum does not satisfy the condition \(0\leq e_j\leq 9\) so it is not even clear that the expression \(\sum_{j=1}^\infty\frac{e_j}{10^j}\) represents a real number.  That is, we may not have the closure property of a number field.  We will have to define some sort of ``carrying'' operation to handle this.%
\begin{problem}{Problem}{}{DecimalExpansion-15}%
Define addition on infinite decimals in a manner that is closed.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{DecimalExpansion-15-3}{}\quad{}Find an appropriate ``carry'' operation for our definition.%
\end{problem}
A similar difficulty arises when we try to define multiplication.  Once we have a notion of carrying in place, we could define multiplication as just the multiplication of series.  Specifically, we could define%
\begin{align*}
(0.a_1a_2a_3\ldots)\cdot(0.b_1b_2b_3\ldots) \amp =\left(\frac{a_1}{10}+\frac{a_2}{10^2}+\cdots\right) \cdot\left(\frac{b_1}{10}+\frac{b_2}{10^2}+\cdots\right)\\
\amp =\frac{a_1b_1}{10^2}+\frac{a_1b_2+a_2b_1}{10^3}+\frac{a_1b_3+a_2b_2+a_3b_1}{10^4}+\cdots\text{.}
\end{align*}
%
\par
We could then convert this to a ``proper'' decimal using our carrying operation.%
\par
Again the devil is in the details to show that such algebraic operations satisfy everything we want them to. Even then, we need to worry about linearly ordering these numbers and our completeness axiom.%
\par
Another way of looking at this is to think of an infinite decimal representation as a (Cauchy) sequence of finite decimal approximations.  Since we know how to add and multiply finite decimal representations, we can just add and multiply the individual terms in the sequences.  Of course, there is no reason to restrict ourselves to only these specific types of Cauchy sequences, as we see in our next approach.%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Cauchy Sequences}
\typeout{************************************************}
%
\begin{subsectionptx}{Subsection}{Cauchy Sequences}{}{Cauchy Sequences}{}{}{CauchySequences}
\index{Cantor, Georg} As we've seen, Georg Cantor began his career studying Fourier series and quickly moved on to more foundational matters in the theory of infinite sets.%
\par
But he did not lose his fascination with real analysis when he moved on.  Like many mathematicians of his time, he realized the need to build \(\RR\) from \(\QQ\).  He and his friend and mentor Richard Dedekind \index{Dedekind, Richard} (who's approach we will see in the next section) both found different ways to build \(\RR\) from \(\QQ\).%
\par
Cantor started with Cauchy sequences in \(\QQ\).%
\par
That is, we consider the set of all Cauchy sequences of rational numbers.  We would like to \emph{define} each such sequence to be a real number.  The goal should be clear.  If \(\left(s_n\right)_{n=1}^\infty\) is a sequence in \(\QQ\) which converges to \(\sqrt{2}\) then we will call \(\left(s_n\right)\) the real number \(\sqrt{2}\).%
\par
This probably seems a bit startling at first.  There are a \emph{lot} of numbers in \(\left(s_n\right)\) (countably infinitely many, to be precise) and we are proposing putting all of them into a big bag, tying it up in a ribbon, and calling the whole thing \(\sqrt{2}\).  It seems a very odd thing to propose, but recall from the discussion in the previous section that we left the concept of ``number'' undefined.  Thus if we can take \emph{any} set of objects and define addition and multiplication in such a way that the field axioms are satisfied, then those objects are legitimately numbers.  To show that they are, in fact, the \emph{real} numbers we will also need the completeness property.%
\par
A bag full of rational numbers works as well as anything \emph{if} we can define addition and multiplication appropriately.%
\par
Our immediate problem though is not addition or multiplication but uniqueness.  If we take one sequence \(\left(s_n\right)\) which converges to \(\sqrt{2}\) and define it to be \(\sqrt{2}\), what will we do with all of the other sequences that converge to \(\sqrt{2}?\)%
\par
Also, we have to be careful not to refer to any real numbers, like the square root of two for example, as we define the real numbers.  This would be a circular \textemdash{} and thus useless \textemdash{} definition.  Obviously though, we can refer to \emph{rational} numbers, since these are the tools we'll be using.%
\par
The solution is clear.  We take \emph{all} sequences of rational numbers that converge to \(\sqrt{2}\), throw them into our bag and call \emph{that} \(\sqrt{2}\). Our bag is getting pretty full now.%
\par
But we need to do this without using \(\sqrt{2}\) because it is a real number.  The following two definitions satisfy all of our needs.%
\begin{definition}{Definition}{}{def_EquivCauchySeq}%
\index{sequences!Cauchy sequences!equivalent}%
Let \(x=\left(s_n\right)_{n=1}^\infty\) and \(y=\left(\sigma_n\right)_{n=1}^\infty\) be Cauchy sequences in \(\QQ\).  \(x\) and \(y\) are said to be equivalent if they satisfy the following property: For every \(\eps>0\), \(\eps\in\QQ\), there is a rational number \(N\) such that for all \(n>N\), \(n\in\NN\),%
\begin{equation*}
\abs{s_n-\sigma_n}\lt \eps\text{.}
\end{equation*}
%
\par
We will denote equivalence by writing, \(x\equiv y\).%
\end{definition}
\begin{problem}{Problem}{}{prob_EquivalentCauchySequences}%
Show that:%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(x\equiv x\)%
\item{}\(x\equiv y \imp y\equiv x\)%
\item{}\(x\equiv y\) and \(y\equiv z \imp x\equiv z\)%
\end{enumerate}%
\end{problem}
\begin{definition}{Definition}{}{def_RealsViaCauchy}%
\index{sequences!Cauchy sequences!real numbers as Cauchy sequences}%
Every set of all equivalent Cauchy sequences defines a real number.%
\end{definition}
A very nice feature of Cantor's method is that it is very clear how addition and multiplication should be defined.%
\begin{definition}{Definition}{}{def_AddTimesViaCauchy}%
\index{sequences!Cauchy sequences!addition and multiplication of} If%
\begin{equation*}
x= \left\{\left.\left(s_n\right)_{k=1}^\infty \right| \left(s_n\right)_{k=1}^\infty \text{ is Cauchy in \(\QQ\) } \right\}
\end{equation*}
and%
\begin{equation*}
y= \left\{\left.\left(\sigma_n\right)_{k=1}^\infty \right| \left(\sigma_n\right)_{k=1}^\infty \text{ is Cauchy in \(\QQ\) } \right\}
\end{equation*}
then we define the following:%
\begin{itemize}[label=\textbullet]
\item{}\lititle{Addition:.}\par%
\(\displaystyle x+y = \left\{\left.\left(t_n\right)_{k=1}^\infty \right| t_k = s_k+\sigma_k, \forall (s_n) \in x, \text{ and } (\sigma_n)\in y\right\}\)%
\item{}\lititle{Multiplication:.}\par%
\(\displaystyle x\cdot y = \left\{\left.\left(t_n\right)_{k=1}^\infty \right| t_k = s_k\sigma_k, \forall (s_n) \in x, \text{ and } (\sigma_n)\in y\right\}\)%
\end{itemize}
%
\end{definition}
The notation used in \hyperref[def_RealsViaCauchy]{Definition~{\xreffont\ref{def_RealsViaCauchy}}} can be difficult to read at first, but basically it says that addition and multiplication are done component-wise.  However since \(x\) and \(y\) consist of all equivalent sequences we have to take \emph{every} possible choice of \((s_n)\in
x\) and \((\sigma_n)\in y\), form the sum (product) \((s_n+\sigma_n)_{n=1}^\infty\) (\((s_n\sigma_n)_{n=1}^\infty\)) and then show that all such sums (products) are equivalent.  Otherwise addition (multiplication) is not well-defined: It would depend on which sequence we choose to represent \(x\) and \(y\).%
\begin{problem}{Problem}{}{prob_CauchyAdditionWellDefined}%
Let \(x\) and \(y\) be real numbers (that is, let them be sets of equivalent Cauchy sequences  in \(\QQ\)). If \((s_n)\) and \((t_n)\) are in \(x\) and \((\sigma_n)\) and \((\tau_n)\) are in \(y\) then%
\begin{equation*}
(s_n+\tau_n)_{n=1}^\infty \equiv (\sigma_n+t_n)_{n=1}^\infty\text{.}
\end{equation*}
%
\end{problem}
\begin{theorem}{Theorem}{}{}{thm_ZeroInCauchySequences}%
\index{sequences!Cauchy sequences!zero as a Cauchy sequence}%
Let \(0^*\) be the set of Cauchy sequences in \(\QQ\) which are all equivalent to the sequence \((0, 0, 0, \ldots)\).  Then%
\begin{equation*}
0^*+x=x\text{.}
\end{equation*}
%
\end{theorem}
\begin{proof}{Proof}{}{CauchySequences-20}
From \hyperref[prob_CauchyAdditionWellDefined]{Problem~{\xreffont\ref{prob_CauchyAdditionWellDefined}}} it is clear that in forming \(0^*+x\) we can choose any sequence in \(0^*\) to represent \(0^*\) and any sequence in \(x\) to represent \(x\).  (This is because any other choice will yield a sequence equivalent to \(0^*+x\).)%
\par
Thus we choose \((0, 0, 0, \ldots)\) to represent \(0^*\) and any element of \(x\), say \((x_1, x_2,
\ldots)\), to represent \(x\).  Then%
\begin{align*}
(0, 0, 0, \ldots) + (x_1, x_2, x_3, \ldots) \amp = (x_1, x_2, x_3, \ldots)\\
\amp = x\text{.}
\end{align*}
%
\par
Since any other sequences taken from \(0^*\) and \(x\) respectively, will yield a sum equivalent to \(x\) (see \hyperref[prob_EquivalentCauchySequences]{Problem~{\xreffont\ref{prob_EquivalentCauchySequences}}}) we conclude that%
\begin{equation*}
0^*+x=x\text{.}
\end{equation*}
%
\end{proof}
\begin{problem}{Problem}{}{CauchySequences-21}%
Identify the set of equivalent Cauchy sequences, \(1^*\), such that%
\begin{equation*}
1^*\cdot x=x\text{.}
\end{equation*}
%
\end{problem}
\begin{problem}{Problem}{}{CauchySequences-22}%
Let \(x, y\), and \(z\) be real numbers (equivalent sets of Cauchy sequences). Show that with addition and multiplication defined as above we have:%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]%
\item{}\(x+y=y+x\)%
\item{}\((x+y)+z=x+(y+z)\)%
\item{}\(x\cdot y=y\cdot x\)%
\item{}\((x\cdot y)\cdot z=x\cdot (y\cdot z)\)%
\item{}\(x\cdot(y+z)=x\cdot y+x\cdot z\)%
\end{enumerate}%
\end{problem}
\begin{aside}{Aside}{Comment.}{CauchySequences-23}%
We will not address this issue here, but you should give some thought to how this might be accomplished.%
\end{aside}
Once the existence of additive and multiplicative inverses is established the collection of all sets of equivalent Cauchy sequences, with addition and multiplication defined as above satisfy all of the field axioms.  It is clear that they form a number field and thus deserve to be called numbers.%
\par
However this does not necessarily show that they form \(\RR\).  We also need to show that they are complete in the sense of \hyperref[IVTandEVT]{Chapter~{\xreffont\ref{IVTandEVT}}}.  It is perhaps not too surprising that when we build the real numbers using equivalent Cauchy sequences the most natural completeness property we can show is that if a sequence of real numbers is Cauchy then it converges.%
\par
However we are not in a position to show that Cauchy sequences in \(\RR\) converge.  To do this we would first need to show that these sets of equivalence classes of Cauchy sequences (real numbers) are linearly ordered.%
\par
Unfortunately showing the linear ordering, while not especially hard, is time consuming.  So we will again invoke the prerogatives of the teacher and brush all of the difficulties aside with the assertion that it is straightforward to show that the real numbers as we have constructed them in this section are linearly ordered and are complete.  If you would like to see this construction in full rigor we recommend the book, \emph{The Number System} by H. A. Thurston\hyperlink{thurston56__number_system}{[{\xreffont 16}]}.%
\begin{aside}{Aside}{Comment.}{CauchySequences-28}%
Thurston first builds \(\RR\) as we've indicated in this section.  Then as a final remark he shows that the real numbers must be exactly the infinite decimals we saw in the previous section.%
\end{aside}
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection  Dedekind Cuts}
\typeout{************************************************}
%
\begin{subsectionptx}{Subsection}{Dedekind Cuts}{}{Dedekind Cuts}{}{}{DedekindCuts}
An advantage of building the reals via Cauchy sequences in the previous section is that once we've identified equivalent sequences with real numbers it is \emph{very} clear how addition and multiplication should be defined.%
\par
On the other hand, before we can even start to understand that construction, we need a fairly strong sense of what it means for a sequence to converge and enough experience with sequences to be comfortable with the notion of a Cauchy sequence.  Thus a good deal of high level mathematics must be mastered before we can even begin.%
\par
\index{Dedekind, Richard!Dedekind cuts} The method of ``Dedekind cuts'' first developed by Richard Dedekind (though he just called them ``cuts'') in his 1872 book, \emph{Continuity and the Irrational Numbers} shares the advantage of the Cauchy sequence method in that, once the candidates for the real numbers have been identified, it is very clear how addition and multiplication should be defined.  It is also straightforward to show that most of the field axioms are satisfied.%
\begin{aside}{Aside}{Comment.}{DedekindCuts-5}%
``Clear'' does not mean ``easy to do'' as we will see.%
\end{aside}
\index{Dedekind, Richard} In addition, Dedekind's method also has the advantage that very little mathematical knowledge is required to get started.  This is intentional.  In the preface to the first edition of his book, Dedekind states:%
\begin{quote}%
This memoir can be understood by anyone possessing what is usually called common sense; no technical philosophic, or mathematical, knowledge is in the least degree required. (quoted in~\hyperlink{hawking05__god_creat_integ}{[{\xreffont 5}]})%
\end{quote}
While he may have overstated his case a bit, it is clear that his intention was to argue from very simple first principles just as Euclid did.%
\par
His starting point was the observation we made in \hyperref[NumbersRealRational]{Chapter~{\xreffont\ref{NumbersRealRational}}}: The rational number line is full of holes.  More precisely we can ``cut'' the rational line in two distinct ways:%
\begin{enumerate}
\item{}We can pick a rational number, \(r\).  This choice divides all other rational numbers into two classes: Those greater than \(r\) and those less than \(r\).%
\item{}We can pick one of the holes in the rational number line.  In this case \emph{all} of the rationals fall into two classes: Those greater than the hole and those less.%
\end{enumerate}
%
\par
But to speak of rational numbers as less than or greater than something that is \emph{not} there is utter nonsense. We'll need a better (that is, a rigorous) definition.%
\par
As before we will develop an overall sense of this construction rather than a fully detailed presentation, as the latter would be far too long to include.%
\par
Our presentation will closely follow that of Edmund Landau's in his classic 1951 text \emph{Foundations of Analysis}~\hyperlink{landau66__found_analy}{[{\xreffont 7}]}. We do this so that if you choose to pursue this construction in more detail you will be able to follow Landau's presentation more easily.%
\begin{definition}{Definition}{The Dedekind Cut.}{def_dedekind-cuts}%
\index{Dedekind, Richard!Dedekind cuts!definition of}%
\index{Dedekind, Richard!Dedekind cuts}%
A set of \emph{positive} rational numbers is called a \emph{cut} if%
\begin{aside}{Aside}{Comment.}{def_dedekind-cuts-4-2}%
Take special notice that we are not using the negative rational numbers or zero to build our cuts.  The reason for this will become clear shortly.%
\end{aside}
%
\begin{descriptionlist}
\begin{dlimedium}{Property I}{def_dedekind-cuts-4-3-1-1}%
It contains a positive rational number but does not contain all positive rational numbers.%
\end{dlimedium}%
\begin{dlimedium}{Property II}{def_dedekind-cuts-4-3-1-2}%
Every positive rational number in the set is less than every positive rational number not in the set.%
\end{dlimedium}%
\begin{dlimedium}{Property III}{def_dedekind-cuts-4-3-1-3}%
There is no element of the set which is greater than every other element of the set.%
\end{dlimedium}%
\end{descriptionlist}
%
\end{definition}
\index{Dedekind, Richard} Because of their intended audiences, Dedekind and Landau shied away from using too much notation.  However, we will include the following for those who are more comfortable with the symbolism as it may help provide more perspective. Specifically the properties defining a Dedekind cut \(\alpha\) can be written as follows.%
\par
%
\begin{descriptionlist}
\begin{dlimedium}{Property I}{DedekindCuts-15-1-1}%
\(\alpha\ne\emptyset\) and \(\QQ^+-\alpha\ne\emptyset\).%
\end{dlimedium}%
\begin{dlimedium}{Property II}{DedekindCuts-15-1-2}%
If \(x\in\alpha\) and \(y\in\QQ^+-\alpha\), then \(x\lt y\). (Alternatively, if \(x\in\alpha\) and \(y\lt x\), then \(y\in\alpha\).)%
\end{dlimedium}%
\begin{dlimedium}{Property III}{DedekindCuts-15-1-3}%
If \(x\in\alpha\), then \(\exists\ z\in\alpha\) such that \(x\lt z\).%
\end{dlimedium}%
\end{descriptionlist}
%
\par
Properties I-III really say that Dedekind cuts are bounded open intervals of rational numbers starting at \(0\). For example, \((0,3)\cap\QQ^+\) is a Dedekind cut (which will eventually be the real number \(3\)).  Likewise, \(\left\{x|x^2\lt 2\right\}\cap\QQ^+\) is a Dedekind cut (which will eventually be the real number \(\sqrt{2}\)). Notice that care must be taken not to actually refer to irrational numbers in the properties as the purpose is to construct them from rational numbers, but it might help to ground you to anticipate what will happen.%
\par
Take particular notice of the following three facts:%
\begin{enumerate}
\item{}Very little mathematical knowledge is required to understand this definition.  We need to know what a set is, we need to know what a rational number is, and we need to know that given two positive rational numbers either they are equal or one is greater.%
\item{}The language Landau uses is \emph{very} precise. This is necessary in order to avoid such nonsense as trying to compare something with nothing like we did a couple of paragraphs up.%
\item{}We are only using the \emph{positive} rational numbers for our construction.  The reason for this will become clear shortly.  As a practical matter for now, this means that the cuts we have just defined will (eventually) correspond to the \emph{positive} real numbers.%
\end{enumerate}
%
\begin{definition}{Definition}{}{def_dedekind-cuts-ordering}%
\index{Dedekind, Richard!Dedekind cuts!ordering of}%
Let \(\alpha\) and \(\beta\) be cuts.  Then we say that \(\alpha\) is less than \(\beta\), and write%
\begin{equation*}
\alpha\lt \beta
\end{equation*}
if there is a rational number in \(\beta\) which is not in \(\alpha\).%
\end{definition}
Note that, in light of what we said prior to \hyperref[def_dedekind-cuts-ordering]{Definition~{\xreffont\ref{def_dedekind-cuts-ordering}}} (which is taken directly from Landau), we notice the following.%
\begin{theorem}{Theorem}{}{}{thm_OrderingCuts}%
\index{Dedekind, Richard!Dedekind cuts!ordering of}%
Let \(\alpha\) and \(\beta\) be cuts.  Then \(\alpha\lt \beta\) if and only if \(\alpha\subset\beta\).%
\end{theorem}
\begin{problem}{Problem}{}{DedekindCuts-21}%
Prove \hyperref[thm_OrderingCuts]{Theorem~{\xreffont\ref{thm_OrderingCuts}}} and use this to conclude that if \(\alpha\) and \(\beta\) are cuts then exactly one of the following is true:%
\begin{enumerate}
\item{}\(\alpha=\beta\).%
\item{}\(\alpha\lt \beta\).%
\item{}\(\beta\lt \alpha\).%
\end{enumerate}
%
\end{problem}
We will need first to define addition and multiplication for our cuts and eventually these will need to be extended to \(\RR\) (once the non-positive reals have also been constructed).  It will be necessary to show that the extended definitions satisfy the field axioms.  As you can see there is a lot to do.%
\par
\index{Dedekind, Richard} As we did with Cauchy sequences and with infinite decimals, we will stop well short of the full construction.  If you are interested in exploring the details of Dedekind's construction, Landau's book~\hyperlink{landau66__found_analy}{[{\xreffont 7}]} is very thorough and was written with the explicit intention that it would be accessible to students.  In his ``Preface for the Teacher'' he says%
\begin{quote}%
I hope that I have written this book, after a preparation stretching over decades, in such a way that a normal student can read it in two days.%
\end{quote}
This may be stretching things.  Give yourself at least a week and make sure you have nothing else to do that week.%
\par
Addition and multiplication are defined in the obvious way.%
\begin{definition}{Definition}{Addition on cuts.}{def_dedekind-cuts-addition}%
\index{Dedekind, Richard!Dedekind cuts!addition of}%
Let \(\alpha\) and \(\beta\) be cuts.  We will denote the set \(\left\{x+y|x\in\alpha,
y\in\beta\right\}\) by \(\alpha+\beta\).%
\end{definition}
\begin{definition}{Definition}{Multiplication on cuts.}{def_dedekind-cuts-multiplication}%
\index{Dedekind, Richard!Dedekind cuts!multiplication of positive cuts}%
Let \(\alpha\) and \(\beta\) be cuts.  We will denote the set \(\left\{xy|x\in\alpha,
y\in\beta\right\}\) by \(\alpha\beta \text{ or }
\alpha\cdot\beta\).%
\end{definition}
If we are to have a hope that these objects will serve as our real numbers we must have closure with respect to addition and multiplication.  We will show closure with respect to addition.%
\begin{theorem}{Theorem}{Closure with Respect to Addition.}{}{thm_dedekind-cuts-closure}%
\index{Dedekind, Richard!Dedekind cuts!closure of}%
If \(\alpha\) and \(\beta\) are cuts then \(\alpha+\beta\) is a cut.%
\end{theorem}
\begin{proof}{Proof}{}{DedekindCuts-31}
We need to show that the set \(\alpha+\beta\) satisfies all three of the properties of a cut.%
\par
%
\begin{descriptionlist}
\begin{dlimedium}{Property I}{DedekindCuts-31-2-1-1}%
Let \(x\) be \emph{any} rational number in \(\alpha\) and let \(x_1\) be a rational number not in \(\alpha\).  Then by Property II \(x\lt
x_1\).%
\par
Let \(y\) be \emph{any} rational number in \(\beta\) and let \(y_1\) be a rational number not in \(\beta\).  Then by Property II \(y\lt
y_1\).%
\par
Thus since \(x+y\) represents a generic element of \(\alpha+\beta\) and \(x+y\lt x_1+y_1\), it follows that \(x_1+y_1\not\in\alpha+\beta\).%
\end{dlimedium}%
\begin{dlimedium}{Property II}{DedekindCuts-31-2-1-2}%
We will show that the contrapositive of \alert{Property II} is true: If \(x\in\alpha+\beta\) and \(y\lt x\) then \(y\in\alpha+\beta\).%
\par
First, let \(x\in\alpha+\beta\).  Then there are \(x_\alpha\in\alpha\) and \(x_\beta\in\beta\) such that \(y\lt x=x_\alpha+x_\beta\).  Therefore \(\frac{y}{x_\alpha+x_\beta}\lt 1\), so that%
\begin{align*}
x_\alpha\left(\frac{y}{x_\alpha+x_\beta}\right)\amp \lt  x_\alpha\\
\intertext{and}
x_\beta\left(\frac{y}{x_\alpha+x_\beta}\right)\amp \lt  x_\beta\text{.}
\end{align*}
%
\par
Therefore \(x_\alpha\left(\frac{y}{x_\alpha+x_\beta}\right)\in\alpha\) and \(x_\beta\left(\frac{y}{x_\alpha+x_\beta}\right)\in\beta\). Therefore%
\begin{equation*}
y=x_\alpha\left(\frac{y}{x_\alpha+x_\beta}\right)+x_\beta\left(\frac{y}{x_\alpha+x_\beta}\right)\in\alpha+\beta\text{.}
\end{equation*}
%
\end{dlimedium}%
\begin{dlimedium}{Property III}{DedekindCuts-31-2-1-3}%
Let \(z\in\alpha+\beta\).  We need to find \(w>z\), \(w\in\alpha+\beta\).  Observe that for some \(x\in\alpha\) and \(y\in\beta\)%
\begin{equation*}
z=x+y\text{.}
\end{equation*}
%
\par
Since \(\alpha\) is a cut, there is a rational number \(x_1\in\alpha\) such that \(x_1>x\). Take \(w=x_1+y\in\alpha+\beta\).  Then%
\begin{equation*}
w=x_1+y>x+y=z\text{.}
\end{equation*}
%
\end{dlimedium}%
\end{descriptionlist}
%
\end{proof}
\begin{problem}{Problem}{}{DedekindCuts-32}%
Show that if \(\alpha\) and \(\beta\) are cuts then \(\alpha\cdot\beta\) is also a cut.%
\end{problem}
At this point we have built our cuts and we have defined addition and multiplication for cuts.  However, as observed earlier the cuts we have will (very soon) correspond only to the positive real numbers.  This may appear to be a problem but it really isn't because the non-positive real numbers can be defined in terms of the positives, that is, in terms of our cuts.  We quote from Landau \hyperlink{landau66__found_analy}{[{\xreffont 7}]}:%
\begin{quote}%
These cuts will henceforth be called the ``positive numbers;''  .  .  .%
\par
We create a new number \(0\) (to be read ``zero''), distinct from the positive numbers.%
\par
We also create numbers which are distinct from the positive numbers as well as distinct from zero, and which we will call negative numbers, in such a way that to each \(\xi\) (I.e. to each positive number) we assign a negative number denoted by \(-\xi\) (\(-\) to be read ``minus''). In this, \(-\xi\) and \(-\nu\) will be considered as the same number (as equal) if and only if \(\xi\) and \(\nu\) are the same number.%
\par
The totality consisting of all positive numbers, of \(0\), and of all negative numbers, will be called the real numbers.%
\end{quote}
Of course it is not nearly enough to simply postulate the existence of the non-negative real numbers.%
\par
All we have so far is a set of objects we're calling the real numbers.  For some of them (the positive reals -{}-{} that is, the cuts) we have defined addition and multiplication. These definitions will eventually turn out to correspond to the addition and multiplication we are familiar with.%
\par
However we do not have either operation for our entire set of proposed real numbers.  Before we do this we need first to define the \emph{absolute value} of a real number. This is a concept you are very familiar with and you have probably seen the following definition: Let \(\alpha\in\RR\).  Then%
\begin{equation*}
\abs{\alpha} = \begin{cases}\alpha\amp  \text{ if \(\alpha\ge0,\) } \\ -\alpha\amp  \text{ if \(\alpha\lt 0.\) } \end{cases}
\end{equation*}
%
\par
Unfortunately we cannot use this definition because we do not yet have a linear ordering on \(\RR\) so the statement \(\alpha\ge0\) is meaningless.  Indeed, it will be our definition of absolute value that orders the real numbers.  We must be careful.%
\par
Notice that by definition a negative real number is denoted with the dash ('-') in front.  That is \(\chi\) is positive while \(-\chi\) is negative.  Thus if \(A\) is \emph{any} real number then one of the following is true:%
\begin{enumerate}
\item{}\(A=\chi\) for some \(\chi\in\RR\) (\(A\) is positive)%
\item{}\(A=-\chi\) for some \(\chi\in\RR\) (\(A\) is negative)%
\item{}\(A=0\).%
\end{enumerate}
%
\par
We define absolute value as follows:%
\begin{definition}{Definition}{}{def_absolute-value}%
\index{Dedekind, Richard!Dedekind cuts!absolute value}%
Let \(A\in\RR\) as above. Then%
\begin{equation*}
\abs{A}= \begin{cases}\chi \amp  \text{ if \(A=\chi{}\)} \\ 0 \amp  \text{ if \(A=0\) } \\ \chi \amp  \text{ if \(A=-\chi{}.\)} {} \end{cases}
\end{equation*}
%
\end{definition}
With this definition in place it is possible to show that \(\RR\) is linearly ordered.  We will not do this explicitly.  Instead we will simply assume that the symbols ``\(\lt\)'' ``\(>\),'' and ``\(=\)'' have been defined and have all of the properties we have learned to expect from them.%
\par
We now extend our definitions of addition and multiplication from the positive real numbers (cuts) to all of them. Curiously, multiplication is the simpler of the two.%
\begin{definition}{Definition}{Multiplication.}{def_dedekind-multiplication}%
\index{Dedekind, Richard!Dedekind cuts!multiplication of}%
Let \(\alpha, \beta\in\RR\).  Then%
\begin{equation*}
\alpha\cdot\beta = \begin{cases}-\abs{\alpha}\abs{\beta}\amp  \text{ if \(\alpha>0,\beta\lt 0\) or \(\alpha\lt 0, \beta>0,\) }  \\ \ \ \,\abs{\alpha}\abs{\beta}\amp  \text{ if \(\alpha\lt 0,\beta\lt 0,\) }  \\ \ \ \ \ \ \, 0\amp  \text{ if \(\alpha=0\) or \(\beta=0.\) }  {} \end{cases}
\end{equation*}
%
\end{definition}
Notice that the case where \(\alpha\) and \(\beta\) are both positive was already handled by \hyperref[def_dedekind-cuts-multiplication]{Definition~{\xreffont\ref{def_dedekind-cuts-multiplication}}} because in that case they are both cuts.%
\par
Next we define addition.%
\begin{definition}{Definition}{Addition.}{def_dedekind-addition}%
\index{Dedekind, Richard!Dedekind cuts!addition of}%
Let \(\alpha, \beta\in\RR\).  Then%
\begin{equation*}
\alpha+\beta= 
\begin{cases}
-(\abs{\alpha}+\abs{\beta}) \amp  \text{ if \(\alpha\lt 0, \beta\lt 0 \) } \\
\ \ \ \abs{\alpha}-\abs{\beta}\amp  \text{ if \(\alpha>0, \beta\lt 0, \abs{\alpha}>\abs{\beta} \)} \\
\ \ \ \ \ \ \ \,0             \amp  \text{ if \(\alpha>0, \beta\lt 0, \abs{\alpha}=\abs{\beta} \)} \\
-(\abs{\alpha}-\abs{\beta})   \amp  \text{ if \(\alpha>0, \beta\lt 0, \abs{\alpha}\lt \abs{\beta} \)} \\
\ \ \ \ \ \beta+\alpha        \amp  \text{ if \(\alpha\lt 0, \beta>0 \) } \\
\ \ \ \ \ \ \ \,\beta{}       \amp  \text{ if \(\alpha=0 \) } \\
\ \ \ \ \ \ \ \,\alpha        \amp  \text{ if \(\beta=0 \) } 
\end{cases} \text{.}
\end{equation*}
%
\end{definition}
\begin{aside}{Aside}{Comment.}{DedekindCuts-48}%
Notice also that the fifth case refers to the addition as defined in the second case.%
\end{aside}
But wait!  In the second and fourth cases of our definition we've actually defined addition in terms of subtraction. But we haven't defined subtraction yet!  Oops!%
\par
This is handled with the definition below, but it illuminates very clearly the care that must be taken in these constructions.  The real numbers are \emph{so} familiar to us that it is extraordinarily easy to make unjustified assumptions.%
\par
Since the subtractions in the second and fourth cases above are done with positive numbers we only need to give meaning to the subtraction of cuts.%
\begin{definition}{Definition}{}{def_CutSubtraction}%
\index{Dedekind, Richard!Dedekind cuts!subtraction of}%
If \(\alpha\), \(\beta\) and \(\delta\) are cuts then the expression%
\begin{equation*}
\alpha-\beta=\delta
\end{equation*}
is defined to mean%
\begin{equation*}
\alpha=\delta+\beta\text{.}
\end{equation*}
%
\end{definition}
Of course, there is the detail of showing that there is such a cut \(\delta\).  (We warned you of the tediousness of all this.)  Landau goes through the details of showing that such a cut exists.  We will present an alternative by defining the cut \(\alpha-\beta\) directly (assuming \(\beta\lt \alpha\)).  To motivate this definition, consider something we are familiar with: \(3-2=1\).  In terms of cuts, we want to say that the open interval from \(0\) to \(3\) ``minus'' the open interval from \(0\) to \(2\) should give us the open interval from \(0\) to \(1\).  Taking elements from \((0,3)\) and subtracting elements from \((0,2)\) won't do it as we would have differences such as \(2.9-.9=2\) which is not in the cut \((0,1)\). A moment's thought tells us that what we need to do is take all the elements from \((0,3)\) and subtract all the elements from \((2,\infty)\), restricting ourselves only to those which are positive rational numbers.  This prompts the following definition.%
\begin{definition}{Definition}{}{def_SubtractionOfCutsAsSets}%
\index{Dedekind, Richard!Dedekind cuts!as sets}%
Let \(\alpha\) and \(\beta\) be cuts with \(\beta\lt \alpha\).  Define \(\alpha-\beta\) as follows:%
\begin{equation*}
\alpha-\beta =\left\{x-y|x\in\alpha \text{ and } y\not\in\beta\right\}\cap\QQ^+\text{.}
\end{equation*}
%
\end{definition}
To show that, in fact, \(\beta+(\alpha-\beta)=\alpha\), the following technical lemma will be helpful.%
\begin{lemma}{Lemma}{}{}{lem_Technical}%
Let \(\beta\) be a cut, \(y\) and \(z\) be positive rational numbers not in \(\beta\) with \(y\lt z\), and let \(\eps>0\) be any rational number.  Then there exist positive rational numbers \(r\) and \(s\) with \(r\in\beta\), and \(s\not\in\beta\), such that \(s\lt z\), and \(s-r\lt \eps\).%
\end{lemma}
\begin{problem}{Problem}{}{DedekindCuts-57}%
Prove \hyperref[lem_Technical]{Lemma~{\xreffont\ref{lem_Technical}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{DedekindCuts-57-3}{}\quad{}Since \(\beta\) is a cut there exists \(r_1\in\beta\). Let \(s_1=y\not\in\beta\).  We know that \(r_1\lt s_1\lt
z\).  Consider the midpoint \(\frac{s_1+r_1}{2}\).  If this is in \(\beta\) then relabel it as \(r_2\) and relabel \(s_1\) as \(s_2\).  If it is not in \(\beta\) then relabel it as \(s_2\) and relabel \(r_1\) as \(r_2\), etc.%
\end{problem}
\begin{problem}{Problem}{}{DedekindCuts-58}%
Let \(\alpha\) and \(\beta\) be cuts with \(\beta\lt
\alpha\).  Prove that \(\beta+(\alpha-\beta)=\alpha\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{DedekindCuts-58-3}{}\quad{}It is pretty straightforward to show that \(\beta+(\alpha-\beta)\subseteq\alpha\).  To show that \(\alpha\subseteq\beta+(\alpha-\beta)\), we let \(x\in\alpha\).  Since \(\beta\lt \alpha\), we have \(y\in\alpha\) with \(y\not\in\beta\). We can assume without loss of generality that \(x\lt
y\). (Why?) Choose \(z\in\alpha\) with \(y\lt
z\).  By the \hyperref[lem_Technical]{Lemma~{\xreffont\ref{lem_Technical}}}, there exists positive rational numbers \(r\) and \(s\) with \(r\in\beta\), \(s\in\beta\), \(s\lt z\), and \(s-r\lt z-x\).  Show that \(x\lt r+(z-s)\).%
\end{problem}
We will end by saying that no matter how you construct the real number system, there is really only one.  More precisely we have the following theorem which we state without proof.%
\begin{aside}{Aside}{Comment.}{DedekindCuts-60}%
In fact, \emph{not} proving this result seems to be standard in real analysis references.  Most often it is simply stated as we do here.%
\end{aside}
\begin{theorem}{Theorem}{}{}{thm_R-is-R}%
\index{\(\RR\)!any complete, linearly ordered field is isomorphic to}%
\index{fields!any complete, linearly ordered field is isomorphic to \(\RR\).}%
Any complete, linearly ordered field is isomorphic to \(\RR\).%
\end{theorem}
Two linearly ordered number fields are said to be isomorphic if there is a one-to-one, onto mapping between them (such a mapping is called a bijection) which preserves addition, multiplication, and order. More precisely, if \({\cal F}_1\) and \({\cal
F}_2\) are both linearly ordered fields, \(x, y
\in{\cal F}_1\) and \(\phi:{\cal F}_1\rightarrow{\cal
F}_2\) is the mapping then%
\begin{enumerate}
\item{}\(\displaystyle \phi(x+y)=\phi(x)+\phi(y)\)%
\item{}\(\displaystyle \phi(x\cdot y) = \phi(x)\cdot\phi(y)\)%
\item{}\(x\lt y \imp \phi(x)\lt \phi(y)\).%
\end{enumerate}
%
\par
Remember that we warned you that these constructions were fraught with technical details that are not necessarily illuminating.  Nonetheless, at this point, you have everything you need to show that the set of all real numbers as defined above is linearly ordered and satisfies the Least Upper Bound property.%
\par
But we will stop here in order, to paraphrase Descartes, to leave for you the joy of further discovery.%
\end{subsectionptx}
\end{sectionptx}
\end{chapterptx}
\end{partptx}
%
\backmatter%
%
\clearpage\phantomsection%
\addcontentsline{toc}{part}{Back Matter}%
%
%
\typeout{************************************************}
\typeout{References  Bibliography}
\typeout{************************************************}
%
\begin{references-chapter-numberless}{References}{Bibliography}{}{Bibliography}{}{}{ASORA-6-1}
%% If this is a top-level references
%%   you can replace with "thebibliography" environment
\begin{referencelist}
\bibitem[1]{bradley09__cauch_cours}\hypertarget{bradley09__cauch_cours}{}Robert E. Bradley and C. Edward Sandifer. \emph{\textit{Cauchy's Cours d'analyse}: An Annotated Translation}. Sources and Studies in the History of Mathematics and Physical Sciences. Springer, 2009.
\bibitem[2]{dunham90__journ_throug_genius}\hypertarget{dunham90__journ_throug_genius}{}William Dunham. \emph{Journey Through Genius}. Penguin Books, 1990.
\bibitem[3]{franks10__cantor_other_proof_rr_uncoun}\hypertarget{franks10__cantor_other_proof_rr_uncoun}{}John Franks. Cantor's other proofs that \(\RR\) is uncountable. \emph{Mathematics Magazine}, 83(4):283-{}-{}289, October 2010.
\bibitem[4]{grabiner81__origin_cauch_rigor_calculy}\hypertarget{grabiner81__origin_cauch_rigor_calculy}{}Judith Grabiner. \emph{The Origins of Cauchy's Rigorous Calculus}. MIT Press, Cambridge MA, 1981.
\bibitem[5]{hawking05__god_creat_integ}\hypertarget{hawking05__god_creat_integ}{}Stephen Hawking, editor. \emph{God Created the Integers: The Mathematical Breakthroughs that Changed History}. Running Press, Philadelphia, London, 2005.
\bibitem[6]{jahnke03__histor_analy}\hypertarget{jahnke03__histor_analy}{}H. Jahnke, editor. \emph{A History of Analysis}. AMS Publications, Providence RI, 2003.
\bibitem[7]{landau66__found_analy}\hypertarget{landau66__found_analy}{}Edmund Landau. \emph{Foundations of Analysis}. Chelsea Publishing Company, New York, NY, 1966. Translated by F. Steinhardt, Columbia University.
\bibitem[8]{levenson09__newton_count}\hypertarget{levenson09__newton_count}{}Thomas Levenson. \emph{Newton and the Counterfeiter}. Houghton Mifflin Harcourt, 2009.
\bibitem[9]{netz07__archim_codex}\hypertarget{netz07__archim_codex}{}Reviel Netz and William Noel. \emph{The Archimedes Codex}. Da Capo Press, 2007.
\bibitem[10]{newton45__sir_isaac_two_treat_quadr}\hypertarget{newton45__sir_isaac_two_treat_quadr}{}Isaac Newton. \emph{Sir Isaac Newton's Two Treatises of the Quadrature of Curves and Analysis by Equation of an Infinite Number of Terms, Explained}. Society for the Encouragement of Learning, 1745. Translated from Latin by John Stewart, A. M. Professor of Mathematicks in the Marishal College and University of Aberdeen.
\bibitem[11]{Bernoulli_bio_mactutor}\hypertarget{Bernoulli_bio_mactutor}{}J. J. O'Connor and E. F. Robertson. \emph{The Brachistochrone Problem}. \emph{http:\slash{}\slash{}www-gap.dcs.st-and.ac.uk", MacTutor History of Mathematics archive}
\bibitem[12]{robinson74__non_stand_analy}\hypertarget{robinson74__non_stand_analy}{}Abraham Robinson. \emph{Non-standard analysis}. North-Holland Pub. Co., 1974.
\bibitem[13]{russo96__forgot_revol}\hypertarget{russo96__forgot_revol}{}Lucio Russo, translated by Silvio Levy. \emph{The Forgotten Revolution: How Science Was Born in 300 BC and Why It Had to Be Reborn}. Springer, 1996.
\bibitem[14]{sandifer07__early_mathem_leonar_euler}\hypertarget{sandifer07__early_mathem_leonar_euler}{}C. Edward Sandifer. \emph{The Early Mathematics of Leonard Euler}. Spectrum, 2007. ISBN 10: 0883855593, ISBN 13: 978-0883855591.
\bibitem[15]{struik69__sourc_book_mathem}\hypertarget{struik69__sourc_book_mathem}{}Dirk Struik, editor. \emph{Source Book in Mathematics, 1200-1800}. Harvard University Press, Cambridge, MA, 1969.
\bibitem[16]{thurston56__number_system}\hypertarget{thurston56__number_system}{}H. A. Thurston. \emph{The Number System}. Blackie and Son Limited, London, Glassgow, 1956.
\end{referencelist}
\end{references-chapter-numberless}
%
%% The index is here, setup is all in preamble
%% Index locators are cross-references, so same font here
{\xreffont\printindex}
%
\end{document}
