<chapter xmlns:xi="http://www.w3.org/2001/XInclude"  xml:id="CalcIn17th18thCentury">
  <title>Calculus in the 17th and 18th Centuries</title>
  <section xml:id="CalcIn17th18thCentury-NewtLeibStart">
    <title>Newton and Leibniz Get Started</title>
    <subsection xml:id="sec_leibn-calc-rules">
      <title>Leibniz<rsq/> Calculus Rules</title>

      <p>
        <idx><h>Leibniz, Gottfried Wilhelm</h><h>first Calculus
        publication</h></idx>

        The rules for Calculus were first laid out in the 1684 paper
        <foreign>Nova methodus pro maximis et minimis, itemque
        tangentibus, quae nec fractas nec irrationales, quantitates
        moratur, et singulare pro illi calculi genus</foreign> (A New
        Method for Maxima and Minima as Well as Tangents, Which is
        Impeded Neither by Fractional Nor by Irrational Quantities,
        and a Remarkable Type of Calculus for This) written by <url
        href="https://mathshistory.st-andrews.ac.uk/Biographies/Leibniz/"
        >Gottfried
        Wilhelm Leibniz</url> (1646<ndash/>1716).  Leibniz started
        with subtraction.  That is, if <m>x_1</m> and <m>x_2</m> are
        very close together then their difference, <m>\Delta
        x=x_2-x_1</m>, is very small.  He expanded this idea to say
        that if <m>x_1</m> and <m>x_2</m> are <em>infinitely</em>
        close together (but still distinct) then their difference
        <m>\dx{ x}</m>, is infinitesimally small (but not zero).
      </p>

      <figure>
        <caption>
          Gottfried Wilhelm Leibniz
        </caption>

        <idx><h>Leibniz, Gottfried Wilhelm</h><h>portrait of</h></idx>
        <idx><h>Portraits</h><h>Leibniz</h></idx>

        <image width="35%" source="images/Leibniz.png" >
          <shortdescription>Portrait of Gottfried Wilhelm Leibniz</shortdescription>
        </image>
      </figure>
      <aside>
        <title>
          <foreign>Calculus Differentialis</foreign>
        </title>
        <p>
          This translates, loosely, as the
          <q>Calculus of Differences</q>.
        </p>  
      </aside>


      <p>
        This idea is logically very suspect and Leibniz knew it.  But
        he also knew that when he used his <foreign>calculus
        differentialis</foreign> he was getting correct answers
        to some very hard problems.  So he persevered.
      </p>

      <p>
        Leibniz called both <m>\Delta x</m> and <m>\dx{ x}</m>
        differentials (Latin for difference) because he thought
        of them as, essentially, the same thing.  Over time it has
        become customary to refer to the infinitesimal <m>\dx{ x}</m>
        as a differential, and to reserve the word difference and the
        notation <m>\Delta x</m> for the finite case.  This is why
        Calculus is often called <em>Differential Calculus</em>.
      </p>

      <p>
        In his paper Leibniz gave rules for dealing with these
        infinitely small differentials.  Specifically, given a
        variable quantity <m>x</m>, <m>\dx{x}</m> represented an
        infinitesimal change in <m>x</m>.  Differentials are related
        via the slope of the tangent line to a curve.  That is, if
        <m>y=f(x)</m>, then <m>\dx{ y}</m> and <m>\dx{ x}</m> are related by
        <me>
          \dx{ y}=\text{ (slope of the tangent line) } \cdot \dx{ x}
          </me>.
      </p>

      <p>
        Leibniz then divided by <m>\dx{ x}</m> giving
        <me>
          \dfdx{y}{x}= \text{ (slope of the tangent line). }
        </me>
      </p>

      <p>
        <idx><h>Newton, Isaac</h></idx>

        The elegant and expressive notation Leibniz invented was so
        useful that it has been retained through the years despite
        some profound changes in the underlying concepts.  For
        example, Leibniz and his contemporaries would have viewed the
        symbol <m>\dfdx{y}{x}</m> as an actual quotient of
        infinitesimals, whereas today we define it via the limit
        concept first suggested by Newton.
      </p>

      <p>
        <idx><h>Leibniz, Gottfried Wilhelm</h><h>differentiation
        rules</h></idx>

        As a result Leibniz<rsq/> rules governing his differentials
        are very modern in appearance:
        <md>
          <mrow>\dx{(\text{ constant } )}\amp =0</mrow>
          <mrow>\dx{(z-y+w+x)}\amp =\dx{ z}-\dx{ y}+\dx{ w}+\dx{ x}</mrow>
          <mrow>\dx{(xv)}\amp =x\dx{ v}+v\dx{ x}</mrow>
          <mrow>\dx{\left(\frac{v}{y}\right)}\amp =\frac{y\dx{ v}-v\dx{ y}}{yy}</mrow>
          <intertext>and, when <m>a</m> is an integer:</intertext>
          <mrow>\dx{(x^a)}\amp =ax^{a-1}\dx{ x}</mrow>
          </md>.
      </p>

      <p>
        Leibniz states these rules without proof: <q>. . . the
        demonstration of all this will be easy to one who is
        experienced in such matters . . .</q>. Mathematicians in
        Leibniz<rsq/>s day would have been expected to understand
        intuitively that if <m>c</m> is a constant, then
        <me>
          \dx(c)=c-c=0
          </me>.  
          Likewise, <m>\dx(x+y)=\dx{x}+\dx{y}</m> is really an
          extension of 
          <me>
            \underbrace{(x_2+y_2)-(x_1+y_1)}_{\Delta
            (x+y)}=\underbrace{(x_2-x_1)}_{\Delta
            x}+\underbrace{(y_2-y_1)}_{\Delta{y}} 
            </me>.
      </p>
    </subsection>

    <subsection xml:id="LeibnizProductRule">
      <title>Leibniz<rsq/>s Approach to the Product Rule</title>
      <p>
        <idx><h>Leibniz, Gottfried Wilhelm</h></idx> 

        The explanation of the product rule using differentials is a
        bit more involved, but Leibniz expected that mathematicans
        would be fluent enough to derive it.  The product <m>p=xv</m>
        can be thought of as the area of the following rectangle
      </p>

      <figure xml:id="fig1">
        <caption>
        </caption>
        <image width="50%" source="images/fig1-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>

      <p>
        With this in mind, <m>\dx{ p}=\dx{(xv)}</m> can be thought of as the
        change in area when <m>x</m> is changed by <m>\dx{ x}</m> and
        <m>v</m> is changed by <m>\dx{ v}</m>.  This can be seen as the L
        shaped region in the following drawing.
      </p>
      <figure xml:id="fig2">
        <caption>
        </caption>
        <image width="90%" source="images/fig2-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>

      <p>
        By dividing the L shaped region into 3 rectangles we obtain
        <men xml:id="eq_LeibnizProductRule">
          \dx{(xv)}=x\dx{ v}+v\dx{ x}+\dx{ x}\,\dx{ v}
          </men>.
      </p>

      <p>
        Even though <m>\dx{ x}</m> and <m>\dx{ v}</m> are infinitely small,
        Leibniz reasoned that <m>\dx{ x}\,\dx{ v}</m> is <em>even more</em>
        infinitely small (quadratically infinitely small?)  compared
        to <m>x\dx{ v}</m> and <m>v\dx{ x}</m> and can thus be ignored
        leaving
        <me>
          \dx{ (xv)}=x\dx{ v}+v\dx{ x}
          </me>.
      </p>

      <p>
        <idx><h>Newton, Isaac</h></idx>

        You should feel some discomfort at the idea of simply tossing
        the product <m>\dx{ x}\,\dx{ v}</m> aside because it is
        <q>comparatively small.</q> This means you have been well
        trained, and have thoroughly internalized Newton<rsq/>s dictum
        <xref ref="newton45__sir_isaac_two_treat_quadr" />: <q>The
        smallest errors may not, in mathematical matters, be
        scorned.</q> It is logically untenable to toss aside an
        expression just because it is small.  Even less so should we
        be willing to ignore an expression on the grounds that it is
        <q>infinitely smaller</q> than another quantity which is
        itself <q>infinitely small.</q>
      </p>

      <p>
        Newton and Leibniz both knew this as well as we do.  But they
        also knew that their methods worked.  They gave
        verifiably correct answers to problems which had, heretofore,
        been completely intractable.  It is the mark of their genius
        that both men persevered in spite of the very evident
        difficulties their methods entailed.
      </p>
    </subsection>

    <subsection xml:id="NewtonsApproach">
      <title>Newton<rsq/>s Approach to the Product Rule</title>
      <p>
        
        <url
            href="https://mathshistory.st-andrews.ac.uk/Biographies/Newton/"
            >Isaac
        Newton<rsq/>s</url> (1643<ndash/>1727) approach to Calculus
        <mdash /> his <sq>Method of Fluxions</sq> <mdash /> depended
        fundamentally on motion.  He conceived of variables (fluents)
        as changing (flowing or fluxing) in time.  The rate of change
        of a fluent he called a fluxion.  As a theoretical foundation
        both Leibniz<rsq/>s and Newton<rsq/>s approaches have fallen
        out of favor, although both are still universally used as a
        conceptual approach, a
        <q>
          way of thinking,
        </q> 
        about the ideas of Calculus.
      </p>
      <figure xml:id="Newton">
        <title>
        </title>
        <caption>Isaac Newton</caption>

        <idx><h>Newton, Isaac</h><h>portrait of</h></idx>
        <idx><h>Portraits</h><h>Newton</h></idx>

        <image width="35%" source="images/Newton.png" >
          <shortdescription>Bust of Isaac Newton</shortdescription>
        </image>
      </figure>
      <p>
        In 
        <pubtitle>
          Philosophiae naturalis principia mathematica
        </pubtitle>
        (this is usually shortened to 
        <pubtitle>
          Principia
          </pubtitle>) 
          Newton <q>proved</q> the Product Rule as
          follows: Let <m>x</m> and <m>v</m> be
          <q>
            flowing quantites
          </q>
          and consider the rectangle, <m>R</m>, whose sides are <m>x</m>
          and <m>v</m>.  <m>R</m> is also a flowing quantity and we wish
          to find its fluxion (derivative) at any time.
      </p>
      <!-- <historical> -->
      <!--   <title>Historical Note: Newton<rsq/>s <pubtitle>Method of Fluxions</pubtitle></title> -->
      <!--   <p> -->
      
      <!--     <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Newton/" >Isaac Newton</url><rsq/>s (1643<ndash/>1727) approach to Calculus <mdash /> his <sq>Method of -->
      <!--     Fluxions</sq> <mdash /> depended fundamentally on motion. -->
      <!--     That is, he viewed his variables (fluents) as changing -->
      <!--     (flowing or fluxing) in time.  The rate of change of a -->
      <!--     fluent he called a fluxion.  As a foundation both Leibniz<rsq/>s -->
      <!--     and Newton<rsq/>s approaches have fallen out of favor, although -->
      <!--     both are still universally used as a conceptual approach, a -->
      <!--     <q> -->
      <!--       way of thinking, -->
      <!--     </q>  -->
      <!--     about the ideas of Calculus. -->
      <!--   </p> -->
      <!-- </historical> -->
      <p>
        First we increment <m>x</m> and <m>v</m> by the half<ndash/>increments <m>\frac{\Delta
        x}{2}</m> and <m>\frac{\Delta v}{2}</m> respectively.  Then
        the corresponding half<ndash/>increment of <m>R</m> is
        <men xml:id="eq_prodruleinc">
          \frac{\Delta R}{2}=\left(x+\frac{\Delta x}{2}\right)\left(v+\frac{\Delta v}{2}\right) = xv + x\frac{\Delta v}{2} + v\frac{\Delta x}{2} +\frac{\Delta x\Delta v}{4}
          </men>.
      </p>

      <p>
        Now decrement <m>x</m> and <m>v</m> by the same amounts:
        <men xml:id="eq_prodruledec">
          -\frac{\Delta R}{2}=\left(x-\frac{\Delta x}{2}\right)\left(v-\frac{\Delta v}{2}\right) = xv - x\frac{\Delta v}{2} - v\frac{\Delta x}{2} + \frac{\Delta x\Delta v}{4}
          </men>.
      </p>

      <p>
        Subtracting  <xref ref="eq_prodruledec">equation</xref>
        from  <xref ref="eq_prodruleinc">equation</xref> gives
        <me>
          \Delta R = x\Delta v + v\Delta x
        </me>
        which is the total change of <m>R = xv</m> over the intervals <m>\Delta x</m> and
        <m>\Delta v</m> and also recognizably the Product Rule.
      </p>



      <p>
        This argument is no better than Leibniz<rsq/>s as it relies
        heavily on the number <m>1/2</m> to make it work.  If we
        take any other increments in <m>x</m> and <m>v</m> whose
        total lengths are <m>\Delta x</m> and <m>\Delta v</m> it
        will simply not work.  Try it and see.
      </p>

      <p>
        In Newton<rsq/>s defense, he wasn<rsq/>t really trying to justify his
        mathematical methods in the 
        <pubtitle>
          Principia
          </pubtitle>.  His attention
          was on physics, not math, so he was really just trying to
          give a convincing demonstration of his methods.  You may
          decide for yourself how convincing his demonstration is.
      </p>

      <p>
        <idx><h>Lagrange, Joseph-Louis</h></idx>

        Notice that there is no mention of limits of difference
        quotients or derivatives.  In fact, the term derivative was
        not coined until 1797, by Lagrange as we will see in <xref
        ref="PowerSeriesQuestions-TaylorsFormula"></xref> .  In a
        sense, these topics were not necessary at the time, as Leibniz
        and Newton both assumed that the curves they dealt with had
        tangent lines and, in fact, Leibniz explicitly used the
        tangent line to relate two differential quantities.  This was
        consistent with the thinking of the time and for the duration
        of this chapter we will also assume that all quantities are
        differentiable.  As we will see later this assumption leads to
        difficulties.
      </p>

      <p>
        Both Newton and Leibniz were satisfied that their Calculus
        provided answers that agreed with what was known at the
        time.  For example the formulas
        <me>
          \dx{ \left(x^2\right)}=\dx{\left(xx\right)}=x\dx{ x}+x\dx{
          x}=2x\dx{ x} 
        </me> 
        and
        <me>
          \dx{\left(x^3\right)}=\dx{\left(x^2x\right)}=x^2\dx{
          x}+x\dx{\left(x^2\right)}=x^2+x\left(2x\dx{
          x}\right)=3x^2\dx{ x}
          </me>, 
          were results that had been derived by others using other methods.
      </p>

      <!-- <problem> -->
      <!--   <idx><h>Leibniz, Gottfried Wilhelm</h><h>Leibniz<rsq/>s product rule</h></idx> -->
      <!--   <idx><h>Leibniz<rsq/>s product rule</h></idx> -->
      <!--   <task> -->
      <!--     <statement> -->
      <!--       <p> -->
      <!--         Use Leibniz<rsq/> product rule -->
      <!--         <m>\dx{ \left(xv\right)}=x\dx{ v}+v\dx{ x}</m>  -->
      <!--         to show that if <m>n</m> is a positive integer then -->
      <!--         <m>\dx{ \left(x^n\right)}=nx^{n-1}\dx{ x}</m> -->
      <!--       </p> -->
      <!--     </statement> -->
      <!--   </task> -->
      <!--   <task> -->
      <!--     <statement> -->
      <!--       <p> -->
      <!--         Use Leibniz<rsq/>s product rule to derive the quotient rule -->
      <!--         <me> -->
      <!--           \dx{ \left(\frac{v}{y}\right)}=\frac{y\,\dx{ v}-v\,\dx{ y}}{yy} -->
      <!--           </me>. -->
      <!--       </p> -->
      <!--     </statement> -->
      <!--   </task> -->
      <!--   <task> -->
      <!--     <statement> -->
      <!--       <p> -->
      <!--         Use the quotient rule to show that if <m>n</m> is a -->
      <!--         positive integer, then -->
      <!--         <me> -->
      <!--           \dx{ \left(x^{-n}\right)}=-nx^{-n-1}\dx{ x}. -->
      <!--         </me> -->
      <!--       </p> -->
      <!--     </statement> -->
      <!--   </task> -->
      <!-- </problem> -->


      <!--       <problem> -->
      <!--         <idx><h>Leibniz, Gottfried Wilhelm</h><h>Leibniz<rsq/>s product rule</h></idx> -->
      <!--         <idx><h>Leibniz<rsq/>s product rule</h></idx> -->
      <!--         <introduction> -->
      <!--           <p> -->
      <!--             Assume that <m>n</m> is a positive integer. -->
      <!--           </p> -->
      <!-- </introduction> -->

      <!-- <task> -->
      <!--   <statement> -->
      <!--     <p> -->
      <!--       Use Leibniz<rsq/> product rule -->
      <!--       <m>\dx{ \left(xv\right)}=x\dx{ v}+v\dx{ x}</m>  -->
      <!--       to show that if <m>n</m> is a positive integer then -->
      <!--       <me>\dx{ \left(x^n\right)}=nx^{n-1}\dx{ x}</me> -->
      <!--     </p> -->
      <!--   </statement> -->
      <!-- </task> -->
      <!-- <task> -->
      <!--   <statement> -->
      <!--     <p> -->
      <!--       Suppose <m>u=\frac{v}{y} </m>. Use Leibniz<rsq/>s product rule to derive the quotient rule -->
      <!--       <me> -->
      <!--         \dx{u}= \dx{ \left(\frac{v}{y}\right)}=\frac{y\,\dx{ -->
      <!--         v}-v\,\dx{y}}{yy}  -->
      <!--         </me>. -->
      <!--     </p> -->
      <!--   </statement> -->
      <!-- </task> -->
      <!-- <task> -->
      <!--   <statement> -->
      <!--     <p> -->
      <!--       Use the quotient rule to show that if <m>n</m> is a -->
      <!--       positive integer, then -->
      <!--       <me> -->
      <!--         \dx{ \left(x^{-n}\right)}=-nx^{-n-1}\dx{ x}. -->
      <!--       </me> -->
      <!--     </p> -->
      <!--   </statement> -->
      <!-- </task> -->
      <!--       </problem> -->
      <problem>
        <idx><h>Problem</h><h>differentiation rules</h></idx>

        <introduction>
          <p>
            Assume <m>n</m> is a positive integer.
          </p>
        </introduction>
        <task>
          <statement>
            <p>
              Use Leibniz<rsq/> product rule <m>\dx{\left(xv\right)}=x\dx{v}+v\dx{x}</m> to show that
              <me>\dx{\left(x^n\right)}=nx^{n-1}\dx{x}</me> 
            </p>
          </statement>
        </task>
        <task>
          <statement>
            <p>
              Suppose <m>y=x^{-1}=\frac{1}{x}</m>, Use Leibniz<rsq/> product rule to show
              <me>dy=-1x^{-2}\dx{x}</me> 
            </p>
          </statement>
        </task>
        <task>
          <statement>
            <p>
              Use the product rule and the result of part (b) to derive the quotient rule  
              <me>d\left(\frac{v}{x}\right)=\frac{x\dx{v}-v\dx{x}}{x^2}</me> 

            </p>
          </statement>
        </task>
        <task>
          <statement>
            <p>
              Use the quotient rule to show that

              <me>d\left(x^{-n}\right)=-nx^{-n-1}\dx{x}</me> 

            </p>
          </statement>
        </task>
      </problem>

      <problem>
        <idx><h>Problem</h><h>power rule with fractional
        exponents</h></idx>
        <idx><h>differentiation</h><h>power rule with fractional
        exponents</h></idx>

        <statement>
          <p>
            Suppose <m>y=x^{\frac{p}{q}}</m> with
            <m>q\neq 0</m>, where <m>p</m> and <m>q</m> are integers.
            Show that 
            <me>
              \dx{y}=\dx{
              \left(x^{\frac{p}{q}}\right)}=\frac{p}{q}x^{\frac{p}{q}-1}\dx{
              x}
              </me>.
          </p>
        </statement>
      </problem>

      <p>
        To prove the worth of his Calculus Leibniz also provided
        applications.  As an example he derived Snell<rsq/>s Law of
        Refraction from his Calculus rules as follows.
      </p>

      <figure  xml:id="FIGURESnellPortrait">
        <caption>
          <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Snell/" >Willebrord Snell</url> (1580<ndash/>1626)
        </caption>
        <image source="images/Snell.png" width="35%">
          <shortdescription>Portrait of Willebrord Snell</shortdescription>
        </image>
      </figure>

      <p>
        Given that light travels through air at a speed of
        <m>v_a</m> and travels through water at a speed of
        <m>v_w</m> the problem is to find the fastest path from
        point <m>A</m> to point <m>B</m>.
      </p>

      <figure xml:id="snellfig">
        <title>
        </title>
        <caption>
        </caption>
        <image width="55%" source="images/snellfig-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>

      <p>
        According to 
        <url href="https://en.wikipedia.org/wiki/Fermat%27s_principle" >Fermat<rsq/>s Principle of Least Time</url>, this fastest
        path is the one that light will travel.
      </p>

      <p>
        Using the fact that <m>\text{ Time }=\frac{\text{Distance}}{\text{Velocity}}</m> and the labeling in the picture below
        we can obtain a formula for the time <m>T</m> it takes for
        light to travel from <m>A</m> to <m>B</m>.
      </p>
      <figure xml:id="snellfig2">
        <caption>
        </caption>
        <image width="55%" source="images/snellfig2-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>
      <p>
        <me>
          T=\frac{\sqrt{x^2+a^2}}{v_a}+\frac{\sqrt{(c-x)^2+b^2}}{v_w}
        </me>
      </p>

      <p>
        Using the rules of Leibniz<rsq/>s Calculus, we obtain
        <md>
          <mrow>\dx{ T}\amp = \left[\frac{1}{v_a}\frac{1}{2}\left(x^2+a^2\right)^{-\frac{1}{2}} (2x)+\frac{1}{v_w}\frac{1}{2}((c-x)^2+b^2)^{-\frac{1}{2}}(2(c-x)(-1))\right] \dx{x}</mrow>
          <mrow>\amp =\left[\frac{1}{v_a}\frac{x}{\sqrt{x^2+a^2}}-\frac{1}{v_w}\frac{c-x}{\sqrt{(c-x)^2+b^2}}\right]\dx{ x}</mrow>
          </md>.
      </p>

      <p>
        Using the fact that at the minimum value for <m>T</m>, <m>\dx{
        T}=0</m>, we see that the fastest path from <m>A</m> to
        <m>B</m> must satisfy
        <me>
          \frac{1}{v_a}\frac{x}{\sqrt{x^2+a^2}}=\frac{1}{v_w}\frac{c-x}{\sqrt{(c-x)^2+b^2}}
          </me>.

      </p>
      <figure xml:id="snellfig3">
        <caption>
        </caption>
        <image width="55%" source="images/snellfig3-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>

      <p>
        From <xref ref="snellfig3"></xref> we see that
        the path that light travels must satisfy
        <m>\frac{\sin\theta_a}{v_a}=\frac{\sin\theta_w}{v_w}</m> which
        is Snell<rsq/>s Law.
      </p>

      <p>
        <idx><h>Bernoulli, Johann</h> </idx> 
        <idx><h>Brachistochrone problem, the</h></idx>
        
        To compare 18th century and modern techniques we will consider
        
        <url
            href="https://mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Johann/"
            >Johann
        Bernoulli<rsq/>s</url> (1667<ndash/>1748) solution of the <url
        href="https://mathshistory.st-andrews.ac.uk/HistTopics/Brachistochrone/"
        >Brachistochrone
        Problem</url>.  In 1696, Bernoulli posed and solved, the
        Brachistochrone problem: To find the shape of a frictionless
        wire joining points <m>A</m> and <m>B</m> so that the time it
        takes for a bead to slide down under the force of gravity is
      as small as possible.  </p>


      <figure xml:id="brachfig1">
        <caption>
        </caption>
        <image width="55%" source="images/brachfig1-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>


      <p>
        <idx><h>Bernoulli, Johann</h><h>Bernoulli<rsq/>s
        challenge</h></idx>

        Bernoulli posed this <q>path of fastest descent</q> problem to
        challenge the mathematicians of Europe and used his solution
        to demonstrate the power of Leibniz<rsq/> Calculus as well as his
        own ingenuity.  
      </p>

      <blockquote>
        <p>
          I, Johann Bernoulli, address the most brilliant mathematicians in
          the world. Nothing is more attractive to intelligent people than
          an honest, challenging problem, whose possible solution will
          bestow fame and remain as a lasting monument. Following the
          example set by Pascal, Fermat, etc., I hope to gain the gratitude
          of the whole scientific community by placing before the finest
          mathematicians of our time a problem which will test their methods
          and the strength of their intellect. If someone communicates to me
          the solution of the proposed problem, I shall publicly declare him
          worthy of praise. <xref ref="Bernoulli_bio_mactutor" />
        </p>    
      </blockquote>

      <figure>
        <caption>
          Johann Bernoulli
        </caption>

        <idx><h>Bernoulli, Johann</h><h>portrait of</h></idx>
        <idx><h>Portraits</h><h>Johann Bernoulli</h></idx>

        <image width="35%" source="images/BernoulliJohann.png" >
          <shortdescription>Portrait of Johann Bernoulli</shortdescription>
        </image>
      </figure>

      <p>
        <idx><h>Bernoulli, Jacob</h></idx>
        <idx><h>Newton, Isaac</h></idx>

        In addition to Johann<rsq/>s, solutions were obtained from
        Newton, Leibniz, Johann<rsq/>s brother Jacob Bernoulli, and
        the Marquis de l<rsq/>Hopital <xref
        ref="struik69__sourc_book_mathem" />.  At the time there was
        an ongoing and very vitriolic controversy raging over whether
        Newton or Leibniz had been the first to invent Calculus.  As
        an advocate for Leibniz, Bernoulli did not believe Newton
        would be able to solve the problem using his fluxions. So this
        challenge was in part an attempt to embarrass Newton.  However
        Newton solved it easily.
      </p>

      <p>
        At this point in his life Newton had all but quit science and
        mathematics and was fully focused on his administrative duties
        as Master of the Mint.  Due in part to rampant counterfeiting,
        England<rsq/>s money had become severely devalued and the nation
        was on the verge of economic collapse.  The solution was to
        recall all of the existing coins, melt them down, and strike
        new ones.  As Master of the Mint this job fell to Newton <xref
        ref="levenson09__newton_count" />.  As you might imagine this
        was a rather Herculean task.  Nevertheless, according to his
        niece (and housekeeper):
      </p>

      <blockquote>
        <p>
          When the problem in 1696 was sent by Bernoulli<ndash />Sir
          I.N. was in the midst of the hurry of the great recoinage
          and did not come home till four from the Tower very much
          tired, but did not sleep till he had solved it, which was by
          four in the morning.
        </p>
      </blockquote>

      <p>
        He is reported to have complained,
        <q>
          I do not love . . . to be . . . teezed by forreigners about
          Mathematical things
          </q> <xref ref="dunham90__journ_throug_genius" />.
      </p>

      <p>
        Newton submitted his solution
        anonymously, presumably to avoid more controversy.
        Nevertheless the methods he used were so distinctively
        Newton<rsq/>s that Bernoulli is said to have exclaimed
        <q><foreign>Tanquam ex ungue leonem</foreign>.</q>
      </p>

      <aside>
        <title>
          Translation: <foreign>Tanquam ex ungue leonem</foreign>
        </title>

        <p>
          <q>I know the lion by his claw.</q>
        </p>
      </aside> 

      <p>
        <idx><h>Brachistochrone problem, the</h><h>Bernoulli<rsq/>s solution</h></idx>

        Newton<rsq/>s solution was clever but it doesn<rsq/>t provide any
        insights we<rsq/>ll be interested in so we will focus on
        Bernoulli<rsq/>s ingenious solution which starts, interestingly
        enough, with Snell<rsq/>s Law of Refraction.  He begins by
        considering the stratified medium in the following figure, where an
        object travels with velocities <m>v_1, v_2, v_3, \ldots</m> in
        the various layers.
      </p>
      <figure xml:id="brachfig2">
        <caption>
        </caption>
        <image width="55%" source="images/brachfig2-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>

      <p>
        By repeatedly applying Snell<rsq/>s Law he concluded that the fastest path must satisfy
        <me>
          \frac{\sin \theta_1}{v_1}=\frac{\sin \theta_2}{v_2}=\frac{\sin\theta_3}{v_3}=\cdots
          </me>.
      </p>

      <p>
        In other words, the ratio of the sine of the angle that the
        curve makes with the vertical and the speed remains constant
        along this fastest path.
      </p>

      <p>
        If we think of a continuously changing medium as stratified
        into infinitesimal layers and extend Snell<rsq/>s law to an object
        whose speed is constantly changing,
        then along the fastest path,
        the ratio of the sine of the angle that the curve<rsq/>s tangent makes with the vertical,
        <m>\alpha</m>, and the speed,
        <m>v</m>, must remain constant,
        <me>
          \frac{\sin\alpha}{v}=c
        </me>
        as in <xref ref="snellfig4"></xref> below.
      </p>

      <figure xml:id="snellfig4">
        <caption>
        </caption>
        <image width="40%" source="images/snellfig4-1.png">
          <shortdescription></shortdescription>
        </image>
      </figure>


      <p>
        If we include the horizontal and vertical axes and let <m>P</m>
        denote the position of the bead at a particular time then we have
        the following picture.
      </p>
      <figure xml:id="snellfig5">

        <caption>
        </caption>
        <image width="55%" source="images/snellfig5-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>

      <p>
        In <xref ref="snellfig5"></xref>, <m>s</m> denotes the length that
        the bead has traveled down to point <m>P</m> (that is, the arc
        length of the curve from the origin to that point) and <m>a</m>
        denotes the tangential component of the acceleration due to gravity
        <m>g</m>.  Since  acceleration is the rate of change of velocity with
        respect to time we see that
        <me>
          \dfdx{v}{t}=a
          </me>.
      </p>

      <p>
        To get a sense of how physical problems were approached using
        Leibniz<rsq/>s Calculus we will use the above equation to show that
        <m>v=\sqrt{2gy}</m>.
      </p>

      <p>
        By similar triangles we have 
        <m>\frac{a}{g}=\frac{\dx{ y}}{\dx{ s}}</m>.  
        As a student of Leibniz, Bernoulli would have regarded 
        <m>\frac{\dx{ y}}{\dx{ s}}</m> as a fraction so

        <me>
          a\dx{s}  = g\dx{y}
        </me>
        and since acceleration is the rate of change of velocity we have
        
        <men xml:id="EQUATIONDifferentialDemo">
          \frac{\dx{ v}}{\dx{ t}}\dx{ s}  = g\dx{ y}
          </men>.

          In the seventeenth, eighteenth, and even well into the
          nineteenth centuries, European mathematicians regarded
          <m>\dx{ v}, \dx{ t}</m>, and <m>\dx{ s}</m> as
          infinitesimally small numbers which nevertheless obey all
          of the usual rules of algebra. Thus they would simply  rearrange <xref ref="EQUATIONDifferentialDemo">equation</xref>,  to get

          <me>
            \frac{\dx{s}}{\dx{t}}\dx{ v}  = g\dx{y}
            </me>.
            Since <m>\frac{\dx{ s}}{\dx{ t}}</m> is the rate of change of position with respect to time it is, in fact, the velocity of the bead. That is
            <me>v\dx{ v}  = g\dx{ y}</me>.

            Bernoulli would have interpreted this as a statement that two rectangles of height <m>v</m> and <m>g</m>, with respective widths <m>\dx{ v}</m> and <m>\dx{ y}</m> have equal area. Summing (integrating) all such rectangles we g
            et:
            <md>
              <mrow>\int{}v\dx{ v} \amp = \int{}g\dx{ y}</mrow>
              <mrow>\frac{v^2}{2} \amp = gy</mrow>
            </md>
            or
            <men xml:id="eq_brach_vel">
              v=\sqrt{2gy}
              </men>.
      </p>

      <p>
        You are undoubtedly uncomfortable with the cavalier
        manipulation of infinitesimal quantities you<rsq/>ve just
        witnessed, so we<rsq/>ll pause for a moment now to compare a
        modern development of <xref
        ref="eq_brach_vel">equation</xref> to Bernoulli<rsq/>s.  As
        before we begin with the equation:
        <md>
          <mrow>\frac{a}{g}\amp = \dfdx{y}{s}</mrow>

          <mrow>a \amp = g\dfdx{y}{s}.</mrow>

          <intertext>
            Moreover, since acceleration is the derivative of velocity this is the same as:
          </intertext>

          <mrow>\dfdx{v}{t} \amp = g\dfdx{y}{s}.</mrow>

          <intertext>
            Now observe that by the Chain Rule <m>\dfdx{v}{t} =
            \dfdx{v}{s}\dfdx{s}{t}</m>. The physical interpretation
            of this formula is that velocity will depend on
            <m>s</m>, how far down the wire the bead has moved, but
            that the distance traveled will depend on how much time
            has elapsed. Therefore
          </intertext>

          <mrow>\dfdx{v}{s}\dfdx{s}{t} \amp = g\dfdx{y}{s}</mrow>
          <intertext>or</intertext>
          <mrow>\dfdx{s}{t}\dfdx{v}{s} \amp = g\dfdx{y}{s}</mrow>
          <intertext>and since <m>\dfdx{s}{t} = v</m> we have</intertext>
          <mrow>v\dfdx{v}{s} \amp = g\dfdx{y}{s}</mrow>
          <intertext>Integrating both sides with respect to <m>s</m> gives:</intertext>
          <mrow>\int{}v\dfdx{v}{s}d s \amp = g\int{}\dfdx{y}{s}d s</mrow>
          <mrow>\int{}vd v \amp = g\int{}d y</mrow>
          <intertext>and integrating gives</intertext>
          <mrow>\frac{v^2}{2} \amp = gy</mrow>
        </md>
        as before.
      </p>

      <p>
        <idx><h>Leibniz, Gottfried Wilhelm</h></idx>

        In effect, in the modern formulation we have traded the
        simplicity and elegance of differentials for a comparatively
        cumbersome repeated use of the Chain Rule.  No doubt you
        noticed when taking Calculus that in the differential notation
        of Leibniz, the Chain Rule looks like we are simply
        <q>canceling</q> a factor in the top and bottom of a fraction:
        <m>\dfdx{y}{u}\dfdx{u}{x} = \dfdx{y}{x}</m>.  This is because
        for 18th century mathematicians, that is exactly what it was,
        and Leibniz designed his notation to reflect that viewpoint.
      </p>

      <p>
        To put it another way, 18th century mathematicians wouldn<rsq/>t
        have recognized a need for what we call the Chain Rule because
        this operation was a triviality for them.  Just reduce the
        fraction.  This begs the question: Why did we abandon such a
        clear and simple interpretation of our symbols in favor of the
        comparatively more cumbersome modern interpretation?  This is
        one of the questions we will try to answer in this course.
      </p>

      <p>
        Returning to the Brachistochrone problem we observe that
        <mdn>
          <mrow number="no">\frac{\sin\alpha}{v} \amp = c</mrow>
          <intertext>and since <m>\sin\alpha = \frac{d x}{d s}</m>   we see that</intertext>
          <mrow number="no">\frac{\frac{d x}{d s}}{\sqrt{2gy}} \amp = c</mrow>
          <mrow number="no">\frac{d x}{\sqrt{2gy(ds)^2}} \amp = c</mrow>
          <mrow xml:id="eq_Brachistochrone">\frac{d x}{\sqrt{2gy\left[(d x)^2+(d y)^2\right]}} \amp = c</mrow>
          </mdn>.
      </p>

      <p>
        Bernoulli was then able to solve this differential
        equation.
      </p>

      <problem>
        <idx><h>Problem</h><h>the Brachistochrone problem</h></idx>

        <idx><h>Brachistochrone problem, the</h></idx>

        <statement>
          <p>
            Show that the equations <m>x=\frac{\phi-\sin
            \phi}{4gc^2},\,y=\frac{1-\cos \phi}{4gc^2}</m> satisfy
            equation <xref ref="eq_Brachistochrone">equation</xref>.
            Bernoulli recognized this solution to be an inverted
            cycloid, the curve traced by a fixed point on a circle
            as the circle rolls along a horizontal surface.
          </p>
        </statement>
      </problem>

      <p>
        This illustrates the state of Calculus in the late 1600s
        and early 1700s; the foundations of the subject were a
        bit shaky but there was no denying its power.
      </p>
    </subsection>
  </section>

  <section xml:id="ExponentAdditionProperty">
    <title>Power Series as Infinite Polynomials</title>
    <p>
      <idx><h>polynomials</h><h>infinite</h></idx>
      
      Applied to polynomials, the rules of differential and integral
      Calculus are straightforward.  Indeed, differentiating and
      integrating polynomials represent some of the easiest tasks in a
      Calculus course.  For example, computing <m>\int(7-x+x^2)\dx{
      x}</m> is relatively easy compared to computing
      <m>\int\sqrt[3]{1+x^3}\dx{ x}</m>.  Unfortunately, not all
      functions can be expressed as a polynomial.  For example,
      <m>f(x)=\sin x</m> cannot be since a polynomial has only
      finitely many roots and the sine function has infinitely many
      roots, namely <m>\{n\pi|\,n\in\ZZ\}</m>.  A standard technique
      in the 18th century was to write such functions as an
      <q>infinite polynomial,</q> or what today we refer to as a <term>power
      series</term>.  Unfortunately an <q>infinite polynomial</q> is a much
      more subtle object than a mere polynomial, which by definition
      is finite.  For now we will not concern ourselves with these
      subtleties. Instead we will follow the example of our forebears and
      manipulate all <q>polynomial<ndash/>like</q> objects (finite or
      infinite) as if they are polynomials.
    </p>

    <definition xml:id="def_PowerSeries">
      
      <idx><h>power series</h><h>definition</h></idx>
      <idx><h>Definition</h><h>power series</h></idx>

      <statement>
        <p>
          A <term>power series centered at
          <m>\boldsymbol{a}</m></term> is a series of the form
          <me>
            \sum_{n=0}^\infty a_n(x-a)^n=a_0+a_1(x-a)+a_2(x-a)^2+\cdots
            </me>.
        </p>
      </statement>
    </definition>


    <p>
      Thus a power series centered around zero has the form
      <me>
        \sum_{n=0}^\infty a_nx^n </me>.  In this section we will focus
        on power series centered around zero. In <xref
        ref="SECTIONPowSerWOTaylor" text="custom" >the next section</xref> we will
        look at power series centered about points other than zero.
        <m>0</m>.
    </p>



    <paragraphs>
      <title> A useful comment on notation: </title>

      <p>
        The most advantageous way to represent a power series is using
        summation notation since there can be no doubt about the
        pattern in the terms.  After all, this notation contains a
        formula for the general term.  However, there are instances
        where summation notation is not practical.  In these cases, it
        is acceptable to indicate the sum by supplying the first few
        terms and using ellipses (the three dots).  If this is done,
        then enough terms must be included to make the pattern clear
        to the reader.
      </p>

    </paragraphs>


    <p>
      <idx><h>series</h><h>Geometric series</h><h>naive derivation</h></idx>

      Returning to our definition of a power series, consider the
      power series 
      <me>
        \sum_{n=0}^\infty x^n=1+x+x^2+\cdots </me>.  If we multiply
        this power series by <m>(1-x)</m>, we obtain

        <md>
          <mrow>
            (1-x)(1+x+x^2+\cdots)=(1+\amp{}x+x^2+\cdots)
          </mrow>
          <mrow>
            -(\amp{}x+x^2+x^3+\cdots)
          </mrow>
          <mrow>
            =1.\ \ \ \ \amp{}
          </mrow>

        </md>
    </p>

    <p>
      Dividing by <m>1-x</m> gives us the power series representation
      <men xml:id="EQUATIONGeometricSeries">
        \frac{1}{1-x}=1+x+x^2+\cdots<!-- =\sum_{n=0}^\infty x^n  -->
        </men>,
        which is known as the <term>Geometric Series</term>.
    </p>

    <p>
      For each value of <m>x</m> a <term>power series</term> reduces to a
      different ordinary (numerical) <term>series</term>. For example, if
      we substitute <m>x=\frac{1}{10}</m> into the left side of <xref
      ref="EQUATIONGeometricSeries">equation</xref>, we obtain the
      (numerical) <term>series</term>

      <md>
        <mrow>
          1+\frac{1}{10}+\left(\frac{1}{10}\right)^2+\left(\frac{1}{10}\right)^3+
          \cdots\amp{}=1+0.1+0.01+0.001+0.0001+\cdots
        </mrow>
        <mrow>
          \amp{}=1.1111\cdots.
        </mrow>
      </md>
      But substituting into the right side yields
      <me>
        \frac{1}{1-\frac{1}{10}}=\frac{10}{9}. 
      </me>
    </p>

    <p>
      If these computations are valid then it must be that
      <m>1.111\ldots=\frac{10}{9}</m>, which seems weird, but you can
      verify it by entering <m>\frac{10}{9}</m> into any calculator.
    </p>
    <p>
      If <m>1.111\ldots=\frac{10}{9}</m>, then subtracting
      <m>1</m> from both sides and multiplying by <m>3</m> gives
      <me>0.333\ldots = \frac{1}{3}</me> which has probably come up on
      your calculator more than once. But what seems wierder still is
      that if we multiply by <m>3</m> again we get <me>0.999\ldots
      =1</me>, which seems like nonsense. It simply can<rsq/>t be
      true, can it? What do you think? Is <m>0.999\ldots =1</m> or
      that just nonsense?  Either way it is clear that the real
      numbers <m>\left(\RR\right) </m> hide deeper mysteries than the
      irrationality of <m>\sqrt{2}</m>.
    </p>

    <p>
      There are other issues with these formal manipulations too.
      Substituting <m>x=1</m> or <m>x=2</m> into <xref
      ref="EQUATIONGeometricSeries">equation</xref> yields the
      questionable results
      <me>
        \frac{1}{0}=1+1+1+\cdots \text{  and  } \frac{1}{-1}=1+2+2^2+\cdots
        </me>.
    </p>
    <aside>
      <title>
        Formal Manipulation
      </title>
      
      <p>
        When we say <q>formal manipulations</q> we mean that we will
        perform purely algebraic operations on an given expression
        without concerning ourselves (much) about whether the
        operations make sense in context. We will formalize them in <xref ref="TaylorSeries"></xref> 
      </p>
    </aside>
    <p>
      A power series representation of our function seems to work in some
      cases, but not in others. Obviously we are missing something
      important here, though it may not be clear exactly what. For
      now, we will continue to follow the example of our 18th century
      predecessors.  That is, for the rest of this section we will
      use formal manipulations to obtain and use power series
      representations of various functions.  Keep in mind that this is
      all highly suspect until we can resolve problems like those
      we<rsq/>ve just seen.
    </p>


    <p>
      Power series became an important tool in analysis in the 1700s.
      By representing various functions as power series they could be
      dealt with as if they were (infinite) polynomials.  The
      following is an example.
    </p>

    <example>
      
      <idx><h><m>e^x</m></h><h>as the solution of an Initial Value Problem</h></idx>

      <statement>
        <p>
          Solve the following Initial Value problem: Find
          <m>y(x)</m> given that 

          <md>
            <mrow>\frac{\dx{ y}}{\dx{
            x}}=y,\amp{}\amp{}y(0)=1.</mrow>

          </md>
        </p>
        <aside>
          
          <p>
            A few seconds of thought should convince you that the
            solution of this problem is <m>y(x) = e^x</m>.  We
            will ignore this for now in favor of emphasising the
            technique.
          </p>  
        </aside>


        <p>
          Assuming the solution can be expressed as a power series
          we have <me> y=\sum_{n=0}^\infty
          a_nx^n=a_0+a_1x+a_2x^2+\cdots </me>.
        </p>

        <p>
          Differentiating gives us <me> \frac{\dx{ y}}{\dx{
          x}}=a_1+2a_2x+3a_3x^2+4a_4x^3+\ldots </me>.
        </p>

        <p>
          Since <m>\frac{\dx{ y}}{\dx{ x}}=y</m> we see that <me>
          a_1=a_0\,,\,2a_2=a_1\,,\,3a_3=a_2\,,\,\ldots,\,na_n=a_{n-1}\,,\ldots
          </me>.
        </p>

        <p>
          This leads to the relationship <me>
          a_n=\frac{1}{n}a_{n-1}=\frac{1}{n(n-1)}a_{n-2}=\cdots=\frac{1}{n!}a_0
          </me>.
        </p>

        <p>
          Thus the power series solution of the differential equation is
          <me>
            y=\sum_{n=0}^\infty\frac{a_0}{n!}x^n
            =a_0\sum_{n=0}^\infty\frac{1}{n!}x^n </me>.
        </p>

        <p>
          Using the initial condition <m>y(0)=1</m>, we get
          <m>1=a_0(1+0+\frac{1}{2!}0^2+\cdots)=a_0</m>.  Thus the
          solution to the initial problem is
          <m>y=\sum_{n=0}^\infty\frac{1}{n!}x^n</m>.  Let<rsq/>s call
          this function <m>E(x)</m>.  Then by definition 
          <men xml:id="EQUATIONExpSeries">
            E(x)=\sum_{n=0}^\infty\frac{1}{n!}x^n=1+\frac{x^1}{1!}+\frac{x^2}{2!}+\frac{x^3}{3!}+\,\ldots
            </men>.
        </p>
      </statement>
    </example>

    <p>
      Let<rsq/>s examine some properties of this function.  The first
      property is clear from the definition.
    </p>

    <p>
      <term>Property 1</term>. 
      <m>E(0)=1</m>
    </p>

    <p>
      <term>Property 2</term>. 
      <m>E(x+y)=E(x)E(y)</m>.
    </p>

    <p>
      To see this we multiply the two series together, so we have
      <mdn>
        <mrow  xml:id="eq_ExponentAdditionProperty">E(x)E(y) =\amp \left(\sum_{n=0}^\infty\frac{1}{n!}x^n\right)\left(\sum_{n=0}^\infty\frac{1}{n!}y^n\right)</mrow>
        <mrow number="no">=\amp \left(\frac{x^0}{0!}+\frac{x^1}{1!}+\frac{x^2}{2!}+\frac{x^3}{3!}+\,\ldots\right)\left(\frac{y^0}{0!}+\frac{y^1}{1!}+\frac{y^2}{2!}+\frac{y^3}{3!}+\,\ldots\right)</mrow>
        <mrow number="no">=\amp \frac{x^0}{0!}\frac{y^0}{0!}+\frac{x^0}{0!}\frac{y^1}{1!}+\frac{x^1}{1!}\frac{y^0}{0!}+\frac{x^0}{0!}\frac{y^2}{2!}+\frac{x^1}{1!}\frac{y^1}{1!}+\frac{x^2}{2!}\frac{y^0}{0!}</mrow>
        <mrow number="no">\amp{}\ \ \         +\frac{x^0}{0!}\frac{y^3}{3!}+\frac{x^1}{1!}\frac{y^2}{2!}+\frac{x^2}{2!}\frac{y^1}{1!}+\frac{x^3}{3!}\frac{y^0}{0!}+\,\ldots</mrow>
        <mrow number="no">=\amp\frac{x^0}{0!}\frac{y^0}{0!}+\left(\frac{x^0}{0!}\frac{y^1}{1!}+ \frac{x^1}{1!}\frac{y^0}{0!}\right)</mrow>
        <mrow number="no">\amp{}\ \ \         +\left(\frac{x^0}{0!}\frac{y^2}{2!}+\frac{x^1}{1!}\frac{y^1}{1!}+\frac{x^2}{2!}\frac{y^0}{0!}\right)</mrow>
        <mrow number="no">\amp{}\ \ \ \ \ \        +\left(\frac{x^0}{0!}\frac{y^3}{3!}+\frac{x^1}{1!}\frac{y^2}{2!}+\frac{x^2}{2!}\frac{y^1}{1!}+\frac{x^3}{3!}\frac{y^0}{0!}\right)+\,\ldots</mrow>
        <mrow number="no">=\amp\frac{1}{0!}+\frac{1}{1!}\left(\frac{1!}{0!1!}x^0y^1+\frac{1!}{1!0!}x^1y^0\right)</mrow>
        <mrow number="no">\amp{}\ \ \        +\frac{1}{2!}\left(\frac{2!}{0!2!}x^0y^2+\frac{2!}{1!1!}x^1y^1+\frac{2!}{2!0!}x^2y^0\right)</mrow>
        <mrow number="no">\amp{}\ \ \ \ \ \        +\frac{1}{3!}\left(\frac{3!}{0!3!}x^0y^3+\frac{3!}{1!2!}x^1y^2+\frac{3!}{2!1!}x^2y^1+\frac{3!}{3!0!}x^3y^0\right)+\ldots</mrow>
        <mrow number="no"> =\amp \frac{1}{0!}+\frac{1}{1!}\left(\binom{1}{0}x^0y^1+\binom{1}{1}x^1y^0\right)</mrow>
        <mrow number="no">\amp{}\ \ \        +\frac{1}{2!}\left(\binom{2}{0}x^0y^2+\binom{2}{1}x^1y^1+\binom{2}{2}x^2y^0\right)</mrow>
        <mrow number="no">\amp{}\ \ \ \ \ \        +\frac{1}{3!}\left(\binom{3}{0}x^0y^3+\binom{3}{1}x^1y^2+\binom{3}{2}x^2y^1+\binom{3}{3}x^3y^0\right)+\ldots</mrow>
        <mrow number="no">=\amp \frac{1}{0!}+\frac{1}{1!}\left(x+y\right)^1+\frac{1}{2!}\left(x+y\right)^2+\frac{1}{3!}\left(x+y\right)^3+\ldots</mrow> 
      </mdn>

      so that, finally

      <men>E(x)E(y) =  E(x+y)</men>.

    </p>

    <p>
      <term>Property 3</term>. If <m>m</m> is a positive integer then <m>E(mx)=\left(E(x\right))^m</m>.
      In particular, <m>E(m)=\left(E(1)\right)^m</m>.
    </p>

    <problem>
      <statement>
        <p>
          Prove Property 3.
        </p>
      </statement>
    </problem>

    <p>
      <term>Property 4</term>. <m>E(-x)=\frac{1}{E(x)}=\left(E(x)\right)^{-1}</m>.
    </p>

    <problem>
      <statement>
        <p>
          Prove Property 4.
        </p>
      </statement>
    </problem>

    <p>
      <term>Property 5</term>. If <m>n</m> is an integer with
      <m>n\neq 0</m>, then
      <m>E(\frac{1}{n})=\sqrt[n]{E(1)}=\left(E(1)\right)^{1/n}</m>.
    </p>

    <problem>
      <statement>
        <p>
          Prove Property 5.
        </p>
      </statement>
    </problem>

    <p>
      <term>Property 6</term>. If <m>m</m> and <m>n</m> are
      integers with <m>n\neq 0</m>, then
      <m>E\left(\frac{m}{n}\right)=\left(E(1)\right)^{m/n}</m>.
    </p>

    <problem>
      <statement>
        <p>
          Prove Property 6.
        </p>
      </statement>
    </problem>

    <definition xml:id="def_e">
      
      <idx><h><m>e^x</m></h><h>definition</h></idx>
      <idx><h>Definition</h><h><m>e^x</m></h></idx>

      <statement>
        <p>
          Let <m>E(1)</m> be denoted by the number <m>e</m>.
          Using the power series <m>e=E(1)=\sum_{n=0}^\infty\frac{1}{n!}</m>,
          we can approximate <m>e</m> to any degree of accuracy.
          In particular <m>e\approx 2.71828</m>.
        </p>
      </statement>
    </definition>

    <p>
      In light of Property 6, we see that for any rational number
      <m>r</m>, <m>E(r)=e^r</m>.  Not only does this give us the
      power series representation
      <m>e^r=\sum_{n=0}^\infty\frac{1}{n!}r^n</m> for any rational
      number <m>r</m>, but it gives us a way to define <m>e^x</m>
      for irrational values of <m>x</m> as well.  That is, we can
      define
      <me>
        e^x=E(x)=\sum_{n=0}^\infty\frac{1}{n!}x^n
      </me>
      for any real number <m>x</m>.
    </p>

    <p>
      As an illustration, we now have
      <m>e^{\sqrt{2}}=\sum_{n=0}^\infty\frac{1}{n!}\left(\sqrt{2}\right)^n</m>.
      The expression <m>e^{\sqrt{2}}</m> is meaningless if we try
      to interpret it as one irrational number raised to another.
      What does it mean to raise anything to the <m>\sqrt{2}</m>
      power?  However the power series
      <m>\sum_{n=0}^\infty\frac{1}{n!}\left(\sqrt{2}\right)^n</m>
      does seem to have meaning and it can be used to extend the
      exponential function to irrational exponents.  In fact,
      defining the exponential function via this power series answers
      the question we raised in <xref
      ref="NumbersRealRational"></xref>: What does
      <m>4^{\sqrt{2}}</m> mean?

      It means 
      <me>
        4^{\sqrt{2}} = e^{\sqrt{2}\log 4} =
        \sum_{n=0}^\infty\frac{(\sqrt{2}\log 4)^n}{n!}  </me>.
    </p>

    <p>
      This may seem to be the long way around just to define
      something as simple as exponentiation.  But that is a
      fundamentally misguided attitude.  Exponentiation only
      <em>seems</em> simple because we<rsq/>ve always thought of it as
      repeated multiplication (in <m>\ZZ</m>) or root<ndash/>taking (in
      <m>\QQ</m>).  When we expand the operation to the real
      numbers this simply can<rsq/>t be the way we interpret something
      like <m>4^{\sqrt{2}}</m>.  How do you take the product of
      <m>\sqrt{2}</m> copies of <m>4?</m> The concept is
      meaningless.  What we need is an interpretation of
      <m>4^{\sqrt{2}}</m> which is consistent with, say <m>4^{3/2}
      = \left(\sqrt{4}\right)^3=8</m>.  This is exactly what the
      power series representation of <m>e^x</m> provides.
    </p>

    <p>
      We also have a means of computing integrals as power series.  For
      example, the famous <q>bell shaped</q> curve given by the
      function <m>f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}</m>
      is of vital importance in statistics as it must be integrated
      to calculate probabilities.  The power series we developed
      gives us a method of integrating this function.  For example,
      we have
      <md>
        <mrow>\int_{x=0}^b\frac{1}{\sqrt{2\pi}}
        e^{-\frac{x^2}{2}}\dx{x} \amp =\frac{1}{\sqrt{2\pi}}
        \int_{x=0}^b \left(\sum_{n=0}^\infty
        \frac{1}{n!}\left(\frac{-x^2}{2}\right)^n\right)\dx{x}
        </mrow>
        <mrow>
          \amp = \frac{1}{\sqrt{2\pi}} \sum_{n=0}^\infty
          \left(
          \frac{\left(-1\right)^n}{n!2^n}\int_{x=0}^bx^{2n}\dx{x}
          \right)
        </mrow>
        <mrow>
        \amp =\frac{1}{\sqrt{2\pi}}\,\sum_{n=0}^\infty\left(\frac{\left(-1\right)^nb^{2n+1}}{n!2^n\left(2n+1\right)}\right)</mrow>
        </md>.
    </p>

    <p>
      This power series can be used to approximate the integral to any
      degree of accuracy.
    </p>

    <problem>
      <idx><h>Problem</h><h>expand <m>e^{-x^3}</m> as a power series</h></idx>

      <statement>
        <p>
          Write <m>e^{-x^3}</m> as a power series expanded about
          <m>0</m> and use your series to represent
          <me>\int^b_{x=0}{e^{-x^3}\dx{x}}</me> 
          as a power series.
        </p>
      </statement>
    </problem>    

    <problem>
      <idx><h>Problem</h><h> power series expansion of <m>a^x</m></h></idx>

      <statement>
        <p>
          Let <m>a>0</m>.  Find a power series expansion about <q>0</q>  for <m>a^x</m>

        </p>

      </statement>

      <hint>

        <p>
          <m>a^x=e^{\ln a^x }</m>
        </p>

      </hint>
    </problem>


    <problem xml:id="PROBLEMSinPwrSeries">
      <idx><h>Problem</h><h>series</h><h>solutions of <m>\frac{\dx^2y}{\dx{ x}^2}=-y</m></h></idx>
      <idx><h>series</h><h>solutions of <m>\frac{\dx^2y}{\dx{ x}^2}=-y</m></h></idx>

      <introduction>
        <p>
          The ability to express complex functions as <term>power
          series</term> (<q>infinite polynomials</q>) became a tool of
          paramount importance for solving differential equations in
          the 1700s.
        </p>

      </introduction>

      <task>
        <statement>
          <p>
            Show that if <m>y=\sum_{n=0}^\infty a_nx^n</m>
            satisfies the differential equation
            <men xml:id="EQUATIONSinCosDiffeq">
              \frac{\dx^2y}{\dx{ x}^2}=-y
              </men>, then
              <me>
                a_{n+2}=\frac{-1}{\left(n+2\right)\left(n+1\right)}\,a_n
              </me>
              and conclude that

              <md>
                <mrow>
                  y=a_0+a_1x-\amp{}\frac{1}{2!}a_0x^2-\frac{1}{3!}a_1x^3+\frac{1}{4!}a_0x^4+\frac{1}{5!}a_1x^5
                </mrow>
                <mrow>
                  \amp{}-\frac{1}{6!}a_0x^6-\frac{1}{7!}a_1x^7+\cdots.
                </mrow>

              </md>
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Since <m>y=\sin x</m> also satisfies <xref ref="EQUATIONSinCosDiffeq" >equation</xref>, we see that
            <md>
              <mrow>
                \sin x=a_0+a_1x-\frac{1}{2!}a_0x^2-\amp{}\frac{1}{3!}a_1x^3+\frac{1}{4!}a_0x^4+\frac{1}{5!}a_1x^5
              </mrow>
              <mrow>
                \amp{}-\frac{1}{6!}\,a_0x^6-\frac{1}{7!}\,a_1x^7+\cdots
              </mrow>

            </md>
            for some constants <m>a_0</m> and <m>a_1</m>.  Show
            that in this case <m>a_0=0</m> and <m>a_1=1</m> and
            obtain 

            <md>
              <mrow> \sin
              x\amp{}=x-\frac{1}{3!}\,x^3+\frac{1}{5!}x^5-\frac{1}{7!}x^7+\cdots</mrow>
              <mrow>
                \amp{}=\sum_{n=0}^\infty\frac{\left(-1\right)^n}{\left(2n+1\right)!}x^{2n+1}.
              </mrow>

            </md>
          </p>
        </statement>
      </task>
    </problem>

    <problem xml:id="PROBLEMSinCosPwrSeries">
      <idx><h>Problem</h><h>differentiation</h><h>of <m>\sin x</m> as a power series</h></idx>
      <idx><h><m>\sin x</m></h><h>derivative of the power series representation</h></idx>
      <idx><h>differentiation</h><h>of <m>\sin x</m> as a power series</h></idx>

      <task>
        <statement>
          <p>
            Use the power series

            <md>
              <mrow>
                \sin x\amp{}=x-\frac{1}{3!}\,x^3+\frac{1}{5!}x^5-\frac{1}{7!}x^7+\cdots
              </mrow>
              <mrow>
                \amp{}=\sum_{n=0}^\infty\frac{\left(-1\right)^n}{\left(2n+1\right)!}x^{2n+1}
              </mrow>

            </md>
            to obtain the power series

            <md>
              <mrow>
                \cos
                x\amp{}=1-\frac{1}{2!}\,x^2+\frac{1}{4!}x^4-\frac{1}{6!}x^6+\cdots
              </mrow>
              <mrow>
                \amp{}=\sum_{n=0}^\infty\frac{\left(-1\right)^n}{\left(2n\right)!}x^{2n}.
              </mrow>

              </md> </p> </statement> </task> <task> <statement> <p> Let

              <me>
                s(x,N)=\sum_{n=0}^N\frac{\left(-1\right)^n}{\left(2n+1\right)!}x^{2n+1}
              </me> 

              and 

              <me>
                c(x,N)=\sum_{n=0}^N\frac{\left(-1\right)^n}{\left(2n\right)!}x^{2n}
              </me> 

              and use a computer algebra system to plot these on the
              interval <m>-4\pi\leq x\leq 4\pi</m>, for <m> N=1,2,5,10,
              15</m>.  Describe what is happening to the graph of the
              power series as <m>N</m> becomes larger.  
            </p> 
          </statement>
        </task> 
    </problem>

    <problem>
      <idx><h>Problem</h><h>power series expansion of <m>\sin x</m></h></idx>


      <statement>

        <p>
          Use the power series for <m>\sin (x)</m> to compute
          
          <me>
            \int^b_{x=0}\sin\left(x^2\right)\dx{x}
          </me>
          as a power series.

        </p>

      </statement>
    </problem>

    <problem xml:id="PROBLEMResultsFromGeometricSeries">
      <idx><h>Problem</h><h>power series expansion of <m>\arctan x</m></h></idx>

      
      <idx><h>series</h><h>Geometric series</h><h>used to derive arctangent series</h></idx>
      <idx><h>series</h><h>of <m>\arctan x</m></h></idx>

      <task>
        <statement>
          <p>
            Use the <xref ref="EQUATIONGeometricSeries" text="custom" >Geometric series</xref> 
            to obtain a power series for 
            <me>
              f(x)=\frac{1}{1+x}
              </me>,
              and use your series to show that 
              <m>
                \sum_{n=0}^\infty \frac{(-1)^n}{2^n} = \frac23 
                </m>.
          </p>
        </statement>
      </task>
      <task>
        <statement>
          
          <p>
            Use the power series you found  in part (a) 
            to obtain a power series for         
            <me>
              f(x)=\frac{1}{1+x^2}
              </me>,
              and use your series to show that <m>\sum_{n=0}^\infty
              \frac{(-1)^n}{2^{2n}}=\frac45</m>.
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Use the result in part (b) to obtain the power series

            <me>
              \arctan
              x=x-\frac{1}{3}x^3+\frac{1}{5}x^5-\cdots
              =\sum_{n=0}^\infty(-1)^n
              \frac{1}{2n+1}x^{2n+1}
              </me>, and use the series to show that
              <m>
                \sum_{n=0}^\infty\frac{(-1)^n}{2n+1} = \frac{\pi}{4} </m>.
          </p>
        </statement>
      </task>
    </problem>

    <!-- <problem> -->

    <!--   <statement> -->
    <!--     <p> -->
    <!--       Compute  -->
    <!--       <me>\int_{x=0}^1 \frac{1}{1+x^2}\dx{x}</me> -->
    <!--       as a power series. -->
    <!--     </p> -->

    <!--   </statement> -->
    <!-- </problem> -->

    <problem xml:id="PROBLEMLnSeriesFromGeoSeries">
      <idx><h>Problem</h><h>Alternating Geometric series</h></idx>
      <idx><h>series</h><h>Geometric series</h><h>alternating</h></idx>
      <idx><h>series</h><h>Geometric series</h><h>derivation of the series representation of <m>\ln(1+x)</m> from</h></idx>

      <task>
        <statement>
          <p>
            Use the <xref ref="EQUATIONGeometricSeries" text="custom"
            >Geometric series</xref> to obtain the power series
            <md>
              <mrow>\ln \left(1+x\right)\amp =x-\frac{1}{2}x^2+\frac{1}{3}x^3-\cdots</mrow>
              <mrow>\amp =\sum_{n=0}^\infty\frac{(-1)^n}{n+1}x^{n+1}.{}</mrow>
            </md>
          </p>

        </statement>
        <hint>
          <p>
            Recall that <m>\ln(1+x) = \int \frac{1}{1+x}\dx{x}</m>.
          </p>
        </hint>
      </task>
      <!-- <task> -->
      <!--   <statement> -->
      <!--     <p> -->
      <!--       Use the result of the previous problem to represent the -->
      <!--       function         <m>\ln \left(1+x^2\right)</m> as power series expanded about <m>0</m>. -->
      <!--     </p> -->
      <!--   </statement> -->
      <!-- </task> -->
      <task>
        <statement>
          <p>
            Use the result of part (a) to represent the function <m>\ln
            \left(1+x^2\right)</m> as a power series expanded about
            <m>0</m>.
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Use the result of part (a) represent the function <m>\ln
            \left(2+x\right)</m> as power series expanded about <m>0</m>.
          </p>
        </statement>
        <hint>
          <p>

            <m>2+x=2\left(1+\frac{x}{2}\right)</m>
          </p>

        </hint>
      </task>
    </problem>


    <problem>
      <idx><h>Problem</h><h>Alternating Geometric series</h></idx>

      <statement>
        <p>
          Use the <xref ref="EQUATIONGeometricSeries" text="custom"
          >Geometric series</xref> to find a power series
          representation for <m>\frac{2x}{1+x^2}</m>.  Integrate
          this to obtain a power series representation for
          <m>\ln\left(1+x^2\right)</m> and compare your answer to
          part (b) of the previous problem. (This shows that there
          may be more than one way to obtain a power series
          representation.)
        </p>
      </statement>
    </problem>

    <p>
      <idx><h>Leibniz, Gottfried Wilhelm</h></idx>


      The power series for arctangent was known by <url
      href="https://mathshistory.st-andrews.ac.uk/Biographies/Gregory/"
      >James
      Gregory</url> (1638-1675) and it is sometimes referred to as
      <q>Gregory<rsq/>s series.</q> Leibniz independently discovered
      <m>\frac{\pi}{4}=1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\cdots</m> by
      examining the area of a circle.  Though it gives us a means for
      approximating <m>\pi</m> to any desired accuracy, the power series
      converges too slowly to be of any practical use.  For example, if we
      compute the sum of the first <m>1000</m> terms we get
      <me>
        4\left(\sum_{n=0}^{1000}(-1)^n\frac{1}{2n+1}\right)\approx 3.142591654
      </me>
      which only approximates <m>\pi</m> to two decimal places.
    </p>

    <figure  xml:id="FIGUREJamesGregoryPortrait">
      <caption>James Gregory</caption>

      <idx><h>Portraits</h><h>Gregory</h></idx>
      <idx><h>Gregory, James</h><h>portrait of</h></idx>

      <image source="images/JamesGregory.png" width="35%">
        <shortdescription>Portrait of James Gregory</shortdescription>
      </image>
    </figure>

    <p>
      <idx><h>Newton, Isaac</h></idx>

      Newton knew of these results and the general scheme of using power
      series to compute areas under curves. He used these results to
      provide a power series approximation for <m>\pi</m> as well, which,
      hopefully, would converge faster.  We will use modern terminology to
      streamline Newton<rsq/>s ideas.  First notice that
      <m>\frac{\pi}{4}=\int_{x=0}^1\sqrt{1-x^2}\dx{ x}</m> as this
      integral gives the area of one quarter of the unit circle,
      <m>\frac{\pi }{4}</m>.  The trick now is to find a power series that
    represents <m>\sqrt{1-x^2}</m>.  </p>

    <p>
      To this end we start with the binomial theorem
      <me>
        \left(a+b\right)^N=\sum_{n=0}^N\binom{N}{n}a^{N-n}b^n
        </me>,
        where
        <md>
          <mrow>\binom{N}{n}\amp =\frac{N!}{n!\left(N-n\right)!}</mrow>
          <mrow>\amp =\frac{N\left(N-1\right)\left(N-2\right)\cdots\left(N-n+1\,\right)}{n!}</mrow>
          <mrow>\amp =\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}</mrow>
          </md>.
    </p>

    <p>
      Unfortunately, we now have a small problem with our notation
      which will be a source of confusion later if we don<rsq/>t fix
      it.  So we will pause to address this matter.  We will come
      back to the binomial expansion afterward.
    </p>

    <p>
      This last expression is becoming awkward in much the same
      way that an expression like
      <me>
        1+\frac{1}{2}+\left(\frac{1}{2}\right)^2+\left(\frac{1}{2}\right)^3+\ldots+\left(\frac{1}{2}\right)^k
      </me>
      is awkward.  Just as this sum is less cumbersome when
      written as <m>\sum_{n=0}^k\left(\frac{1}{2}\right)^n</m> the
      <em>product</em>
      <me>
        N\left(N-1\right)\left(N-2\right)\cdots\left(N-n+1\,\right)
      </me>
      is less cumbersome when we write it as
      <m>\prod_{j=0}^{n-1}\left(N-j\right)</m>.
    </p>

    <p>
      A capital pi <m>\left(\Pi\right)</m> is used to denote a product
      in the same way that a capital sigma <m>\left(\Sigma\right)</m>
      is used to denote a sum.  The most familiar example would be
      writing
      <me>
        n!=\prod_{j=1}^{n}j
        </me>.
    </p>

    <p>
      Just as it is convenient to define <m>0!=1</m>, we will find
      it convenient to define <me>\prod_{j=1}^{0}\left(\text{whatever}\right)=1</me>.
      Similarly, the fact that <m>\binom{N}{0}=1</m> leads to the
      convention <m>\prod_{j=0}^{-1}\left(N-j\right)=1</m>.
      Strange as this may look, it is convenient and is
      consistent with the convention <m>\sum_{j=0}^{-1}s_j=0</m>.
    </p>

    <p>
      Returning to the binomial expansion and recalling our convention
      <me>
        \prod_{j=0}^{-1}\left(N-j\right)=1
        </me>,
        we can write,
        <me>
          \left(1+x\right)^N=1+\sum_{n=1}^N\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\right)x^n = \sum_{n=0}^N\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\right)x^n
          </me>.
    </p>
    <p>
      These two representations may  look slightly different to you at first.
      Be sure that you see that they are really the same before reading further.  
    </p> 

    <p>
      There is an advantage to using this convention (especially
      when programing a product into a computer), but this is not
      a deep mathematical insight.  It is just a notational
      convenience and we don<rsq/>t want you to fret over it, so we
      will use both formulations (at least initially).
    </p>

    <p>
      Notice that we can extend the above definition of
      <m>\binom{N}{n}</m> to values <m>n>N</m>.  In this case,
      <m>\prod_{j=0}^{n-1}\left(N-j\right)</m> will equal 0 as one
      of the factors in the product will be <m>0</m> (the one
      where <m>j=N</m>).  This gives us that <m>\binom{N}{n}=0</m>
      when <m>n>N</m> and so
      <me>
        \left(1+x\right)^N=1+\sum_{n=1}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\text{ } \right)x^n= \sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\text{ } \right)x^n
      </me>
      holds true for any nonnegative integer <m>N</m>.
      Essentially Newton asked if it could be possible that the
      above equation could hold values of <m>N</m> which are not
      nonnegative integers.  For example, if the equation held
      true for <m>N=\frac{1}{2}</m> , we would obtain
      <me>
        \left(1+x\right)^{\frac{1}{2}}=1+\sum_{n=1}^\infty\left(\frac{ \prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\right)x^n=\sum_{n=0}^\infty\left(\frac{ \prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\right)x^n
      </me>
      or
      <men xml:id="eq_BinomialSeries">
        \left(1+x\right)^{\frac{1}{2}}=1+\frac{1}{2}x+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)}{2!}x^2+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right)}{3!}x^3+\cdots
        </men>.
    </p>

    <p>
      Notice that since <m>\frac{1}{2}</m> is not an integer the power
      series no longer terminates.  Although Newton did not prove that
      this power series was correct (nor did we), he tested it by
      multiplying the power series by itself.  When he saw that by
      squaring the power series he started to obtain
      <m>1+x+0\,x^2+0\,x^3+\cdots</m>, he was convinced that the power
      series was exactly equal to <m>\sqrt{1+x}</m>.
    </p>

    <problem>
      <idx><h>Problem</h><h>Binomial series</h></idx>
      <idx><h>Binomial Series, the</h><h>squaring the</h></idx>

      <statement>
        <p>
          Consider the power series representation
          <md>
            <mrow>\left(1+x\right)^{\frac{1}{2}}\amp =1+\sum_{n=1}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n</mrow>
            <mrow>\amp  =\sum_{n=0}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n</mrow>
            </md>.
        </p>

        <p>
          Multiply this power series by itself and compute the
          coefficients for <m>x^0,\,x^1,\,x^2,\,x^3,\,x^4</m> in
          the resulting power series.
        </p>
      </statement>
    </problem>

    <problem xml:id="prob_SqrtSeriesProb">
      <idx><h>Problem</h><h>power series expansion of <m>\sqrt{1+x}</m></h></idx>

      
      <idx><h>series</h><h>graph the square root power series</h></idx>

      <statement>
        <p>
          Let
          <me>
            S(x,M)=\sum_{n=0}^M\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j \right)}{n!}x^n
            </me>.
        </p>

        <p>
          Use a computer algebra system to plot <m>S(x,M)</m> for
          <m>M=5, 10, 15, 95, 100</m> and compare these to the
          graph for <m>\sqrt{1+x}</m>.  What seems to be
          happening?  For what values of <m>x</m> does the power series
          appear to converge to <m>\sqrt{1+x}?</m>
        </p>
      </statement>
    </problem>

    <p>
      Convinced that he had the correct power series, Newton used it to
      find a power series representation of <m>\int_{x=0}^1\sqrt{1-x^2}
      \dx{ x}</m>.
    </p>

    <problem>
      <idx><h>Problem</h><h>power series expansion</h></idx>

      
      <idx><h><m>\pi</m></h><h>first series expansion</h></idx>

      <statement>
        <p>
          Use the power series <m>\displaystyle
          \left(1+x\right)^{\frac{1}{2}}=\sum_{n=0}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n</m>
          to obtain the power series
          <md>
            <mrow>\frac{\pi}{4}\amp =\int_{x=0}^1\sqrt{1-x^2} \dx{ x}</mrow>
            <mrow>\amp =\sum_{n=0}^\infty\left[\left(\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\text{ } \right)\left(\frac{\left(-1\right)^n}{2n+1}\right)\right]</mrow>
            <mrow>\amp =1-\frac{1}{6}-\frac{1}{40}-\frac{1}{112}-\frac{5}{1152}-\cdots</mrow>
            </md>.
        </p>

        <p>
          Use a computer algebra system to sum the first 100 terms
          of this power series and compare the answer to
          <m>\frac{\pi}{4}</m>.
        </p>
      </statement>
    </problem>

    <p>
      Again, Newton had a power series which could be verified
      (somewhat) computationally.  This convinced him even further
      that he had the correct power series.
    </p>

    <problem>
      <idx><h>Problem</h><h>power series expansion</h></idx>

      
      <idx><h><m>\pi</m></h><h>second series expansion</h></idx>

      <statement>
        <p>
          <ol marker="(a)">
            <li>
              <p>
                Show that
                <me>
                  \int_{x=0}^{1/2}\sqrt{x-x^2}\dx{ x}=\sum_{n=0}^\infty\frac{(-1)^n\,\,\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{\sqrt{2\,}n!\left(2n+3\right)2^n}
                </me>
                and use this to show that
                <me>
                  \pi=16\left(\sum_{n=0}^\infty\frac{(-1)^n\,\,\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{\sqrt{2\,}n!\left(2n+3\right)2^n}\right)
                  </me>.
              </p>
            </li>
            <li>
              <p>
                We now have two power series for calculating <m>\pi</m>:
                the one from part (a) and the one derived earlier,
                namely <me>
                \pi=4\left(\sum_{n=0}^\infty\frac{(-1)^n\,\,}{2n+1}\right)
                </me>.  We will explore which one converges to
                <m>\pi</m> faster. First define
                <me>
                  S1(N)=16\left(\sum_{n=0}^N\frac{(-1)^n\,\,\prod_{j=0}^{n-1}\left(
                  \frac{1}{2}-j\right)}{\sqrt{2\,}n!\left(2n+3\right)2^n}\right)
                </me>
                and
                <me>
                  S2(N)=4\left(\sum_{n=0}^N\frac{(-1)^n\,\,}{2n+1}\right)
                  </me>.
                  Use a computer algebra system to compute
                  <m>S1(N)</m>and <m>S2(N)</m> for
                  <m>N=5,10,15,20</m>.  Which one appears to
                  converge to <m>\pi</m> faster?
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </problem>

    <p>
      In general the power series representation
      <md>
        <mrow>\left(1+x\right)^\alpha \amp  =\sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(\alpha-j\right)}{n!}\text{ } \right)x^n</mrow>
        <mrow>\amp =1+\alpha x+\frac{\alpha\left(\alpha-1\right)}{2!}x^2+\frac{\alpha\left(\alpha-1\right)\left(\alpha-2\right)}{3!}x^3+\cdots</mrow>
      </md>
      is called the <term>binomial series</term> (or Newton<rsq/>s
      binomial series).  This power series is correct when <m>\alpha</m>
      is a non-negative integer (after all, that is how we got the
      series in the first place).  We can also see that it is correct when
      <m>\alpha=-1</m> as we obtain
      <md>
        <mrow>\left(1+x\right)^{-1}\amp =\sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(-1-j\right)}{n!}\text{ } \right)x^n</mrow>
        <mrow>\amp =1+(-1)x+\frac{-1\left(-1-1\right)}{2!}x^2+\frac{-1\left(-1-1\right)\left(-1-2\right)}{3!}x^3+\cdots</mrow>
        <mrow>\amp =1-x+x^2-x^3+\cdots</mrow>
      </md>
      which can be obtained from the geometric series
      <m>\frac{1}{1-x}=1+x+x^2+\cdots</m> .
    </p>

    <p>
      In fact, the binomial series is the correct power series
      representation for all values of the exponent <m>\alpha</m>
      (though we haven<rsq/>t proved this yet).
    </p>

    <problem>
      <idx><h>Problem</h><h>Binomial series</h></idx>

      <task>
        <statement>
          <p>
            Assuming that the binomial series works for <m>\alpha =-\frac{1}{2}</m>, show that

            <md>
              <mrow> 
                \frac{1}{\sqrt{1-x^2}}\amp{}=\sum^{\infty}_{n=0}{\frac{\left(\prod^{n-1}_{j=0}{\left(\frac{1}{2}+j\right)}\right)}{n!}x^{2n}}
              </mrow> 
              <mrow> 
                \amp{}
                =1+\frac{1}{2}x^2+\frac{\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)}{2!}x^4+\frac{\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)\left(\frac{5}{2}\right)}{3!}x^6+\dots.
              </mrow>

            </md>

          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Integrate the above to obtain the following power series for <m>\arcsin (x)</m>.

            <md>
              <mrow>
                \arcsin \left(x\right)\amp{}=\sum^{\infty
                }_{n=0}{\frac{\left(\prod^{n-1}_{j=0}{\left(\frac{1}{2}+j\right)}\right)}{n!\left(2n+1\right)}x^{2n+1}}
              </mrow>
              <mrow>
                \amp{}=x+\frac{\frac{1}{2}}{3}x^3+\frac{\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)}{2!5}x^5+\frac{\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)\left(\frac{5}{2}\right)}{3!7}x^7+\dots 
              </mrow>

            </md>


          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Substitute <m>x=\frac{1}{2}</m> into the above power series to
            obtain a power series representation for <m>\frac{\pi
            }{6}</m>.  Add the first four terms of this power series to obtain
            an approximation for <m>\pi </m>, and compare with <m>\pi
            \approx 3.14159265359</m>.  How close did your approximation
            come?
          </p>
        </statement>
      </task>
    </problem>

    <problem>
      <idx><h>Problem</h><h>Binomial series</h></idx>

      
      <idx><h>Binomial Series, the</h></idx>
      <idx><h>Binomial Series, the</h><h>as a power series centered at zero</h></idx>
      <idx><h>series</h><h>Geometric series</h><h>differentiating</h></idx>

      <introduction>
        <p>
          Let <m>k</m> be a positive integer.  Find the power
          series, centered at zero, for <m>f(x) =
          \left(1-x\right)^{-k}</m> by
        </p>
      </introduction>
      <task>
        <statement>
          <p>
            Differentiating the <xref ref="EQUATIONGeometricSeries"
            text="custom">Geometric series</xref> <m>\left(k-1\right)</m>
            times.
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Applying the binomial series.
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Compare the results in parts (a) and (b).
          </p>
        </statement>
      </task>
    </problem>


    <p>
      <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Euler/" >Leonhard Euler</url> (1707<ndash/>1783) was a master at exploiting power series.  In
      1735, the 28 year-old Euler won acclaim for what is now
      called the Basel problem: to evaluate the sum
      <me>
        \sum_{n=1}^\infty\frac{1}{n^2}
        </me>.  

        Other mathematicans had shown that the power series converged, but
        Euler was the first to find its exact value.  The following
        problem essentially provides Euler<rsq/>s solution.
    </p>
    <figure>
      <caption>Leonhard Euler</caption>

      <idx><h>Euler, Leonhard</h><h>portrait of</h></idx>
      <idx><h>Portraits</h><h>Euler</h></idx>

      <image width="35%" source="images/Euler.png" >
        <shortdescription>Portrait of Euler</shortdescription>
      </image>
    </figure>

    <problem>
      <title>The Basel Problem</title>

      <idx><h>Problem</h><h>the Basel Problem</h></idx>
      <idx><h>Euler, Leonhard</h><h>Basel Problem, the</h></idx>

      <introduction>
        <p>
          Recall that in <xref ref="PROBLEMSinCosPwrSeries" ></xref> we
          developed a power series representation of the function  <m>\sin x</m>.
        </p>
      </introduction>

      <task>
        <statement>
          <p>
            Show that the power series for <m>\frac{\sin x}{x}</m>
            is given by
            <m>1-\frac{1}{3!}x^2+\frac{1}{5!}x^4-\cdots</m>
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Use part (a) to infer that the roots of
            <m>1-\frac{1}{3!}x^2+\frac{1}{5!}x^4-\cdots</m> are
            given by
            <me>
              x=\pm\pi,\,\pm 2\pi,\,\pm 3\pi,\,\ldots
            </me>
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Suppose <m>p(x)=a_0+a_1x+\cdots+a_nx^n</m> is a
            polynomial with roots <m>r_1,\,r_2,\,\ldots,r_n</m>.
            Show that if <m>a_0\neq</m> <m>0</m>, then all the roots
            are non-zero and
            <me>
              p(x)=a_0\left(1-\frac{x}{r_1}\right)\left(1-\frac{x}{r_2}\right)\cdots\left(1-\frac{x}{r_n}\right)
              </me>.
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Assuming that the result in part (c) holds for an
            infinite polynomial (power series), deduce that
            <me>
              1-\frac{1}{3!}x^2+\frac{1}{5!}x^4-\cdots =
              \left(1-\left(\frac{x}{\pi}\right)^2\right)
              \left(1-\left(\frac{x}{2\pi}\right)^2\right)
              \left(1-\left(\frac{x}{3\pi}\right)^2\right)\cdots
            </me>
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Expand this product to deduce that
            <me>
              \sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6}.{}
            </me>
          </p>
        </statement>
      </task>
    </problem>

    <problem>
      <title>Euler<rsq/>s Formula</title>

      <idx><h>Problem</h><h>Euler<rsq/>s Formula</h></idx>
      <idx><h>Euler, Leonhard</h><h>Euler<rsq/>s Formula</h></idx>

      <task>
        <statement>
          <p>
            Use the power series expansion of <m>e^x</m>, <m>\sin
            x,</m> and <m>\cos x</m> to derive <term>Euler<rsq/>s
            Formula</term>:
            <me>
              e^{i\theta} = \cos\theta+i\sin\theta.
            </me>
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Use Euler<rsq/>s formula to derive the
            Addition/Subtraction formulas from Trigonometry:
            <me>
              \sin(\alpha\pm\beta) = \sin\alpha\cos\beta\pm\sin\beta\cos\alpha
            </me>
            <me>
              \cos(\alpha\pm\beta) = \cos\alpha\cos\beta\mp\sin\alpha\sin\beta
            </me>
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Use Euler<rsq/>s formula to show that
            <me>
              \sin 2\theta = 2\cos\theta\sin\theta
            </me>
            <me>
              \cos 2\theta =\cos^2\theta-\sin^2\theta
            </me>
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Use Euler<rsq/>s formula to show that
            <me>
              \sin 3\theta = 3\cos^2\theta\sin\theta-\sin^3\theta
            </me>
            <me>
              \cos 3\theta=\cos^3\theta-3\cos\theta\sin^2\theta
            </me>
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Find a formula <m>\sin(n\theta)</m> and
            <m>\cos(n\theta)</m> for any positive integer <m>n</m>.
          </p>
        </statement>
      </task>
    </problem>

  </section>

  <section xml:id="SECTIONPowSerWOTaylor">
    <title>Expanding Simple Power Series by Algebraic Methods</title>
    <p>
      We call the power series expansions we<rsq/>ll see in this section
      <q>simple</q> because all that is needed to generate them is prior
      knowledge of a few series (e.g.,the <xref
      ref="EQUATIONGeometricSeries" text="custom">Geometric
      Series</xref>, the <xref ref="PROBLEMSinCosPwrSeries"
      text="custom">sine and cosine series</xref>, the <xref
      ref="EQUATIONExpSeries" text="custom">exponential series</xref>,
      the <xref ref="eq_BinomialSeries" text="custom" >Binomial
      Series</xref>), and a creative use of algebra. In particular <xref
      ref="TaylorsTheorem" text="custom">Taylor<rsq/>s Theorem</xref> is
      not needed. We assume that you are familiar with the use of
      Taylor<rsq/>s Theorem from your Calculus course.
    </p>

    <p>
      As we saw in the last section, it can be particularly fruitful to
      expand a function as a power series centered at <m>a=0</m>.
      Unfortunately, this isn<rsq/>t always possible. For example, it is
      not possible to expand the function
      <m>f\left(x\right)=\frac{1}{x}</m> about zero. (Why not?)
    </p>

    <p>
      But we are not confined to expanding about zero.  Consider that
      the following is a power series for <m>f(x)=\frac{1}{x}</m>
      expanded about <m>a=1</m>

      <me>
        \frac{1}{x}=\frac{1}{1+\left(x-1\right)}=\sum^{\infty}_{n=0}{-1^n{\left(x-1\right)}^n}
        </me>.


        Of course, there are still questions that need to be
        resolved. Chief among these is the question, <q>For which values
        of <m>x</m> is this series a valid representation of the function
        we started with?</q> We will explore this in <xref
        ref="PowerSeriesQuestions-TaylorsFormula"></xref>. For now we will
        content ourselves with having a representation which seems
        reasonable.
    </p>
    
    <problem xml:id="PROBLEMOneOverXTayl">
      <idx><h>Problem</h><h>power series expansion of <m>\frac1x </m></h></idx>

      <introduction>
        <p>
          Let <m>a\neq 0</m>.
        </p>
      </introduction>
      <task>
        <statement>
          <p>
            Represent <m>\frac{1}{x}</m> as a power
            series expanded about <m>a</m>.  That is, as a power series of the
            form <m>\sum^{\infty }_{n=0}{a_n{(x-a)}^n}</m>.
          </p>
        </statement>
        <hint>
          <p>
            <m>\frac{1}{x}=\frac{1}{a+x-a}=\frac{1}{a}\left(\frac{1}{1+\frac{x-a}{a}}\right)</m>
          </p>
        </hint>  

      </task>
      <task>
        <statement>
          <p>
            Represent <m>\ln (x)</m> as a power series expanded about <m>a</m> by integrating your solution to part (a).
          </p>
        </statement>
      </task>

      <!-- <task> -->
      <!--   <statement> -->
      <!--     <p> -->
      <!--       By integrating the answer from the previous problem. -->
      <!--     </p> -->
      <!--   </statement> -->
      <!-- </task> -->
      <!-- <task> -->
      <!--   <statement> -->
      <!--     <p> -->
      <!--       By writing <m>\ln (x) =\ln (a+x-a)</m> and using the (known) -->
      <!--       expansion of <m>\ln(x)</m>. -->
      <!--     </p> -->
      <!--   </statement> -->
      <!-- </task> -->

    </problem>
    

    <problem>
      <idx><h>Problem</h><h>power series expansion of <m>\sqrt{x}</m></h></idx>

      <p>
        Let <m>a>0</m> and use <xref ref="eq_BinomialSeries"
        >equation</xref> to represent <m>\sqrt{x}</m> as a power series
        expanded about <m>a</m>.
      </p>
    </problem>
    
    <problem xml:id="PROBLEMExpTaylAta">
      <idx><h>Problem</h><h>power series expansion of <m>e^x</m> at <m>a</m></h></idx>

      <p>
        Let <m>a</m> be a real number.  Represent <m>e^x</m> as power
        series expanded about <m>a</m>.  Notice there is no restriction
        on <m>a</m>.  
      </p>
    </problem>
    
    <problem>
      <idx><h>Problem</h><h>power series expansion of <m>x^3+2x^2+3</m></h></idx>

      <p>
        Let <m>a</m> be a real number.  Represent <m>x^3+2x^2+3</m> as a
        power series expanded about <m>a</m>.  What happens if <m>a=0</m>?
      </p>
    </problem>
    
    <problem xml:id="PROBLEMSinExpandedAta">
      <idx><h>Problem</h><h>power series expansion of <m>\sin x</m></h></idx>


      <statement>
        <p>
          Let <m>a</m> be a real number.  Use the power series expansions

          <md>

            <mrow>
              \sin \left(x\right)=\sum^{\infty
              }_{n=0}{\frac{{\left(-1\right)}^n}{\left(2n+1\right)!}x^{2n+1}}\amp{}\amp{} \cos\left(x\right)=\sum^{\infty
              }_{n=0}{\frac{{\left(-1\right)}^n}{\left(2n\right)!}x^{2n}}
            </mrow>

          </md>

          to obtain the power series representation

          <md>
            <mrow>
              \sin (x)=\sin (a)+\amp{}\cos \left(a\right)\left(x-a\right)-\frac{1}{2!}\sin
              \left(a\right){\left(x-a\right)}^2
            </mrow>
            <mrow>
              \amp{}-\frac{1}{3!}\cos
              \left(a\right){\left(x-a\right)}^3+\frac{1}{4!}\sin
              \left(a\right){\left(x-a\right)}^4
            </mrow>
            <mrow>
              \amp{}\ \ \ \ \ +\frac{1}{5!}\cos
              \left(a\right){\left(x-a\right)}^5+\cdots.
            </mrow>
          </md>

        </p>
        <p>
          This result will come into play in the next section.

        </p>

      </statement>
      <hint>
        <p>
          <m >\sin \left(x\right) =\sin \left(a+\left(x-a\right)\right)</m>
        </p>
      </hint>
    </problem>
  </section>


  <!-- <section xml:id="CalcIn17th18thCentury-AddProb"> -->
  <!--   <title>Additional Problems</title> -->
  <!--   <p> -->
  <!--     We assume that you were introduced to the Taylor<rsq/> series in -->
  <!--     your Calculus course. In the next chapter we will discuss -->
  <!--     Taylor<rsq/>s series in considerable detail. But since we have not -->
  <!--     done that yet you are not allowed to use Taylor<rsq/>s series to -->
  <!--     solve these problems. -->
  <!--   </p> -->



  <!--   <problem> -->
  <!--     <idx><h>power series</h><h> drills</h></idx> -->
  <!--     <introduction> -->

  <!--       <p> -->
  <!--         <em>Without</em> using Taylor<rsq/>s Theorem, represent the -->
  <!--         following functions as power series expanded about 0 -->
  <!--         (i.e., in the form <m>\sum_{n=0}^\infty a_nx^n</m>). -->
  <!--       </p> -->
  <!--     </introduction> -->
  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           Use the result of <xref -->
  <!--           ref="PROBLEMLnSeriesFromGeoSeries"></xref> to find the -->
  <!--           series representation of <me>\ln(1+x^2) </me>. -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--     </task> -->
  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           <m>f(x)=\ln\left(2+x\right)</m> -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--       <hint> -->
  <!--         <p> -->
  <!--           <m>2+x=2\left(1+\frac{x}{2}\right)</m> -->
  <!--         </p> -->
  <!--       </hint> -->
  <!--     </task> -->
  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           Use the Geometric Series to find the series representation of  -->
  <!--           <me>f(x) = \frac{1}{1+x^2}</me> -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--     </task> -->

  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           Use the result of part (c) to find the series representation of  -->
  <!--           <me> -->
  <!--             f(x)=\frac{x}{1+x^2}  -->
  <!--             </me>. -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--     </task>  -->

  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           Use the result of part (d) to find the series representation of  -->
  <!--           <!-\- <m>\ln\left(1-x^2\right)</m> -\-> -->
  <!--           <me> -->
  <!--             f(x)= \frac{1}{1-x^2} -->
  <!--             </me>. -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--     </task> -->
  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           Use the result of part (c) to find the series representation of  -->
  <!--           <me>\displaystyle f(x) =\arctan \left(x^3\right)</me> -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--     </task> -->


  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           <m>f(x)=a^x</m>, where <m>a\gt 0</m> -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--       <hint> -->
  <!--         <p> -->
  <!--           <m>a^x=e^{\ln\,\left(a^x\right)}</m> -->
  <!--         </p> -->
  <!--       </hint> -->
  <!--     </task> -->

  <!--   </problem> -->

  <!--   <!-\- <problem> -\-> -->
  <!--   <!-\-   <idx><h>power series</h><h>for <m>a^x</m> expanded about 0</h></idx> -\-> -->
  <!--   <!-\-   <statement> -\-> -->

  <!--   <!-\-     <p> -\-> -->
  <!--   <!-\-       Let <m>a</m> be a positive real number.  Find a power -\-> -->
  <!--   <!-\-       series for <m>a^x</m> expanded about 0. -\-> -->
  <!--   <!-\-     </p> -\-> -->
  <!--   <!-\-   </statement> -\-> -->
  <!--   <!-\-   <hint> -\-> -->
  <!--   <!-\-     <p> -\-> -->
  <!--   <!-\-       <m>a^x=e^{\ln\,\left(a^x\right)}</m> -\-> -->
  <!--   <!-\-     </p> -\-> -->
  <!--   <!-\-   </hint> -\-> -->
  <!--   <!-\- </problem> -\-> -->
  <!--   <!-\- <aside> -\-> -->
  <!--   <!-\-   <title>Note to Self</title> -\-> -->
  <!--   <!-\-   <p> -\-> -->
  <!--   <!-\-     Should this problem be in <xref ref="PowerSeriesQuestions-TaylorsFormula" ></xref> since that is where we give the Taylor expansion? -\-> -->
  <!--   <!-\-   </p> -\-> -->
  <!--   <!-\- </aside> -\-> -->

  <!--   <!-\- <problem> -\-> -->
  <!--   <!-\-   <idx><h>Taylor series drills</h></idx>  -\-> -->
  <!--   <!-\-   <introduction> -\-> -->
  <!--   <!-\-     <p> -\-> -->
  <!--   <!-\-       <em>Without</em> using Taylor<rsq/>s Theorem, represent the -\-> -->
  <!--   <!-\-       following functions as a power series expanded about -\-> -->
  <!--   <!-\-       <m>a</m> for the given value of <m>a</m> (i.e., in the -\-> -->
  <!--   <!-\-       form <m>\sum_{n=0}^\infty a_n\left(x-a\right)^n</m>). -\-> -->
  <!--   <!-\-     </p> -\-> -->
  <!--   <!-\-   </introduction> -\-> -->
  <!--   <!-\-   <task> -\-> -->
  <!--   <!-\-     <statement> -\-> -->
  <!--   <!-\-       <p> -\-> -->
  <!--   <!-\-         <m>\ln x</m>, <m>a=1</m> -\-> -->
  <!--   <!-\-       </p> -\-> -->
  <!--   <!-\-     </statement> -\-> -->
  <!--   <!-\-     <!-\\-         <hint> -\\-> -\-> -->
  <!--   <!-\-     <!-\\-           <p> -\\-> -\-> -->
  <!--   <!-\-     <!-\\-             Use <xref ref="PROBLEMLnSeriesFromGeoSeries" ></xref> and -\\-> -\-> -->
  <!--   <!-\-     <!-\\-             observe that <m>x=1+(x-1)</m>. -\\-> -\-> -->
  <!--   <!-\-     <!-\\-           </p> -\\-> -\-> -->
  <!--   <!-\-     <!-\\- </hint> -\\-> -\-> -->

  <!--   <!-\-   </task> -\-> -->

  <!--   <!-\-   <task> -\-> -->
  <!--   <!-\-     <statement> -\-> -->
  <!--   <!-\-       <p> -\-> -->
  <!--   <!-\-         <m>e^x</m>, <m>a=3</m> -\-> -->
  <!--   <!-\-       </p> -\-> -->
  <!--   <!-\-     </statement> -\-> -->
  <!--   <!-\-   </task> -\-> -->

  <!--   <!-\-   <task> -\-> -->
  <!--   <!-\-     <statement> -\-> -->
  <!--   <!-\-       <p> -\-> -->
  <!--   <!-\-         <m>x^3+2x^2+3</m> , <m>a=1</m> -\-> -->
  <!--   <!-\-       </p> -\-> -->
  <!--   <!-\-     </statement> -\-> -->
  <!--   <!-\-   </task> -\-> -->

  <!--   <!-\-   <task> -\-> -->
  <!--   <!-\-     <statement> -\-> -->
  <!--   <!-\-       <p> -\-> -->
  <!--   <!-\-         <m>\frac{1}{x}</m> , <m>a=5</m> -\-> -->
  <!--   <!-\-       </p> -\-> -->
  <!--   <!-\-     </statement> -\-> -->
  <!--   <!-\-   </task>  -\-> -->

  <!--   <!-\- </problem> -\-> -->

  <!--   <problem> -->
  <!--     <statement> -->
  <!--       <p> -->
  <!--         Expand the function -->
  <!--         <m>f(x)=x^3+2x^2+3</m> as a series about the point  <m>a=1</m>. -->
  <!--       </p> -->
  <!--     </statement> -->
  <!--     <hint> -->
  <!--       <p> -->
  <!--         Find the coefficients <m>A_0</m>, <m>A_1</m>, and <m>A_2</m> for which  -->
  <!--         <me> -->
  <!--           x^3+2x^2+3 = A_2(x-1)^2+A_1(x-1)+A_0 -->
  <!--         </me> -->
  <!--       </p> -->
  <!--     </hint> -->

  <!--   </problem> -->

  <!--   <problem> -->
  <!--     <idx><h>series</h><h>term by term integration of </h></idx> -->
  <!--     <introduction> -->
  <!--       <p> -->
  <!--         Evaluate the following integrals as series. That is, replace each integrand with it<rsq/>s series representation and compute the integral term<ndash/>by<ndash/>term. -->
  <!--       </p> -->
  <!--     </introduction> -->
  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           <m>\displaystyle\int_{x=0}^1e^{x^2}\dx{ x}</m> -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--     </task> -->
  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           <m>\displaystyle\int_{x=0}^1\frac{1}{1+x^4}\dx{ x}</m> -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--     </task> -->
  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           <m>            \displaystyle\int_{x=0}^1 \sqrt[3]{1-x^3}\dx{x}</m> -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--     </task> -->


  <!--   </problem> -->
  <!-- </section> -->


</chapter>

