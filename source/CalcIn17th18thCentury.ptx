<chapter xmlns:xi="http://www.w3.org/2001/XInclude"  xml:id="CalcIn17th18thCentury">
  <title>Calculus in the 17th and 18th Centuries</title>
  <section xml:id="CalcIn17th18thCentury-NewtLeibStart">
    <title>Newton and Leibniz Get Started</title>
    <subsection xml:id="sec_leibn-calc-rules">
      <title>Leibniz<rsq/> Calculus Rules</title>

      <p>
        <idx><h>Leibniz, Gottfried Wilhelm</h><h>first Calculus
        publication</h></idx>

        The rules for Calculus were first laid out in the 1684 paper
        <foreign>Nova methodus pro maximis et minimis, itemque
        tangentibus, quae nec fractas nec irrationales, quantitates
        moratur, et singulare pro illi calculi genus</foreign> (A New
        Method for Maxima and Minima as Well as Tangents, Which is
        Impeded Neither by Fractional Nor by Irrational Quantities,
        and a Remarkable Type of Calculus for This) written by <url
        href="https://mathshistory.st-andrews.ac.uk/Biographies/Leibniz/"
        visual="mathshistory.st-andrews.ac.uk/Biographies/Leibniz/">Gottfried
        Wilhelm Leibniz</url> (1646<ndash/>1716).  Leibniz started
        with subtraction.  That is, if <m>x_1</m> and <m>x_2</m> are
        very close together then their difference, <m>\Delta
        x=x_2-x_1</m>, is very small.  He expanded this idea to say
        that if <m>x_1</m> and <m>x_2</m> are <em>infinitely</em>
        close together (but still distinct) then their difference
        <m>\dx{ x}</m>, is infinitesimally small (but not zero).
      </p>

      <figure>
        <caption>
          Gottfried Wilhelm Leibniz
        </caption>
          <idx><h>Leibniz, Gottfried Wilhelm</h><h>portrait of</h></idx>
          <image width="35%" source="images/Leibniz.png" >
            <shortdescription>Portrait of Gottfried Wilhelm Leibniz</shortdescription>
          </image>
      </figure>
      <aside>
        <title>
          <foreign>Calculus Differentialis</foreign>
        </title>
        <p>
          This translates, loosely, as the
          <q>Calculus of Differences</q>.
        </p>  
      </aside>


      <p>
        This idea is logically very suspect and Leibniz knew it.  But
        he also knew that when he used his <foreign>calculus
        differentialis</foreign> he was getting correct answers
        to some very hard problems.  So he persevered.
      </p>

      <p>
        Leibniz called both <m>\Delta x</m> and <m>\dx{ x}</m>
        differentials (Latin for difference) because he thought
        of them as, essentially, the same thing.  Over time it has
        become customary to refer to the infinitesimal <m>\dx{ x}</m>
        as a differential, and to reserve the word difference and the
        notation <m>\Delta x</m> for the finite case.  This is why
        Calculus is often called <em>Differential Calculus</em>.
      </p>

      <p>
        In his paper Leibniz gave rules for dealing with these
        infinitely small differentials.  Specifically, given a
        variable quantity <m>x</m>, <m>\dx{x}</m> represented an
        infinitesimal change in <m>x</m>.  Differentials are related
        via the slope of the tangent line to a curve.  That is, if
        <m>y=f(x)</m>, then <m>\dx{ y}</m> and <m>\dx{ x}</m> are related by
        <me>
          \dx{ y}=\text{ (slope of the tangent line) } \cdot \dx{ x}
          </me>.
      </p>

      <p>
        Leibniz then divided by <m>\dx{ x}</m> giving
        <me>
          \dfdx{y}{x}= \text{ (slope of the tangent line). }
        </me>
      </p>

      <p>
        The elegant and expressive notation Leibniz invented was so
        useful that it has been retained through the years despite
        some profound changes in the underlying concepts.  For
        example, Leibniz and his contemporaries would have viewed the
        symbol <m>\dfdx{y}{x}</m> as an actual quotient of
        infinitesimals, whereas today we define it via the limit
        concept first suggested by Newton.  <idx><h>Newton,
        Isaac</h></idx>
      </p>

      <p>
        <idx><h>Leibniz, Gottfried Wilhelm</h><h>differentiation
        rules</h></idx>

        As a result Leibniz<rsq/> rules governing his differentials
        are very modern in appearance:
        <md>
          <mrow>\dx{(\text{ constant } )}\amp =0</mrow>
          <mrow>\dx{(z-y+w+x)}\amp =\dx{ z}-\dx{ y}+\dx{ w}+\dx{ x}</mrow>
          <mrow>\dx{(xv)}\amp =x\dx{ v}+v\dx{ x}</mrow>
          <mrow>\dx{\left(\frac{v}{y}\right)}\amp =\frac{y\dx{ v}-v\dx{ y}}{yy}</mrow>
          <intertext>and, when <m>a</m> is an integer:</intertext>
          <mrow>\dx{(x^a)}\amp =ax^{a-1}\dx{ x}</mrow>
          </md>.
      </p>

      <p>
        Leibniz states these rules without proof: <q>. . . the
        demonstration of all this will be easy to one who is
        experienced in such matters . . .</q>. Mathematicians in
        Leibniz<rsq/>s day would have been expected to understand
        intuitively that if <m>c</m> is a constant, then
        <me>
          \dx(c)=c-c=0
          </me>.  
          Likewise, <m>\dx(x+y)=\dx{x}+\dx{y}</m> is really an
          extension of 
          <me>
            \underbrace{(x_2+y_2)-(x_1+y_1)}_{\Delta
            (x+y)}=\underbrace{(x_2-x_1)}_{\Delta
            x}+\underbrace{(y_2-y_1)}_{\Delta{y}} 
            </me>.
      </p>
    </subsection>

    <subsection xml:id="LeibnizProductRule">
      <title>Leibniz<rsq/>s Approach to the Product Rule</title>
      <p>
        <idx><h>Leibniz, Gottfried Wilhelm</h></idx> 
        The explanation of the product rule using differentials is a
        bit more involved, but Leibniz expected that mathematicans
        would be fluent enough to derive it.  The product <m>p=xv</m>
        can be thought of as the area of the following rectangle
      </p>

      <figure xml:id="fig1">
        <caption>
        </caption>
        <image width="50%" source="images/fig1-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>

      <p>
        With this in mind, <m>\dx{ p}=\dx{(xv)}</m> can be thought of as the
        change in area when <m>x</m> is changed by <m>\dx{ x}</m> and
        <m>v</m> is changed by <m>\dx{ v}</m>.  This can be seen as the L
        shaped region in the following drawing.
      </p>
      <figure xml:id="fig2">
        <title>
        </title>
        <caption>
        </caption>
        <image width="90%" source="images/fig2-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>

      <p>
        By dividing the L shaped region into 3 rectangles we obtain
        <men xml:id="eq_LeibnizProductRule">
          \dx{(xv)}=x\dx{ v}+v\dx{ x}+\dx{ x}\,\dx{ v}
          </men>.
      </p>

      <p>
        Even though <m>\dx{ x}</m> and <m>\dx{ v}</m> are infinitely small,
        Leibniz reasoned that <m>\dx{ x}\,\dx{ v}</m> is <em>even more</em>
        infinitely small (quadratically infinitely small?)  compared
        to <m>x\dx{ v}</m> and <m>v\dx{ x}</m> and can thus be ignored
        leaving
        <me>
          \dx{ (xv)}=x\dx{ v}+v\dx{ x}
          </me>.
      </p>

      <p>
        You should feel some discomfort at the idea of simply tossing
        the product <m>\dx{ x}\,\dx{ v}</m> aside because it is
        <q>comparatively small.</q> This means you have been well
        trained, and have thoroughly internalized Newton<rsq/>s
        <idx><h>Newton, Isaac</h></idx> dictum<nbsp /><xref
        ref="newton45__sir_isaac_two_treat_quadr" />: <q>The smallest
        errors may not, in mathematical matters, be scorned.</q> It is
        logically untenable to toss aside an expression just because
        it is small.  Even less so should we be willing to ignore an
        expression on the grounds that it is <q>infinitely smaller</q>
        than another quantity which is itself <q>infinitely small.</q>
      </p>

      <p>
        Newton and Leibniz both knew this as well as we do.  But they
        also knew that their methods worked.  They gave
        verifiably correct answers to problems which had, heretofore,
        been completely intractable.  It is the mark of their genius
        that both men persevered in spite of the very evident
        difficulties their methods entailed.
      </p>
    </subsection>

    <subsection xml:id="NewtonsApproach">
      <title>Newton<rsq/>s Approach to the Product Rule</title>
      <p>
        
        <url
        href="https://mathshistory.st-andrews.ac.uk/Biographies/Newton/"
        visual="mathshistory.st-andrews.ac.uk/Biographies/Newton/">Isaac
        Newton<rsq/>s</url> (1643<ndash/>1727) approach to Calculus
        <mdash /> his <sq>Method of Fluxions</sq> <mdash /> depended
        fundamentally on motion.  He conceived of variables (fluents)
        as changing (flowing or fluxing) in time.  The rate of change
        of a fluent he called a fluxion.  As a theoretical foundation
        both Leibniz<rsq/>s and Newton<rsq/>s approaches have fallen
        out of favor, although both are still universally used as a
        conceptual approach, a
        <q>
          way of thinking,
        </q> 
        about the ideas of Calculus.
      </p>
      <figure xml:id="Newton">
        <title>
        </title>
        <caption>Isaac Newton</caption>
        <idx><h>Newton, Isaac</h><h>portrait of</h></idx>
        <image width="35%" source="images/Newton.png" >
          <shortdescription>Bust of Isaac Newton</shortdescription>
        </image>
      </figure>
      <p>
        In 
        <pubtitle>
          Philosophiae naturalis principia mathematica
        </pubtitle>
        (this is usually shortened to 
        <pubtitle>
          Principia
          </pubtitle>) 
          Newton <q>proved</q> the Product Rule as
        follows: Let <m>x</m> and <m>v</m> be
        <q>
          flowing quantites
        </q>
        and consider the rectangle, <m>R</m>, whose sides are <m>x</m>
        and <m>v</m>.  <m>R</m> is also a flowing quantity and we wish
        to find its fluxion (derivative) at any time.
      </p>
      <!-- <historical> -->
      <!--   <title>Historical Note: Newton<rsq/>s <pubtitle>Method of Fluxions</pubtitle></title> -->
      <!--   <p> -->
      
      <!--     <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Newton/" visual="mathshistory.st-andrews.ac.uk/Biographies/Newton/">Isaac Newton</url><rsq/>s (1643<ndash/>1727) approach to Calculus <mdash /> his <sq>Method of -->
      <!--     Fluxions</sq> <mdash /> depended fundamentally on motion. -->
      <!--     That is, he viewed his variables (fluents) as changing -->
      <!--     (flowing or fluxing) in time.  The rate of change of a -->
      <!--     fluent he called a fluxion.  As a foundation both Leibniz<rsq/>s -->
      <!--     and Newton<rsq/>s approaches have fallen out of favor, although -->
      <!--     both are still universally used as a conceptual approach, a -->
      <!--     <q> -->
      <!--       way of thinking, -->
      <!--     </q>  -->
      <!--     about the ideas of Calculus. -->
      <!--   </p> -->
      <!-- </historical> -->
      <p>
        First we increment <m>x</m> and <m>v</m> by the half<ndash/>increments <m>\frac{\Delta
        x}{2}</m> and <m>\frac{\Delta v}{2}</m> respectively.  Then
        the corresponding half<ndash/>increment of <m>R</m> is
        <men xml:id="eq_prodruleinc">
          \frac{\Delta R}{2}=\left(x+\frac{\Delta x}{2}\right)\left(v+\frac{\Delta v}{2}\right) = xv + x\frac{\Delta v}{2} + v\frac{\Delta x}{2} +\frac{\Delta x\Delta v}{4}
          </men>.
      </p>

      <p>
        Now decrement <m>x</m> and <m>v</m> by the same amounts:
        <men xml:id="eq_prodruledec">
          -\frac{\Delta R}{2}=\left(x-\frac{\Delta x}{2}\right)\left(v-\frac{\Delta v}{2}\right) = xv - x\frac{\Delta v}{2} - v\frac{\Delta x}{2} + \frac{\Delta x\Delta v}{4}
          </men>.
      </p>

      <p>
        Subtracting  <xref ref="eq_prodruledec">equation</xref>
        from  <xref ref="eq_prodruleinc">equation</xref> gives
        <me>
          \Delta R = x\Delta v + v\Delta x
        </me>
        which is the total change of <m>R = xv</m> over the intervals <m>\Delta x</m> and
        <m>\Delta v</m> and also recognizably the Product Rule.
      </p>



      <p>
        This argument is no better than Leibniz<rsq/>s as it relies
        heavily on the number <m>1/2</m> to make it work.  If we
        take any other increments in <m>x</m> and <m>v</m> whose
        total lengths are <m>\Delta x</m> and <m>\Delta v</m> it
        will simply not work.  Try it and see.
      </p>

      <p>
        In Newton<rsq/>s defense, he wasn<rsq/>t really trying to justify his
        mathematical methods in the 
        <pubtitle>
          Principia
        </pubtitle>.  His attention
        was on physics, not math, so he was really just trying to
        give a convincing demonstration of his methods.  You may
        decide for yourself how convincing his demonstration is.
      </p>

      <p>
        <idx><h>Lagrange, Joseph-Louis</h></idx>
        Notice that there is no mention of limits of difference
        quotients or derivatives.  In fact, the term derivative was
        not coined until 1797, by Lagrange as we will see in <xref ref="PowerSeriesQuestions-TaylorsFormula"></xref> .  In a sense, these
        topics were not necessary at the time, as Leibniz and Newton
        both assumed that the curves they dealt with had tangent
        lines and, in fact, Leibniz explicitly used the tangent line
        to relate two differential quantities.  This was consistent
        with the thinking of the time and for the duration of this
        chapter we will also assume that all quantities are
        differentiable.  As we will see later this assumption leads
        to difficulties.
      </p>

      <p>
        Both Newton and Leibniz were satisfied that their Calculus
        provided answers that agreed with what was known at the
        time.  For example the formulas
        <me>
          \dx{ \left(x^2\right)}=\dx{\left(xx\right)}=x\dx{ x}+x\dx{
          x}=2x\dx{ x} 
        </me> 
        and
        <me>
          \dx{\left(x^3\right)}=\dx{\left(x^2x\right)}=x^2\dx{
          x}+x\dx{\left(x^2\right)}=x^2+x\left(2x\dx{
          x}\right)=3x^2\dx{ x}
          </me>, 
          were results that had been derived by others using other methods.
          ways.
      </p>

      <!-- <problem> -->
      <!--   <idx><h>Leibniz, Gottfried Wilhelm</h><h>Leibniz<rsq/>s product rule</h></idx> -->
      <!--   <idx><h>Leibniz<rsq/>s product rule</h></idx> -->
      <!--   <task> -->
      <!--     <statement> -->
      <!--       <p> -->
      <!--         Use Leibniz<rsq/> product rule -->
      <!--         <m>\dx{ \left(xv\right)}=x\dx{ v}+v\dx{ x}</m>  -->
      <!--         to show that if <m>n</m> is a positive integer then -->
      <!--         <m>\dx{ \left(x^n\right)}=nx^{n-1}\dx{ x}</m> -->
      <!--       </p> -->
      <!--     </statement> -->
      <!--   </task> -->
      <!--   <task> -->
      <!--     <statement> -->
      <!--       <p> -->
      <!--         Use Leibniz<rsq/>s product rule to derive the quotient rule -->
      <!--         <me> -->
      <!--           \dx{ \left(\frac{v}{y}\right)}=\frac{y\,\dx{ v}-v\,\dx{ y}}{yy} -->
      <!--           </me>. -->
      <!--       </p> -->
      <!--     </statement> -->
      <!--   </task> -->
      <!--   <task> -->
      <!--     <statement> -->
      <!--       <p> -->
      <!--         Use the quotient rule to show that if <m>n</m> is a -->
      <!--         positive integer, then -->
      <!--         <me> -->
      <!--           \dx{ \left(x^{-n}\right)}=-nx^{-n-1}\dx{ x}. -->
      <!--         </me> -->
      <!--       </p> -->
      <!--     </statement> -->
      <!--   </task> -->
      <!-- </problem> -->


      <!--       <problem> -->
      <!--         <idx><h>Leibniz, Gottfried Wilhelm</h><h>Leibniz<rsq/>s product rule</h></idx> -->
      <!--         <idx><h>Leibniz<rsq/>s product rule</h></idx> -->
      <!--         <introduction> -->
      <!--           <p> -->
      <!--             Assume that <m>n</m> is a positive integer. -->
      <!--           </p> -->
      <!-- </introduction> -->

      <!-- <task> -->
      <!--   <statement> -->
      <!--     <p> -->
      <!--       Use Leibniz<rsq/> product rule -->
      <!--       <m>\dx{ \left(xv\right)}=x\dx{ v}+v\dx{ x}</m>  -->
      <!--       to show that if <m>n</m> is a positive integer then -->
      <!--       <me>\dx{ \left(x^n\right)}=nx^{n-1}\dx{ x}</me> -->
      <!--     </p> -->
      <!--   </statement> -->
      <!-- </task> -->
      <!-- <task> -->
      <!--   <statement> -->
      <!--     <p> -->
      <!--       Suppose <m>u=\frac{v}{y} </m>. Use Leibniz<rsq/>s product rule to derive the quotient rule -->
      <!--       <me> -->
      <!--         \dx{u}= \dx{ \left(\frac{v}{y}\right)}=\frac{y\,\dx{ -->
      <!--         v}-v\,\dx{y}}{yy}  -->
      <!--         </me>. -->
      <!--     </p> -->
      <!--   </statement> -->
      <!-- </task> -->
      <!-- <task> -->
      <!--   <statement> -->
      <!--     <p> -->
      <!--       Use the quotient rule to show that if <m>n</m> is a -->
      <!--       positive integer, then -->
      <!--       <me> -->
      <!--         \dx{ \left(x^{-n}\right)}=-nx^{-n-1}\dx{ x}. -->
      <!--       </me> -->
      <!--     </p> -->
      <!--   </statement> -->
      <!-- </task> -->
      <!--       </problem> -->
      <problem>
        <introduction>
          <p>
            Assume <m>n</m> is a positive integer.
          </p>
        </introduction>
        <task>
          <statement>
            <p>
              Use Leibniz<rsq/> product rule <m>\dx{\left(xv\right)}=x\dx{v}+v\dx{x}</m> to show that
              <me>\dx{\left(x^n\right)}=nx^{n-1}\dx{x}</me> 
            </p>
          </statement>
        </task>
        <task>
          <statement>
            <p>
              Suppose <m>y=x^{-1}=\frac{1}{x}</m>, Use Leibniz<rsq/> product rule to show
              <me>dy=-1x^{-2}\dx{x}</me> 
            </p>
          </statement>
        </task>
        <task>
          <statement>
            <p>
              Use the product rule and the result of part (b) to derive the quotient rule  
              <me>d\left(\frac{v}{x}\right)=\frac{x\dx{v}-v\dx{x}}{x^2}</me> 

            </p>
          </statement>
        </task>
        <task>
          <statement>
            <p>
              Use the quotient rule to show that

              <me>d\left(x^{-n}\right)=-nx^{-n-1}\dx{x}</me> 

            </p>
          </statement>
        </task>
      </problem>

      <problem>
        <statement>
          <p>
            <idx><h>differentiation</h><h>power rule with fractional
            exponents</h></idx> Suppose <m>y=x^{\frac{p}{q}}</m> with
            <m>q\neq 0</m>, where <m>p</m> and <m>q</m> are integers.
            Show that 
            <me>
              \dx{y}=\dx{
              \left(x^{\frac{p}{q}}\right)}=\frac{p}{q}x^{\frac{p}{q}-1}\dx{
              x}
              </me>.
          </p>
        </statement>
      </problem>

      <p>
        To prove the worth of his Calculus Leibniz also provided
        applications.  As an example he derived Snell<rsq/>s Law of
        Refraction from his Calculus rules as follows.
      </p>

      <figure  xml:id="FIGURESnellPortrait">
        <caption>
          <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Snell/" visual="mathshistory.st-andrews.ac.uk/Biographies/Snell/">Willebrord Snell</url> (1580<ndash/>1626)
        </caption>
        <image source="images/Snell.png" width="35%">
          <shortdescription>Portrait of Willebrord Snell</shortdescription>
        </image>
      </figure>

      <p>
        Given that light travels through air at a speed of
        <m>v_a</m> and travels through water at a speed of
        <m>v_w</m> the problem is to find the fastest path from
        point <m>A</m> to point <m>B</m>.
      </p>

      <figure xml:id="snellfig">
        <title>
        </title>
        <caption>
        </caption>
        <image width="55%" source="images/snellfig-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>

      <p>
        According to 
        <url href="https://en.wikipedia.org/wiki/Fermat%27s_principle" visual="en.wikipedia.org/wiki/Fermat%27s_principle">Fermat<rsq/>s Principle of Least Time</url>, this fastest
        path is the one that light will travel.
      </p>

      <p>
        Using the fact that <m>\text{ Time }=\frac{\text{Distance}}{\text{Velocity}}</m> and the labeling in the picture below
        we can obtain a formula for the time <m>T</m> it takes for
        light to travel from <m>A</m> to <m>B</m>.
      </p>
      <figure xml:id="snellfig2">
        <caption>
        </caption>
        <image width="55%" source="images/snellfig2-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>
      <p>
        <me>
          T=\frac{\sqrt{x^2+a^2}}{v_a}+\frac{\sqrt{(c-x)^2+b^2}}{v_w}
        </me>
      </p>

      <p>
        Using the rules of Leibniz<rsq/>s Calculus, we obtain
        <md>
          <mrow>\dx{ T}\amp = \left[\frac{1}{v_a}\frac{1}{2}\left(x^2+a^2\right)^{-\frac{1}{2}} (2x)+\frac{1}{v_w}\frac{1}{2}((c-x)^2+b^2)^{-\frac{1}{2}}(2(c-x)(-1))\right] \dx{x}</mrow>
          <mrow>\amp =\left[\frac{1}{v_a}\frac{x}{\sqrt{x^2+a^2}}-\frac{1}{v_w}\frac{c-x}{\sqrt{(c-x)^2+b^2}}\right]\dx{ x}</mrow>
          </md>.
      </p>

      <p>
        Using the fact that at the minimum value for <m>T</m>, <m>\dx{
        T}=0</m>, we see that the fastest path from <m>A</m> to
        <m>B</m> must satisfy
        <me>
          \frac{1}{v_a}\frac{x}{\sqrt{x^2+a^2}}=\frac{1}{v_w}\frac{c-x}{\sqrt{(c-x)^2+b^2}}
          </me>.

      </p>
      <figure xml:id="snellfig3">
        <caption>
        </caption>
        <image width="55%" source="images/snellfig3-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>

      <p>
        From <xref ref="snellfig3"></xref> we see that
        the path that light travels must satisfy
        <m>\frac{\sin\theta_a}{v_a}=\frac{\sin\theta_w}{v_w}</m> which
        is Snell<rsq/>s Law.
      </p>

      <p>
        <idx><h>Bernoulli, Johann</h> </idx> 
        <idx><h>Brachistochrone problem, the</h></idx>
        To compare 18th century and modern techniques we will consider
        
        <url
            href="https://mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Johann/"
            visual="mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Johann">Johann
        Bernoulli<rsq/>s</url> (1667<ndash/>1748) solution of the <url
        href="https://mathshistory.st-andrews.ac.uk/HistTopics/Brachistochrone/"
        visual="mathshistory.st-andrews.ac.uk/HistTopics/Brachistochrone/">Brachistochrone
        Problem</url>.  In 1696, Bernoulli posed and solved, the
        Brachistochrone problem: To find the shape of a frictionless
        wire joining points <m>A</m> and <m>B</m> so that the time it
        takes for a bead to slide down under the force of gravity is
      as small as possible.  </p>


      <figure xml:id="brachfig1">
        <caption>
        </caption>
        <image width="55%" source="images/brachfig1-1.png" >
          <shortdescription></shortdescription>
        </image>
      </figure>


      <p>
        Bernoulli posed this <q>path of fastest descent</q> problem to
        challenge the mathematicians of Europe and used his solution
        to demonstrate the power of Leibniz<rsq/> Calculus as well as his
        own ingenuity.  <idx><h>Bernoulli, Johann</h><h>Bernoulli<rsq/>s
        challenge</h></idx>
      </p>

      <blockquote>
        <p>
          I, Johann Bernoulli, address the most brilliant mathematicians in
          the world. Nothing is more attractive to intelligent people than
          an honest, challenging problem, whose possible solution will
          bestow fame and remain as a lasting monument. Following the
          example set by Pascal, Fermat, etc., I hope to gain the gratitude
          of the whole scientific community by placing before the finest
          mathematicians of our time a problem which will test their methods
          and the strength of their intellect. If someone communicates to me
          the solution of the proposed problem, I shall publicly declare him
          worthy of praise. <xref ref="Bernoulli_bio_mactutor" />
        </p>    
      </blockquote>

      <figure>
        <caption>
          Johann Bernoulli
        </caption>
        <idx><h>Bernoulli, Johann</h><h>portrait of</h></idx>

        <image width="35%" source="images/BernoulliJohann.png" >
          <shortdescription>Portrait of Johann Bernoulli</shortdescription>
        </image>
      </figure>

      <p>
        In addition to Johann<rsq/>s, solutions were obtained from
        Newton, <idx><h>Newton, Isaac</h></idx> Leibniz, Johann<rsq/>s
        brother Jacob Bernoulli, <idx><h>Bernoulli Jacob</h></idx> and
        the Marquis de l<rsq/>Hopital <xref
        ref="struik69__sourc_book_mathem" />.  At the time there was
        an ongoing and very vitriolic controversy raging over whether
        Newton or Leibniz had been the first to invent Calculus.  As
        an advocate for Leibniz, Bernoulli did not believe Newton
        would be able to solve the problem using his fluxions. So
        Bernoulli<rsq/>s this challenge was in part an attempt to
        embarrass Newton.  However Newton solved it easily.
      </p>

      <p>
        At this point in his life Newton had all but quit science and
        mathematics and was fully focused on his administrative duties
        as Master of the Mint.  Due in part to rampant counterfeiting,
        England<rsq/>s money had become severely devalued and the nation
        was on the verge of economic collapse.  The solution was to
        recall all of the existing coins, melt them down, and strike
        new ones.  As Master of the Mint this job fell to Newton <xref
        ref="levenson09__newton_count" />.  As you might imagine this
        was a rather Herculean task.  Nevertheless, according to his
        niece (and housekeeper):
      </p>

      <blockquote>
        <p>
          When the problem in 1696 was sent by Bernoulli<ndash />Sir
          I.N. was in the midst of the hurry of the great recoinage
          and did not come home till four from the Tower very much
          tired, but did not sleep till he had solved it, which was by
          four in the morning.
        </p>
      </blockquote>

      <p>
        He is reported to have complained,
        <q>
          I do not love . . . to be . . . teezed by forreigners about
          Mathematical things
          </q> <xref ref="dunham90__journ_throug_genius" />.
      </p>

      <p>
        <idx><h>Bernoulli, Johann</h><h><foreign>"Tanquam ex ungue
        leonem"</foreign></h></idx> Newton submitted his solution
        anonymously, presumably to avoid more controversy.
        Nevertheless the methods he used were so distinctively
        Newton<rsq/>s that Bernoulli is said to have exclaimed
        <q><foreign>Tanquam ex ungue leonem</foreign>.</q>
      </p>
<p>
</p>

<aside>
  <title>
    Translation: <foreign>Tanquam ex ungue leonem</foreign>
  </title>
  <p>
    <q>I know the lion by his claw.</q>
  </p>
</aside> 

<p>
  <idx><h>Brachistochrone problem, the</h><h>Bernoulli<rsq/>s solution</h></idx>

  Newton<rsq/>s solution was clever but it doesn<rsq/>t provide any
  insights we<rsq/>ll be interested in so we will focus on
  Bernoulli<rsq/>s ingenious solution which starts, interestingly
  enough, with Snell<rsq/>s Law of Refraction.  He begins by
  considering the stratified medium in the following figure, where an
  object travels with velocities <m>v_1, v_2, v_3, \ldots</m> in
  the various layers.
</p>
<figure xml:id="brachfig2">
  <caption>
  </caption>
  <image width="55%" source="images/brachfig2-1.png" >
    <shortdescription></shortdescription>
  </image>
</figure>

<p>
  By repeatedly applying Snell<rsq/>s Law he concluded that the fastest path must satisfy
  <me>
    \frac{\sin \theta_1}{v_1}=\frac{\sin \theta_2}{v_2}=\frac{\sin\theta_3}{v_3}=\cdots
    </me>.
</p>

<p>
  In other words, the ratio of the sine of the angle that the
  curve makes with the vertical and the speed remains constant
  along this fastest path.
</p>

<p>
  If we think of a continuously changing medium as stratified
  into infinitesimal layers and extend Snell<rsq/>s law to an object
  whose speed is constantly changing,
  then along the fastest path,
  the ratio of the sine of the angle that the curve<rsq/>s tangent makes with the vertical,
  <m>\alpha</m>, and the speed,
  <m>v</m>, must remain constant,
  <me>
    \frac{\sin\alpha}{v}=c
    </me>
    as in <xref ref="snellfig4"></xref> below.
</p>

<figure xml:id="snellfig4">
  <caption>
  </caption>
  <image width="40%" source="images/snellfig4-1.png">
    <shortdescription></shortdescription>
  </image>
</figure>


<p>
  If we include the horizontal and vertical axes and let <m>P</m>
  denote the position of the bead at a particular time then we have
  the following picture.
</p>
<figure xml:id="snellfig5">

  <caption>
  </caption>
  <image width="55%" source="images/snellfig5-1.png" >
    <shortdescription></shortdescription>
  </image>
</figure>

<p>
  In <xref ref="snellfig5"></xref>, <m>s</m> denotes the length that
  the bead has traveled down to point <m>P</m> (that is, the arc
  length of the curve from the origin to that point) and <m>a</m>
  denotes the tangential component of the acceleration due to gravity
  <m>g</m>.  Since  acceleration is the rate of change of velocity with
  respect to time we see that
  <me>
  \dfdx{v}{t}=a
  </me>.
</p>

<p>
  To get a sense of how physical problems were approached using
  Leibniz<rsq/>s Calculus we will use the above equation to show that
  <m>v=\sqrt{2gy}</m>.
</p>

<p>
  By similar triangles we have 
  <m>\frac{a}{g}=\frac{\dx{ y}}{\dx{ s}}</m>.  
  As a student of Leibniz, Bernoulli would have regarded 
  <m>\frac{\dx{ y}}{\dx{ s}}</m> as a fraction so

  <md>
    <mrow>a\dx{ s} \amp = g\dx{ y}</mrow>
    <intertext>
      and since acceleration is the rate of change of velocity we have
    </intertext>
    <mrow>\frac{\dx{ v}}{\dx{ t}}\dx{ s} \amp = g\dx{ y}.</mrow>
    <intertext>

    Again, 18th century European mathematicians regarded <m>\dx{ v}, \dx{ t}</m>, and <m>\dx{ s}</m> as infinitesimally small numbers which nevertheless obey all of the usual rules of algebra. Thus we can rearrange the above to get</intertext>

    <mrow>\frac{\dx{ s}}{\dx{ t}}\dx{ v} \amp = g\dx{ y}.</mrow>
    <intertext>Since <m>\frac{\dx{ s}}{\dx{ t}}</m> is the rate of change of position with respect to time it is, in fact, the velocity of the bead. That is</intertext>
    <mrow>v\dx{ v} \amp = g\dx{ y}.</mrow>
    <intertext>Bernoulli would have interpreted this as a statement that two rectangles of height <m>v</m> and <m>g</m>, with respective widths <m>\dx{ v}</m> and <m>\dx{ y}</m> have equal area. Summing (integrating) all such rectangles we get:</intertext>
    <mrow>\int{}v\dx{ v} \amp = \int{}g\dx{ y}</mrow>
    <mrow>\frac{v^2}{2} \amp = gy</mrow>
  </md>
  or
  <men xml:id="eq_brach_vel">
    v=\sqrt{2gy}
    </men>.
</p>

<p>
  You are undoubtedly uncomfortable with the cavalier
  manipulation of infinitesimal quantities you<rsq/>ve just
  witnessed, so we<rsq/>ll pause for a moment now to compare a
  modern development of <xref
  ref="eq_brach_vel">equation</xref> to Bernoulli<rsq/>s.  As
  before we begin with the equation:
  <md>
    <mrow>\frac{a}{g}\amp = \dfdx{y}{s}</mrow>

    <mrow>a \amp = g\dfdx{y}{s}.</mrow>

    <intertext>
      Moreover, since acceleration is the derivative of velocity this is the same as:
    </intertext>

    <mrow>\dfdx{v}{t} \amp = g\dfdx{y}{s}.</mrow>

    <intertext>
      Now observe that by the Chain Rule <m>\dfdx{v}{t} =
      \dfdx{v}{s}\dfdx{s}{t}</m>. The physical interpretation
      of this formula is that velocity will depend on
      <m>s</m>, how far down the wire the bead has moved, but
      that the distance traveled will depend on how much time
      has elapsed. Therefore
    </intertext>

    <mrow>\dfdx{v}{s}\dfdx{s}{t} \amp = g\dfdx{y}{s}</mrow>
    <intertext>or</intertext>
    <mrow>\dfdx{s}{t}\dfdx{v}{s} \amp = g\dfdx{y}{s}</mrow>
    <intertext>and since <m>\dfdx{s}{t} = v</m> we have</intertext>
    <mrow>v\dfdx{v}{s} \amp = g\dfdx{y}{s}</mrow>
    <intertext>Integrating both sides with respect to <m>s</m> gives:</intertext>
    <mrow>\int{}v\dfdx{v}{s}d s \amp = g\int{}\dfdx{y}{s}d s</mrow>
    <mrow>\int{}vd v \amp = g\int{}d y</mrow>
    <intertext>and integrating gives</intertext>
    <mrow>\frac{v^2}{2} \amp = gy</mrow>
  </md>
  as before.
</p>

<p>
  In effect, in the modern formulation we have traded the
  simplicity and elegance of differentials for a comparatively
  cumbersome repeated use of the Chain Rule.  No doubt you
  noticed when taking Calculus that in the differential
  notation of Leibniz,<idx><h>Leibniz, Gottfried
  Wilhelm</h></idx> the Chain Rule looks like <q>canceling</q>
  an expression in the top and bottom of a fraction:
  <m>\dfdx{y}{u}\dfdx{u}{x} = \dfdx{y}{x}</m>.  This is
  because for 18th century mathematicians, that is exactly
  what it was.
</p>

<p>
  To put it another way, 18th century mathematicians wouldn<rsq/>t
  have recognized a need for what we call the Chain Rule because
  this operation was a triviality for them.  Just reduce the
  fraction.  This begs the question: Why did we abandon such a
  clear and simple interpretation of our symbols in favor of the
  comparatively more cumbersome modern interpretation?  This is
  one of the questions we will try to answer in this course.
</p>

<p>
  Returning to the Brachistochrone problem we observe that
  <mdn>
    <mrow number="no">\frac{\sin\alpha}{v} \amp = c</mrow>
    <intertext>and since <m>\sin\alpha = \frac{d x}{d s}</m>   we see that</intertext>
    <mrow number="no">\frac{\frac{d x}{d s}}{\sqrt{2gy}} \amp = c</mrow>
    <mrow number="no">\frac{d x}{\sqrt{2gy(ds)^2}} \amp = c</mrow>
    <mrow xml:id="eq_Brachistochrone">\frac{d x}{\sqrt{2gy\left[(d x)^2+(d y)^2\right]}} \amp = c</mrow>
    </mdn>.
</p>

<p>
  Bernoulli was then able to solve this differential
  equation.
</p>

<problem>
  <idx><h>Brachistochrone problem, the</h></idx>
  <statement>
    <p>
      Show that the equations <m>x=\frac{\phi-\sin
      \phi}{4gc^2},\,y=\frac{1-\cos \phi}{4gc^2}</m> satisfy
      equation <xref ref="eq_Brachistochrone">equation</xref>.
      Bernoulli recognized this solution to be an inverted
      cycloid, the curve traced by a fixed point on a circle
      as the circle rolls along a horizontal surface.
    </p>
  </statement>
</problem>

<p>
  This illustrates the state of Calculus in the late 1600s
  and early 1700s; the foundations of the subject were a
  bit shaky but there was no denying its power.
</p>
    </subsection>
  </section>

  <section xml:id="ExponentAdditionProperty">
    <title>Power Series as Infinite Polynomials</title>
    <p>
      <idx><h>polynomials</h><h>infinite</h></idx>
      
      Applied to polynomials, the rules of differential and integral
      Calculus are straightforward.  Indeed, differentiating and
      integrating polynomials represent some of the easiest tasks in a
      Calculus course.  For example, computing <m>\int(7-x+x^2)\dx{
      x}</m> is relatively easy compared to computing
      <m>\int\sqrt[3]{1+x^3}\dx{ x}</m>.  Unfortunately, not all
      functions can be expressed as a polynomial.  For example,
      <m>f(x)=\sin x</m> cannot be since a polynomial has only
      finitely many roots and the sine function has infinitely many
      roots, namely <m>\{n\pi|\,n\in\ZZ\}</m>.  A standard technique
      in the 18th century was to write such functions as an
      <q>infinite polynomial,</q> or what today we refer to as a <term>power
      series</term>.  Unfortunately an <q>infinite polynomial</q> is a much
      more subtle object than a mere polynomial, which by definition
      is finite.  For now we will not concern ourselves with these
      subtleties. Instead we will follow the example of our forebears and
      manipulate all <q>polynomial<ndash/>like</q> objects (finite or
      infinite) as if they are polynomials.
    </p>

    <definition xml:id="def_PowerSeries">
      <idx><h>power series</h><h>definition</h></idx>
      <idx><h>Definition</h><h>power series</h></idx>
      <statement>
        <p>
          A <term>power series centered at
          <m>\boldsymbol{a}</m></term> is a series of the form
          <me>
            \sum_{n=0}^\infty a_n(x-a)^n=a_0+a_1(x-a)+a_2(x-a)^2+\cdots
            </me>.
        </p>
      </statement>
    </definition>


    <p>
      Thus a power series centered around zero has the form
      <me>
        \sum_{n=0}^\infty a_nx^n </me>.  In this section we will focus
        on power series centered around zero. In <xref
        ref="SECTIONPowSerWOTaylor" text="custom" >the next section</xref> we will
        look at power series centered about points other than zero.
        <m>0</m>.
    </p>



<paragraphs>
  <title> A useful comment on notation: </title>

  <p>
    The most advantageous way to represent a power series is using
    summation notation since there can be no doubt about the
    pattern in the terms.  After all, this notation contains a
    formula for the general term.  However, there are instances
    where summation notation is not practical.  In these cases, it
    is acceptable to indicate the sum by supplying the first few
    terms and using ellipses (the three dots).  If this is done,
    then enough terms must be included to make the pattern clear
    to the reader.
  </p>

</paragraphs>


<p>
  <idx><h>series</h><h>Geometric series</h><h>naive derivation</h></idx>

  Returning to our definition of a power series, consider the
  power series 
  <me>
    \sum_{n=0}^\infty x^n=1+x+x^2+\cdots </me>.  If we multiply
    this power series by <m>(1-x)</m>, we obtain

    <md>
      <mrow>
        (1-x)(1+x+x^2+\cdots)=(1+\amp{}x+x^2+\cdots)
      </mrow>
      <mrow>
        -(\amp{}x+x^2+x^3+\cdots)
      </mrow>
      <mrow>
        =1.\ \ \ \ \amp{}
      </mrow>

    </md>
</p>

<p>
  Dividing by <m>1-x</m> gives us the power series representation
  <men xml:id="EQUATIONGeometricSeries">
    \frac{1}{1-x}=1+x+x^2+\cdots<!-- =\sum_{n=0}^\infty x^n  -->
    </men>,
    which is known as the <term>Geometric Series</term>.
</p>

<p>
  For each value of <m>x</m> a <term>power series</term> reduces to a
  different ordinary (numerical) <term>series</term>. For example, if
  we substitute <m>x=\frac{1}{10}</m> into the left side of <xref
  ref="EQUATIONGeometricSeries">equation</xref>, we obtain the
  (numerical) <term>series</term>

  <md>
    <mrow>
      1+\frac{1}{10}+\left(\frac{1}{10}\right)^2+\left(\frac{1}{10}\right)^3+
      \cdots\amp{}=1+0.1+0.01+0.001+0.0001+\cdots
    </mrow>
    <mrow>
      \amp{}=1.1111\cdots.
    </mrow>
  </md>
  But substituting into the right side yields
  <me>
    \frac{1}{1-\frac{1}{10}}=\frac{10}{9}. 
  </me>
</p>

<p>
  If these computations are valid then it must be that
  <m>1.111\ldots=\frac{10}{9}</m>, which seems weird, but you can
  verify it by entering <m>\frac{10}{9}</m> into any calculator.
</p>
<p>
  If <m>1.111\ldots=\frac{10}{9}</m>, then subtracting
  <m>1</m> from both sides and multiplying by <m>3</m> gives
  <me>0.333\ldots = \frac{1}{3}</me> which has probably come up on
  your calculator more than once. But what seems wierder still is
  that if we multiply by <m>3</m> again we get <me>0.999\ldots
  =1</me>, which seems like nonsense. It simply can<rsq/>t be
  true, can it? What do you think? Is <m>0.999\ldots =1</m> or
  that just nonsense?  Either way it is clear that the real
  numbers <m>\left(\RR\right) </m> hide deeper mysteries than the
  irrationality of <m>\sqrt{2}</m>.
</p>

<p>
  There are other issues with these formal manipulations too.
  Substituting <m>x=1</m> or <m>x=2</m> into <xref
  ref="EQUATIONGeometricSeries">equation</xref> yields the
  questionable results
  <me>
    \frac{1}{0}=1+1+1+\cdots \text{  and  } \frac{1}{-1}=1+2+2^2+\cdots
    </me>.
</p>
<aside>
  <title>Comment</title>
  <p>
    When we say <q>formal manipulations</q> we mean that we will
    perform purely algebraic operations on an given expression
    without concerning ourselves (much) about whether the
    operations make sense in context. We will formalize them in <xref ref="TaylorSeries"></xref> 
  </p>
</aside>
<p>
  A power series representation of our function seems to work in some
  cases, but not in others. Obviously we are missing something
  important here, though it may not be clear exactly what. For
  now, we will continue to follow the example of our 18th century
  predecessors.  That is, for the rest of this section we will
  use formal manipulations to obtain and use power series
  representations of various functions.  Keep in mind that this is
  all highly suspect until we can resolve problems like those
  we<rsq/>ve just seen.
</p>


<p>
  Power series became an important tool in analysis in the 1700s.
  By representing various functions as power series they could be
  dealt with as if they were (infinite) polynomials.  The
  following is an example.
</p>

<example>
  <idx><h><m>e^x</m></h><h>as the solution of an Initial Value Problem</h></idx>
  <statement>
    <p>
      Solve the following Initial Value problem: Find
      <m>y(x)</m> given that 

      <md>
        <mrow>\frac{\dx{ y}}{\dx{
        x}}=y,\amp{}\amp{}y(0)=1.</mrow>

      </md>
    </p>
    <aside>
      <title>Comment</title>
      <p>
        A few seconds of thought should convince you that the
        solution of this problem is <m>y(x) = e^x</m>.  We
        will ignore this for now in favor of emphasising the
        technique.
      </p>  
    </aside>


    <p>
      Assuming the solution can be expressed as a power series
      we have <me> y=\sum_{n=0}^\infty
      a_nx^n=a_0+a_1x+a_2x^2+\cdots </me>.
    </p>

    <p>
      Differentiating gives us <me> \frac{\dx{ y}}{\dx{
      x}}=a_1+2a_2x+3a_3x^2+4a_4x^3+\ldots </me>.
    </p>

    <p>
      Since <m>\frac{\dx{ y}}{\dx{ x}}=y</m> we see that <me>
      a_1=a_0\,,\,2a_2=a_1\,,\,3a_3=a_2\,,\,\ldots,\,na_n=a_{n-1}\,,\ldots
      </me>.
    </p>

    <p>
      This leads to the relationship <me>
      a_n=\frac{1}{n}a_{n-1}=\frac{1}{n(n-1)}a_{n-2}=\cdots=\frac{1}{n!}a_0
      </me>.
    </p>

    <p>
      Thus the power series solution of the differential equation is
      <me>
        y=\sum_{n=0}^\infty\frac{a_0}{n!}x^n
        =a_0\sum_{n=0}^\infty\frac{1}{n!}x^n </me>.
    </p>

    <p>
      Using the initial condition <m>y(0)=1</m>, we get
      <m>1=a_0(1+0+\frac{1}{2!}0^2+\cdots)=a_0</m>.  Thus the
      solution to the initial problem is
      <m>y=\sum_{n=0}^\infty\frac{1}{n!}x^n</m>.  Let<rsq/>s call
      this function <m>E(x)</m>.  Then by definition 
      <men xml:id="EQUATIONExpSeries">
        E(x)=\sum_{n=0}^\infty\frac{1}{n!}x^n=1+\frac{x^1}{1!}+\frac{x^2}{2!}+\frac{x^3}{3!}+\,\ldots
        </men>.
    </p>
  </statement>
</example>

<p>
  Let<rsq/>s examine some properties of this function.  The first
  property is clear from the definition.
</p>

<p>
  <term>Property 1</term>. 
  <m>E(0)=1</m>
</p>

<p>
  <term>Property 2</term>. 
  <m>E(x+y)=E(x)E(y)</m>.
</p>

<p>
  To see this we multiply the two series together, so we have
  <md>
    <mrow>E(x)E(y) =\amp \left(\sum_{n=0}^\infty\frac{1}{n!}x^n\right)\left(\sum_{n=0}^\infty\frac{1}{n!}y^n\right)</mrow>
    <mrow>=\amp \left(\frac{x^0}{0!}+\frac{x^1}{1!}+\frac{x^2}{2!}+\frac{x^3}{3!}+\,\ldots\right)\left(\frac{y^0}{0!}+\frac{y^1}{1!}+\frac{y^2}{2!}+\frac{y^3}{3!}+\,\ldots\right)</mrow>
    <mrow>=\amp \frac{x^0}{0!}\frac{y^0}{0!}+\frac{x^0}{0!}\frac{y^1}{1!}+\frac{x^1}{1!}\frac{y^0}{0!}+\frac{x^0}{0!}\frac{y^2}{2!}+\frac{x^1}{1!}\frac{y^1}{1!}+\frac{x^2}{2!}\frac{y^0}{0!}</mrow>
    <mrow>\amp{}\ \ \         +\frac{x^0}{0!}\frac{y^3}{3!}+\frac{x^1}{1!}\frac{y^2}{2!}+\frac{x^2}{2!}\frac{y^1}{1!}+\frac{x^3}{3!}\frac{y^0}{0!}+\,\ldots</mrow>
    <mrow>=\amp\frac{x^0}{0!}\frac{y^0}{0!}+\left(\frac{x^0}{0!}\frac{y^1}{1!}+ \frac{x^1}{1!}\frac{y^0}{0!}\right)</mrow>
    <mrow>\amp{}\ \ \         +\left(\frac{x^0}{0!}\frac{y^2}{2!}+\frac{x^1}{1!}\frac{y^1}{1!}+\frac{x^2}{2!}\frac{y^0}{0!}\right)</mrow>
    <mrow>\amp{}\ \ \ \ \ \        +\left(\frac{x^0}{0!}\frac{y^3}{3!}+\frac{x^1}{1!}\frac{y^2}{2!}+\frac{x^2}{2!}\frac{y^1}{1!}+\frac{x^3}{3!}\frac{y^0}{0!}\right)+\,\ldots</mrow>
    <mrow>=\amp\frac{1}{0!}+\frac{1}{1!}\left(\frac{1!}{0!1!}x^0y^1+\frac{1!}{1!0!}x^1y^0\right)</mrow>
    <mrow>\amp{}\ \ \        +\frac{1}{2!}\left(\frac{2!}{0!2!}x^0y^2+\frac{2!}{1!1!}x^1y^1+\frac{2!}{2!0!}x^2y^0\right)</mrow>
    <mrow>\amp{}\ \ \ \ \ \        +\frac{1}{3!}\left(\frac{3!}{0!3!}x^0y^3+\frac{3!}{1!2!}x^1y^2+\frac{3!}{2!1!}x^2y^1+\frac{3!}{3!0!}x^3y^0\right)+\ldots</mrow>
    <!-- </md> -->

    <!-- <mdn> -->
    <mrow> =\amp \frac{1}{0!}+\frac{1}{1!}\left(\binom{1}{0}x^0y^1+\binom{1}{1}x^1y^0\right)</mrow>
    <mrow>\amp{}\ \ \        +\frac{1}{2!}\left(\binom{2}{0}x^0y^2+\binom{2}{1}x^1y^1+\binom{2}{2}x^2y^0\right)</mrow>
    <mrow>\amp{}\ \ \ \ \ \        +\frac{1}{3!}\left(\binom{3}{0}x^0y^3+\binom{3}{1}x^1y^2+\binom{3}{2}x^2y^1+\binom{3}{3}x^3y^0\right)+\ldots</mrow>
    <mrow>=\amp \frac{1}{0!}+\frac{1}{1!}\left(x+y\right)^1+\frac{1}{2!}\left(x+y\right)^2+\frac{1}{3!}\left(x+y\right)^3+\ldots</mrow> 
  </md>

  so that, finally

  <men xml:id="eq_ExponentAdditionProperty">E(x)E(y) =  E(x+y)</men>.

</p>

<p>
  <term>Property 3</term>. If <m>m</m> is a positive integer then <m>E(mx)=\left(E(x\right))^m</m>.
  In particular, <m>E(m)=\left(E(1)\right)^m</m>.
</p>

<problem>
  <statement>
    <p>
      Prove Property 3.
    </p>
  </statement>
</problem>

<p>
  <term>Property 4</term>. <m>E(-x)=\frac{1}{E(x)}=\left(E(x)\right)^{-1}</m>.
</p>

<problem>
  <statement>
    <p>
      Prove Property 4.
    </p>
  </statement>
</problem>

<p>
  <term>Property 5</term>. If <m>n</m> is an integer with
  <m>n\neq 0</m>, then
  <m>E(\frac{1}{n})=\sqrt[n]{E(1)}=\left(E(1)\right)^{1/n}</m>.
</p>

<problem>
  <statement>
    <p>
      Prove Property 5.
    </p>
  </statement>
</problem>

<p>
  <term>Property 6</term>. If <m>m</m> and <m>n</m> are
  integers with <m>n\neq 0</m>, then
  <m>E\left(\frac{m}{n}\right)=\left(E(1)\right)^{m/n}</m>.
</p>

<problem>
  <statement>
    <p>
      Prove Property 6.
    </p>
  </statement>
</problem>

<definition xml:id="def_e">
  <idx><h><m>e^x</m></h><h>definition</h></idx>
  <idx><h>Definition</h><h><m>e^x</m></h></idx>
  <statement>
    <p>
      Let <m>E(1)</m> be denoted by the number <m>e</m>.
      Using the power series <m>e=E(1)=\sum_{n=0}^\infty\frac{1}{n!}</m>,
      we can approximate <m>e</m> to any degree of accuracy.
      In particular <m>e\approx 2.71828</m>.
    </p>
  </statement>
</definition>

<p>
  In light of Property 6, we see that for any rational number
  <m>r</m>, <m>E(r)=e^r</m>.  Not only does this give us the
  power series representation
  <m>e^r=\sum_{n=0}^\infty\frac{1}{n!}r^n</m> for any rational
  number <m>r</m>, but it gives us a way to define <m>e^x</m>
  for irrational values of <m>x</m> as well.  That is, we can
  define
  <me>
    e^x=E(x)=\sum_{n=0}^\infty\frac{1}{n!}x^n
  </me>
  for any real number <m>x</m>.
</p>

<p>
  As an illustration, we now have
  <m>e^{\sqrt{2}}=\sum_{n=0}^\infty\frac{1}{n!}\left(\sqrt{2}\right)^n</m>.
  The expression <m>e^{\sqrt{2}}</m> is meaningless if we try
  to interpret it as one irrational number raised to another.
  What does it mean to raise anything to the <m>\sqrt{2}</m>
  power?  However the power series
  <m>\sum_{n=0}^\infty\frac{1}{n!}\left(\sqrt{2}\right)^n</m>
  does seem to have meaning and it can be used to extend the
  exponential function to irrational exponents.  In fact,
  defining the exponential function via this power series answers
  the question we raised in <xref
  ref="NumbersRealRational"></xref>: What does
  <m>4^{\sqrt{2}}</m> mean?

  It means 
  <me>
    4^{\sqrt{2}} = e^{\sqrt{2}\log 4} =
    \sum_{n=0}^\infty\frac{(\sqrt{2}\log 4)^n}{n!}  </me>.
</p>

<p>
  This may seem to be the long way around just to define
  something as simple as exponentiation.  But that is a
  fundamentally misguided attitude.  Exponentiation only
  <em>seems</em> simple because we<rsq/>ve always thought of it as
  repeated multiplication (in <m>\ZZ</m>) or root<ndash/>taking (in
  <m>\QQ</m>).  When we expand the operation to the real
  numbers this simply can<rsq/>t be the way we interpret something
  like <m>4^{\sqrt{2}}</m>.  How do you take the product of
  <m>\sqrt{2}</m> copies of <m>4?</m> The concept is
  meaningless.  What we need is an interpretation of
  <m>4^{\sqrt{2}}</m> which is consistent with, say <m>4^{3/2}
  = \left(\sqrt{4}\right)^3=8</m>.  This is exactly what the
  power series representation of <m>e^x</m> provides.
</p>

<p>
  We also have a means of computing integrals as power series.  For
  example, the famous <q>bell shaped</q> curve given by the
  function <m>f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}</m>
  is of vital importance in statistics as it must be integrated
  to calculate probabilities.  The power series we developed
  gives us a method of integrating this function.  For example,
  we have
  <md>
    <mrow>\int_{x=0}^b\frac{1}{\sqrt{2\pi}}
    e^{-\frac{x^2}{2}}\dx{x} \amp =\frac{1}{\sqrt{2\pi}}
    \int_{x=0}^b \left(\sum_{n=0}^\infty
    \frac{1}{n!}\left(\frac{-x^2}{2}\right)^n\right)\dx{x}
    </mrow>
    <mrow>
      \amp = \frac{1}{\sqrt{2\pi}} \sum_{n=0}^\infty
      \left(
      \frac{\left(-1\right)^n}{n!2^n}\int_{x=0}^bx^{2n}\dx{x}
      \right)
    </mrow>
    <mrow>
    \amp =\frac{1}{\sqrt{2\pi}}\,\sum_{n=0}^\infty\left(\frac{\left(-1\right)^nb^{2n+1}}{n!2^n\left(2n+1\right)}\right)</mrow>
    </md>.
</p>

<p>
  This power series can be used to approximate the integral to any
  degree of accuracy.
</p>

<problem>
  <statement>
    <p>
      Write <m>e^{-x^3}</m> as a power series expanded about
      <m>0</m> and use your series to represent
      <me>\int^b_{x=0}{e^{-x^3}\dx{x}}</me> 
      as a power series.
    </p>
  </statement>
</problem>    

<problem>
  <statement>
    <p>
      Let <m>a>0</m>.  Find a power series expansion about 0 for <m>a^x</m>

    </p>

  </statement>

  <hint>

    <p>
      <m>a^x=e^{\ln a^x }</m>
    </p>

  </hint>
</problem>


<problem xml:id="PROBLEMSinPwrSeries">
  <idx><h>series</h><h>solutions of <m>\frac{\dx^2y}{\dx{ x}^2}=-y</m></h></idx>
  <introduction>
    <p>
      The ability to express complex functions as <term>power
      series</term> (<q>infinite polynomials</q>) became a tool of
      paramount importance for solving differential equations in
      the 1700s.
    </p>

  </introduction>

  <task>
    <statement>
      <p>
        Show that if <m>y=\sum_{n=0}^\infty a_nx^n</m>
        satisfies the differential equation
        <men xml:id="EQUATIONSinCosDiffeq">
          \frac{\dx^2y}{\dx{ x}^2}=-y
          </men>, then
          <me>
            a_{n+2}=\frac{-1}{\left(n+2\right)\left(n+1\right)}\,a_n
          </me>
          and conclude that

          <md>
            <mrow>
              y=a_0+a_1x-\amp{}\frac{1}{2!}a_0x^2-\frac{1}{3!}a_1x^3+\frac{1}{4!}a_0x^4+\frac{1}{5!}a_1x^5
            </mrow>
            <mrow>
              \amp{}-\frac{1}{6!}a_0x^6-\frac{1}{7!}a_1x^7+\cdots.
            </mrow>

          </md>
      </p>
    </statement>
  </task>
  <task>
    <statement>
      <p>
        Since <m>y=\sin x</m> also satisfies <xref ref="EQUATIONSinCosDiffeq" >equation</xref>, we see that
        <md>
          <mrow>
            \sin x=a_0+a_1x-\frac{1}{2!}a_0x^2-\amp{}\frac{1}{3!}a_1x^3+\frac{1}{4!}a_0x^4+\frac{1}{5!}a_1x^5
          </mrow>
          <mrow>
            \amp{}-\frac{1}{6!}\,a_0x^6-\frac{1}{7!}\,a_1x^7+\cdots
          </mrow>

        </md>
        for some constants <m>a_0</m> and <m>a_1</m>.  Show
        that in this case <m>a_0=0</m> and <m>a_1=1</m> and
        obtain 

        <md>
          <mrow> \sin
          x\amp{}=x-\frac{1}{3!}\,x^3+\frac{1}{5!}x^5-\frac{1}{7!}x^7+\cdots</mrow>
          <mrow>
            \amp{}=\sum_{n=0}^\infty\frac{\left(-1\right)^n}{\left(2n+1\right)!}x^{2n+1}.
          </mrow>

        </md>
      </p>
    </statement>
  </task>
</problem>

<problem xml:id="PROBLEMSinCosPwrSeries">
  <idx><h><m>\sin x</m></h><h>derivative of the power series representation</h></idx>
  <idx><h>differentiation</h><h>of <m>\sin x</m> as a power series</h></idx>

  <task>
    <statement>
      <p>
        Use the power series

        <md>
          <mrow>
            \sin x\amp{}=x-\frac{1}{3!}\,x^3+\frac{1}{5!}x^5-\frac{1}{7!}x^7+\cdots
          </mrow>
          <mrow>
            \amp{}=\sum_{n=0}^\infty\frac{\left(-1\right)^n}{\left(2n+1\right)!}x^{2n+1}
          </mrow>

        </md>
        to obtain the power series

        <md>
          <mrow>
            \cos
            x\amp{}=1-\frac{1}{2!}\,x^2+\frac{1}{4!}x^4-\frac{1}{6!}x^6+\cdots
          </mrow>
          <mrow>
            \amp{}=\sum_{n=0}^\infty\frac{\left(-1\right)^n}{\left(2n\right)!}x^{2n}.
          </mrow>

          </md> </p> </statement> </task> <task> <statement> <p> Let

          <me>
            s(x,N)=\sum_{n=0}^N\frac{\left(-1\right)^n}{\left(2n+1\right)!}x^{2n+1}
          </me> 

          and 

          <me>
            c(x,N)=\sum_{n=0}^N\frac{\left(-1\right)^n}{\left(2n\right)!}x^{2n}
          </me> 

          and use a computer algebra system to plot these on the
          interval <m>-4\pi\leq x\leq 4\pi</m>, for <m> N=1,2,5,10,
          15</m>.  Describe what is happening to the graph of the
          power series as <m>N</m> becomes larger.  
        </p> 
      </statement>
    </task> 
</problem>

<problem>

  <statement>

    <p>
      Use the power series for <m>\sin (x)</m> to compute
      
      <me>
        \int^b_{x=0}\sin\left(x^2\right)\dx{x}
      </me>
      as a power series.

    </p>

  </statement>
</problem>

<problem xml:id="PROBLEMResultsFromHarmonicSeries">
  <idx><h>series</h><h>Geometric series</h><h>used to derive arctangent series</h></idx>
  <idx><h>series</h><h><m>\tan^{-1}x</m></h></idx>
  <task>
    <statement>
      <p>
        Use the <xref ref="EQUATIONGeometricSeries" text="custom" >Geometric series</xref> 
        to obtain a power series for 
        <me>
          f(x)=\frac{1}{1+x}
          </me>,
          and use your series to show that 
          <m>
            \sum_{n=0}^\infty \frac{(-1)^n}{2^n} = \frac23 
            </m>.
      </p>
    </statement>
  </task>
  <task>
    <statement>
      
      <p>
        Use the power series you found  in part (a) 
        to obtain a power series for         
        <me>
          f(x)=\frac{1}{1+x^2}
          </me>,
          and use your series to show that <m>\sum_{n=0}^\infty
          \frac{(-1)^n}{2^{2n}}=\frac45</m>.
      </p>
    </statement>
  </task>
  <task>
    <statement>
      <p>
        Use the result in part (b) to obtain the power series

        <me>
          \arctan
          x=x-\frac{1}{3}x^3+\frac{1}{5}x^5-\cdots
          =\sum_{n=0}^\infty(-1)^n
          \frac{1}{2n+1}x^{2n+1}
          </me>, and use the series to show that
        <m>
          \sum_{n=0}^\infty\frac{(-1)^n}{2n+1} = \frac{\pi}{4} </m>.
      </p>
    </statement>
  </task>
</problem>

<problem>

  <statement>
    <p>
      Compute 
      <me>\int_{x=0}^1 \frac{1}{1+x^2}\dx{x}</me>
      as a power series.
    </p>

  </statement>
</problem>

<problem xml:id="PROBLEMLnSeriesFromGeoSeries">
  <idx><h>series</h><h>Geometric series</h><h>alternating</h></idx>
  <idx><h>series</h><h>Geometric series</h><h>derivation of the series representation of <m>\ln(1+x)</m> from</h></idx>
  <task>
    <statement>
      <p>
        Use the <xref ref="EQUATIONGeometricSeries" text="custom"
        >Geometric series</xref> to obtain the power series
        <md>
          <mrow>\ln \left(1+x\right)\amp =x-\frac{1}{2}x^2+\frac{1}{3}x^3-\cdots</mrow>
          <mrow>\amp =\sum_{n=0}^\infty\frac{(-1)^n}{n+1}x^{n+1}.{}</mrow>
        </md>
      </p>

    </statement>
    <hint>
      <p>
        Recall that <m>\ln(1+x) = \int \frac{1}{1+x}\dx{x}</m>.
      </p>
    </hint>
  </task>
  <!-- <task> -->
  <!--   <statement> -->
  <!--     <p> -->
  <!--       Use the result of the previous problem to represent the -->
  <!--       function         <m>\ln \left(1+x^2\right)</m> as power series expanded about <m>0</m>. -->
  <!--     </p> -->
  <!--   </statement> -->
  <!-- </task> -->
  <task>
    <statement>
      <p>
        Use the result of part (a) to represent the function <m>\ln
        \left(1+x^2\right)</m> as a power series expanded about
        <m>0</m>.
      </p>
    </statement>
  </task>
  <task>
    <statement>
      <p>
        Use the result of part (a) represent the function <m>\ln
        \left(2+x\right)</m> as power series expanded about <m>0</m>.
      </p>
    </statement>
    <hint>
      <p>

        <m>2+x=2\left(1+\frac{x}{2}\right)</m>
      </p>

    </hint>
  </task>
</problem>


<problem>
  <statement>
    <p>
      Use the <xref ref="EQUATIONGeometricSeries" text="custom"
      >Geometric series</xref> to find a power series
      representation for <m>\frac{2x}{1+x^2}</m>.  Integrate
      this to obtain a power series representation for
      <m>\ln\left(1+x^2\right)</m> and compare your answer to
      part (b) of the previous problem. (This shows that there
      may be more than one way to obtain a power series
      representation.)
    </p>
  </statement>
</problem>

<p>
  The power series for arctangent was known by <url
  href="https://mathshistory.st-andrews.ac.uk/Biographies/Gregory/"
  visual="mathshistory.st-andrews.ac.uk/Biographies/Gregory/">James
  Gregory</url> (1638-1675) and it is sometimes referred to as
  <q>Gregory<rsq/>s series.</q> Leibniz<idx><h>Leibniz, Gottfried
  Wilhelm</h></idx> independently discovered
  <m>\frac{\pi}{4}=1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\cdots</m> by
  examining the area of a circle.  Though it gives us a means for
  approximating <m>\pi</m> to any desired accuracy, the power series
  converges too slowly to be of any practical use.  For example, if we
  compute the sum of the first <m>1000</m> terms we get
  <me>
    4\left(\sum_{n=0}^{1000}(-1)^n\frac{1}{2n+1}\right)\approx 3.142591654
  </me>
  which only approximates <m>\pi</m> to two decimal places.
</p>

<figure  xml:id="FIGUREJamesGregoryPortrait">
  <caption>James Gregory</caption>
  <image source="images/JamesGregory.png" width="35%">
    <shortdescription>Portrait of James Gregory</shortdescription>
  </image>
</figure>

<p>
  <idx><h>Newton, Isaac</h></idx>

  Newton knew of these results and the general scheme of using power
  series to compute areas under curves. He used these results to
  provide a power series approximation for <m>\pi</m> as well, which,
  hopefully, would converge faster.  We will use modern terminology to
  streamline Newton<rsq/>s ideas.  First notice that
  <m>\frac{\pi}{4}=\int_{x=0}^1\sqrt{1-x^2}\dx{ x}</m> as this
  integral gives the area of one quarter of the unit circle,
  <m>\frac{\pi }{4}</m>.  The trick now is to find a power series that
  represents <m>\sqrt{1-x^2}</m>.  </p>

<p>
  To this end we start with the binomial theorem
  <me>
    \left(a+b\right)^N=\sum_{n=0}^N\binom{N}{n}a^{N-n}b^n
    </me>,
    where
    <md>
      <mrow>\binom{N}{n}\amp =\frac{N!}{n!\left(N-n\right)!}</mrow>
      <mrow>\amp =\frac{N\left(N-1\right)\left(N-2\right)\cdots\left(N-n+1\,\right)}{n!}</mrow>
      <mrow>\amp =\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}</mrow>
      </md>.
</p>

<p>
  Unfortunately, we now have a small problem with our notation
  which will be a source of confusion later if we don<rsq/>t fix
  it.  So we will pause to address this matter.  We will come
  back to the binomial expansion afterward.
</p>

<p>
  This last expression is becoming awkward in much the same
  way that an expression like
  <me>
    1+\frac{1}{2}+\left(\frac{1}{2}\right)^2+\left(\frac{1}{2}\right)^3+\ldots+\left(\frac{1}{2}\right)^k
  </me>
  is awkward.  Just as this sum is less cumbersome when
  written as <m>\sum_{n=0}^k\left(\frac{1}{2}\right)^n</m> the
  <em>product</em>
  <me>
    N\left(N-1\right)\left(N-2\right)\cdots\left(N-n+1\,\right)
  </me>
  is less cumbersome when we write it as
  <m>\prod_{j=0}^{n-1}\left(N-j\right)</m>.
</p>

<p>
  A capital pi <m>\left(\Pi\right)</m> is used to denote a product
  in the same way that a capital sigma <m>\left(\Sigma\right)</m>
  is used to denote a sum.  The most familiar example would be
  writing
  <me>
    n!=\prod_{j=1}^{n}j
    </me>.
</p>

<p>
  Just as it is convenient to define <m>0!=1</m>, we will find
  it convenient to define <m>\prod_{j=1}^{0}=1</m>.
  Similarly, the fact that <m>\binom{N}{0}=1</m> leads to the
  convention <m>\prod_{j=0}^{-1}\left(N-j\right)=1</m>.
  Strange as this may look, it is convenient and is
  consistent with the convention <m>\sum_{j=0}^{-1}s_j=0</m>.
</p>

<p>
  Returning to the binomial expansion and recalling our convention
  <me>
    \prod_{j=0}^{-1}\left(N-j\right)=1
    </me>,
    we can write,
    <me>
      \left(1+x\right)^N=1+\sum_{n=1}^N\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\right)x^n = \sum_{n=0}^N\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\right)x^n
      </me>.
</p>
<p>
  These two representations probably look the same at first.
  Take a moment and be sure you see where they differ.
</p> 

<p>
  There is an advantage to using this convention (especially
  when programing a product into a computer), but this is not
  a deep mathematical insight.  It is just a notational
  convenience and we don<rsq/>t want you to fret over it, so we
  will use both formulations (at least initially).
</p>

<p>
  Notice that we can extend the above definition of
  <m>\binom{N}{n}</m> to values <m>n>N</m>.  In this case,
  <m>\prod_{j=0}^{n-1}\left(N-j\right)</m> will equal 0 as one
  of the factors in the product will be <m>0</m> (the one
  where <m>j=N</m>).  This gives us that <m>\binom{N}{n}=0</m>
  when <m>n>N</m> and so
  <me>
    \left(1+x\right)^N=1+\sum_{n=1}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\text{ } \right)x^n= \sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\text{ } \right)x^n
  </me>
  holds true for any nonnegative integer <m>N</m>.
  Essentially Newton asked if it could be possible that the
  above equation could hold values of <m>N</m> which are not
  nonnegative integers.  For example, if the equation held
  true for <m>N=\frac{1}{2}</m> , we would obtain
  <me>
    \left(1+x\right)^{\frac{1}{2}}=1+\sum_{n=1}^\infty\left(\frac{ \prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\right)x^n=\sum_{n=0}^\infty\left(\frac{ \prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\right)x^n
  </me>
  or
  <men xml:id="eq_BinomialSeries">
    \left(1+x\right)^{\frac{1}{2}}=1+\frac{1}{2}x+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)}{2!}x^2+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right)}{3!}x^3+\cdots
    </men>.
</p>

<p>
  Notice that since <m>\frac{1}{2}</m> is not an integer the power
  series no longer terminates.  Although Newton did not prove that
  this power series was correct (nor did we), he tested it by
  multiplying the power series by itself.  When he saw that by
  squaring the power series he started to obtain
  <m>1+x+0\,x^2+0\,x^3+\cdots</m>, he was convinced that the power
  series was exactly equal to <m>\sqrt{1+x}</m>.
</p>

<problem>
  <idx><h>Binomial Series, the</h><h>squaring the</h></idx>
  <statement>
    <p>
      Consider the power series representation
      <md>
        <mrow>\left(1+x\right)^{\frac{1}{2}}\amp =1+\sum_{n=1}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n</mrow>
        <mrow>\amp  =\sum_{n=0}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n</mrow>
        </md>.
    </p>

    <p>
      Multiply this power series by itself and compute the
      coefficients for <m>x^0,\,x^1,\,x^2,\,x^3,\,x^4</m> in
      the resulting power series.
    </p>
  </statement>
</problem>

<problem xml:id="prob_SqrtSeriesProb">
  <idx><h>series</h><h>graph the square root power series</h></idx>
  <statement>
    <p>
      Let
      <me>
        S(x,M)=\sum_{n=0}^M\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j \right)}{n!}x^n
        </me>.
    </p>

    <p>
      Use a computer algebra system to plot <m>S(x,M)</m> for
      <m>M=5, 10, 15, 95, 100</m> and compare these to the
      graph for <m>\sqrt{1+x}</m>.  What seems to be
      happening?  For what values of <m>x</m> does the power series
      appear to converge to <m>\sqrt{1+x}?</m>
    </p>
  </statement>
</problem>

<p>
  Convinced that he had the correct power series, Newton used it to
  find a power series representation of <m>\int_{x=0}^1\sqrt{1-x^2}
  \dx{ x}</m>.
</p>

<problem>
  <idx><h><m>\pi</m></h><h>first series expansion</h></idx>
  <statement>
    <p>
      Use the power series <m>\displaystyle
      \left(1+x\right)^{\frac{1}{2}}=\sum_{n=0}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n</m>
      to obtain the power series
      <md>
        <mrow>\frac{\pi}{4}\amp =\int_{x=0}^1\sqrt{1-x^2} \dx{ x}</mrow>
        <mrow>\amp =\sum_{n=0}^\infty\left[\left(\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\text{ } \right)\left(\frac{\left(-1\right)^n}{2n+1}\right)\right]</mrow>
        <mrow>\amp =1-\frac{1}{6}-\frac{1}{40}-\frac{1}{112}-\frac{5}{1152}-\cdots</mrow>
        </md>.
    </p>

    <p>
      Use a computer algebra system to sum the first 100 terms
      of this power series and compare the answer to
      <m>\frac{\pi}{4}</m>.
    </p>
  </statement>
</problem>

<p>
  Again, Newton had a power series which could be verified
  (somewhat) computationally.  This convinced him even further
  that he had the correct power series.
</p>

<problem>
  <idx><h><m>\pi</m></h><h>second series expansion</h></idx>
  <statement>
    <p>
      <ol marker="(a)">
        <li>
          <p>
            Show that
            <me>
              \int_{x=0}^{1/2}\sqrt{x-x^2}\dx{ x}=\sum_{n=0}^\infty\frac{(-1)^n\,\,\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{\sqrt{2\,}n!\left(2n+3\right)2^n}
            </me>
            and use this to show that
            <me>
              \pi=16\left(\sum_{n=0}^\infty\frac{(-1)^n\,\,\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{\sqrt{2\,}n!\left(2n+3\right)2^n}\right)
              </me>.
          </p>
        </li>
        <li>
          <p>
            We now have two power series for calculating <m>\pi</m>:
            the one from part (a) and the one derived earlier,
            namely <me>
            \pi=4\left(\sum_{n=0}^\infty\frac{(-1)^n\,\,}{2n+1}\right)
            </me>.  We will explore which one converges to
            <m>\pi</m> faster. First define
            <me>
              S1(N)=16\left(\sum_{n=0}^N\frac{(-1)^n\,\,\prod_{j=0}^{n-1}\left(
              \frac{1}{2}-j\right)}{\sqrt{2\,}n!\left(2n+3\right)2^n}\right)
            </me>
            and
            <me>
              S2(N)=4\left(\sum_{n=0}^N\frac{(-1)^n\,\,}{2n+1}\right)
              </me>.
              Use a computer algebra system to compute
              <m>S1(N)</m>and <m>S2(N)</m> for
              <m>N=5,10,15,20</m>.  Which one appears to
              converge to <m>\pi</m> faster?
          </p>
        </li>
      </ol>
    </p>
  </statement>
</problem>

<p>
  In general the power series representation
  <md>
    <mrow>\left(1+x\right)^\alpha \amp  =\sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(\alpha-j\right)}{n!}\text{ } \right)x^n</mrow>
    <mrow>\amp =1+\alpha x+\frac{\alpha\left(\alpha-1\right)}{2!}x^2+\frac{\alpha\left(\alpha-1\right)\left(\alpha-2\right)}{3!}x^3+\cdots</mrow>
  </md>
  is called the <term>binomial series</term> (or Newton<rsq/>s
  binomial series).  This power series is correct when <m>\alpha</m>
  is a non-negative integer (after all, that is how we got the
  series in the first place).  We can also see that it is correct when
  <m>\alpha=-1</m> as we obtain
  <md>
    <mrow>\left(1+x\right)^{-1}\amp =\sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(-1-j\right)}{n!}\text{ } \right)x^n</mrow>
    <mrow>\amp =1+(-1)x+\frac{-1\left(-1-1\right)}{2!}x^2+\frac{-1\left(-1-1\right)\left(-1-2\right)}{3!}x^3+\cdots</mrow>
    <mrow>\amp =1-x+x^2-x^3+\cdots</mrow>
  </md>
  which can be obtained from the geometric series
  <m>\frac{1}{1-x}=1+x+x^2+\cdots</m> .
</p>

<p>
  In fact, the binomial series is the correct power series
  representation for all values of the exponent <m>\alpha</m>
  (though we haven<rsq/>t proved this yet).
</p>

<problem>
  <task>
    <statement>
      <p>
        Assuming that the binomial series works for <m>\alpha =-\frac{1}{2}</m>, show that

        <md>
        <mrow> 
          \frac{1}{\sqrt{1-x^2}}\amp{}=\sum^{\infty}_{n=0}{\frac{\left(\prod^{n-1}_{j=0}{\left(\frac{1}{2}+j\right)}\right)}{n!}x^{2n}}
          </mrow> 
          <mrow> 
            \amp{}
            =1+\frac{1}{2}x^2+\frac{\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)}{2!}x^4+\frac{\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)\left(\frac{5}{2}\right)}{3!}x^6+\dots.
          </mrow>

        </md>

      </p>
    </statement>
  </task>
  <task>
    <statement>
      <p>
        Integrate the above to obtain the following power series for <m>\arcsin (x)</m>.

        <md>
          <mrow>
            \arcsin \left(x\right)\amp{}=\sum^{\infty
            }_{n=0}{\frac{\left(\prod^{n-1}_{j=0}{\left(\frac{1}{2}+j\right)}\right)}{n!\left(2n+1\right)}x^{2n+1}}
            </mrow>
            <mrow>
              \amp{}=x+\frac{\frac{1}{2}}{3}x^3+\frac{\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)}{2!5}x^5+\frac{\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)\left(\frac{5}{2}\right)}{3!7}x^7+\dots 
            </mrow>

        </md>


      </p>
    </statement>
  </task>
  <task>
    <statement>
      <p>
        Substitute <m>x=\frac{1}{2}</m> into the above power series to
        obtain a power series representation for <m>\frac{\pi
        }{6}</m>.  Add the first four terms of this power series to obtain
        an approximation for <m>\pi </m>, and compare with <m>\pi
        \approx 3.14159265359</m>.  How close did your approximation
        come?
      </p>
    </statement>
  </task>
</problem>

<problem>
  <idx><h>Binomial Series, the</h></idx>
  <idx><h>Binomial Series, the</h><h>as a power series centered at zero</h></idx>
  <idx><h>series</h><h>Geometric series</h><h>differentiating</h></idx>
  <introduction>
    <p>
      Let <m>k</m> be a positive integer.  Find the power
      series, centered at zero, for <m>f(x) =
      \left(1-x\right)^{-k}</m> by
    </p>
</introduction>
<task>
  <statement>
    <p>
      Differentiating the <xref ref="EQUATIONGeometricSeries"
      text="custom">Geometric series</xref> <m>\left(k-1\right)</m>
      times.
    </p>
  </statement>
</task>
<task>
  <statement>
    <p>
      Applying the binomial series.
    </p>
  </statement>
</task>
<task>
  <statement>
    <p>
      Compare the results in parts (a) and (b).
    </p>
  </statement>
</task>
</problem>


<p>
  <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Euler/" visual="mathshistory.st-andrews.ac.uk/Biographies/Euler/">Leonhard Euler</url> (1707<ndash/>1783) was a master at exploiting power series.  In
  1735, the 28 year-old Euler won acclaim for what is now
  called the Basel problem: to evaluate the sum
  <me>
    \sum_{n=1}^\infty\frac{1}{n^2}
    </me>.  

    Other mathematicans had shown that the power series converged, but
    Euler was the first to find its exact value.  The following
    problem essentially provides Euler<rsq/>s solution.
</p>
<figure>
  <caption>Leonhard Euler</caption>
  <idx><h>Euler, Leonhard</h><h>portrait of</h></idx>
  <image width="35%" source="images/Euler.png" >
    <shortdescription>Portrait of Euler</shortdescription>
  </image>
</figure>

<problem><title>The Basel Problem</title>
<idx><h>Euler, Leonhard</h><h>Basel Problem, the</h></idx>

<introduction>
  <p>
    Recall that in <xref ref="PROBLEMSinCosPwrSeries" ></xref> we
    developed a power series representation of the function  <m>\sin x</m>.
  </p>
</introduction>

<task>
  <statement>
    <p>
      Show that the power series for <m>\frac{\sin x}{x}</m>
      is given by
      <m>1-\frac{1}{3!}x^2+\frac{1}{5!}x^4-\cdots</m>
    </p>
  </statement>
</task>
<task>
  <statement>
    <p>
      Use part (a) to infer that the roots of
      <m>1-\frac{1}{3!}x^2+\frac{1}{5!}x^4-\cdots</m> are
      given by
      <me>
        x=\pm\pi,\,\pm 2\pi,\,\pm 3\pi,\,\ldots
      </me>
    </p>
  </statement>
</task>
<task>
  <statement>
    <p>
      Suppose <m>p(x)=a_0+a_1x+\cdots+a_nx^n</m> is a
      polynomial with roots <m>r_1,\,r_2,\,\ldots,r_n</m>.
      Show that if <m>a_0\neq</m> <m>0</m>, then all the roots
      are non-zero and
      <me>
        p(x)=a_0\left(1-\frac{x}{r_1}\right)\left(1-\frac{x}{r_2}\right)\cdots\left(1-\frac{x}{r_n}\right)
        </me>.
    </p>
  </statement>
</task>
<task>
  <statement>
    <p>
      Assuming that the result in part (c) holds for an
      infinite polynomial (power series), deduce that
      <me>
        1-\frac{1}{3!}x^2+\frac{1}{5!}x^4-\cdots =
        \left(1-\left(\frac{x}{\pi}\right)^2\right)
        \left(1-\left(\frac{x}{2\pi}\right)^2\right)
        \left(1-\left(\frac{x}{3\pi}\right)^2\right)\cdots
      </me>
</p>
  </statement>
</task>
<task>
  <statement>
    <p>
      Expand this product to deduce that
      <me>
        \sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6}.{}
      </me>
    </p>
  </statement>
</task>
</problem>

<problem><title>Euler<rsq/>s Formula</title>
<idx><h>Euler, Leonhard</h><h>Euler<rsq/>s Formula</h></idx>

<task>
  <statement>
    <p>
      Use the power series expansion of <m>e^x</m>, <m>\sin
      x,</m> and <m>\cos x</m> to derive <term>Euler<rsq/>s
      Formula</term>:
      <me>
        e^{i\theta} = \cos\theta+i\sin\theta.
      </me>
    </p>
  </statement>
</task>
<task>
  <statement>
    <p>
      Use Euler<rsq/>s formula to derive the
      Addition/Subtraction formulas from Trigonometry:
      <me>
        \sin(\alpha\pm\beta) = \sin\alpha\cos\beta\pm\sin\beta\cos\alpha
      </me>
      <me>
        \cos(\alpha\pm\beta) = \cos\alpha\cos\beta\mp\sin\alpha\sin\beta
      </me>
    </p>
  </statement>
</task>
<task>
  <statement>
    <p>
      Use Euler<rsq/>s formula to show that
      <me>
        \sin 2\theta = 2\cos\theta\sin\theta
      </me>
      <me>
        \cos 2\theta =\cos^2\theta-\sin^2\theta
      </me>
    </p>
  </statement>
</task>
<task>
  <statement>
    <p>
      Use Euler<rsq/>s formula to show that
      <me>
        \sin 3\theta = 3\cos^2\theta\sin\theta-\sin^3\theta
      </me>
      <me>
        \cos 3\theta=\cos^3\theta-3\cos\theta\sin^2\theta
      </me>
    </p>
  </statement>
</task>
<task>
  <statement>
    <p>
      Find a formula <m>\sin(n\theta)</m> and
      <m>\cos(n\theta)</m> for any positive integer <m>n</m>.
    </p>
  </statement>
</task>
</problem>

</section>

<section xml:id="SECTIONPowSerWOTaylor">
  <title>Expanding Simple Power Series by Algebraic Methods</title>
  <p>
    We call the power series expansions we<rsq/>ll see in this section
    <q>simple</q> because all that is needed to generate them is prior knowledge of a few series (e.g.,the
    <xref ref="EQUATIONGeometricSeries" text="custom">Geometric
    Series</xref>, the <xref ref="PROBLEMSinCosPwrSeries" text="custom">sine and cosine series</xref>, the <xref ref="EQUATIONExpSeries"  text="custom">exponential series</xref>, the <xref ref="eq_BinomialSeries" text="custom" >Binomial Series</xref>),  and a creative use of algebra. In particular <xref
    ref="TaylorsTheorem" text="custom">Taylor<rsq/>s Theorem</xref> is
    not needed. We assume that you are familiar with the use of
    Taylor<rsq/>s Theorem from your Calculus course.
  </p>

  <p>
    As we saw in the last section, it can be particularly fruitful to
    expand a function as a power series centered at <m>a=0</m>.
    Unfortunately, this isn<rsq/>t always possible. For example, it is
    not possible to expand the function
    <m>f\left(x\right)=\frac{1}{x}</m> about zero. (Why not?)
  </p>

  <p>
    However we are not confined to expanding about zero.
    Consider that the following
    
    <me>
      \frac{1}{x}=\frac{1}{1+\left(x-1\right)}=\sum^{\infty}_{n=0}{-1^n{\left(x-1\right)}^n}
    </me> 


    is a power series for <m>f(x)=\frac{1}{x}</m> expanded about
    <m>a=1</m>.  Of course, there are still questions that need to be
    resolved. For example, for which values of <m>x</m> is this series
    a valid representation of the function we started with?  We will
    explore this in <xref
    ref="PowerSeriesQuestions-TaylorsFormula"></xref>, but for now at
    least we have a representation which seems reasonable.
  </p>
   
  <problem xml:id="PROBLEMOneOverXTayl">
    <introduction>
      <p>
        Let <m>a\neq 0</m>.
      </p>
    </introduction>
    <task>
      <statement>
        <p>
          Represent <m>\frac{1}{x}</m> as a power
          series expanded about <m>a</m>.  That is, as a power series of the
          form <m>\sum^{\infty }_{n=0}{a_n{(x-a)}^n}</m>.
        </p>
      </statement>
      <hint>
        <p>
          <m>\frac{1}{x}=\frac{1}{a+x-a}=\frac{1}{a}\left(\frac{1}{1+\frac{x-a}{a}}\right)</m>
        </p>
      </hint>  

      </task>
      <task>
        <statement>
          <p>
            Represent <m>\ln (x)</m> as a power series expanded about <m>a</m> by integrating your solution to part (a).
          </p>
        </statement>
      </task>

      <!-- <task> -->
      <!--   <statement> -->
      <!--     <p> -->
      <!--       By integrating the answer from the previous problem. -->
      <!--     </p> -->
      <!--   </statement> -->
      <!-- </task> -->
      <!-- <task> -->
      <!--   <statement> -->
      <!--     <p> -->
      <!--       By writing <m>\ln (x) =\ln (a+x-a)</m> and using the (known) -->
      <!--       expansion of <m>\ln(x)</m>. -->
      <!--     </p> -->
      <!--   </statement> -->
      <!-- </task> -->

  </problem>
   

  <problem>
    <p>
      Let <m>a>0</m> and use <xref ref="eq_BinomialSeries"
      >equation</xref> to represent <m>\sqrt{x}</m> as a power series
      expanded about <m>a</m>.
    </p>
  </problem>
  
  <problem>
    <p>
      Let <m>a</m> be a real number.  Represent <m>e^x</m> as power
      series expanded about <m>a</m>.  Notice there is no restriction
      on <m>a</m>.  What happens if <m>a=0</m>?
    </p>
</problem>
  
<problem>
  <p>
    Let <m>a</m> be a real number.  Represent <m>x^3+2x^2+3</m> as a
    power series expanded about <m>a</m>.  What happens if <m>a=0</m>
    here?
  </p>
</problem>
  
<problem xml:id="PROBLEMSinExpandedAta">

  <statement>
<p>
  Let <m>a</m> be a real number.  Use the power series expansions

  <md>

    <mrow>
      \sin \left(x\right)=\sum^{\infty
      }_{n=0}{\frac{{\left(-1\right)}^n}{\left(2n+1\right)!}x^{2n+1}}\amp{}\amp{} \left(x\right)=\sum^{\infty
      }_{n=0}{\frac{{\left(-1\right)}^n}{\left(2n\right)!}x^{2n}}
    </mrow>

  </md>

  to obtain the power series representation

  <md>
    <mrow>
      \sin (x)=\sin (a)+\amp{}\cos \left(a\right)\left(x-a\right)-\frac{1}{2!}\sin
      \left(a\right){\left(x-a\right)}^2
    </mrow>
    <mrow>
      \amp{}-\frac{1}{3!}\cos
      \left(a\right){\left(x-a\right)}^3+\frac{1}{4!}\sin
      \left(a\right){\left(x-a\right)}^4
    </mrow>
    <mrow>
      \amp{}\ \ \ \ \ +\frac{1}{5!}\cos
      \left(a\right){\left(x-a\right)}^5+\cdots.
    </mrow>
  </md>

</p>
<p>
  This result will come into play in the next section.

</p>

</statement>
<hint>
<p>
  <m >\sin \left(x\right) =\sin \left(a+\left(x-a\right)\right)</m>
</p>
</hint>
</problem>
</section>


<!-- <section xml:id="CalcIn17th18thCentury-AddProb"> -->
<!--   <title>Additional Problems</title> -->
<!--   <p> -->
<!--     We assume that you were introduced to the Taylor<rsq/> series in -->
<!--     your Calculus course. In the next chapter we will discuss -->
<!--     Taylor<rsq/>s series in considerable detail. But since we have not -->
<!--     done that yet you are not allowed to use Taylor<rsq/>s series to -->
<!--     solve these problems. -->
<!--   </p> -->



<!--   <problem> -->
<!--     <idx><h>power series</h><h> drills</h></idx> -->
<!--     <introduction> -->

<!--       <p> -->
<!--         <em>Without</em> using Taylor<rsq/>s Theorem, represent the -->
<!--         following functions as power series expanded about 0 -->
<!--         (i.e., in the form <m>\sum_{n=0}^\infty a_nx^n</m>). -->
<!--       </p> -->
<!--     </introduction> -->
<!--     <task> -->
<!--       <statement> -->
<!--         <p> -->
<!--           Use the result of <xref -->
<!--           ref="PROBLEMLnSeriesFromGeoSeries"></xref> to find the -->
<!--           series representation of <me>\ln(1+x^2) </me>. -->
<!--         </p> -->
<!--       </statement> -->
<!--     </task> -->
<!--     <task> -->
<!--       <statement> -->
<!--         <p> -->
<!--           <m>f(x)=\ln\left(2+x\right)</m> -->
<!--         </p> -->
<!--       </statement> -->
<!--       <hint> -->
<!--         <p> -->
<!--           <m>2+x=2\left(1+\frac{x}{2}\right)</m> -->
<!--         </p> -->
<!--       </hint> -->
<!--     </task> -->
<!--     <task> -->
<!--       <statement> -->
<!--         <p> -->
<!--           Use the Geometric Series to find the series representation of  -->
<!--           <me>f(x) = \frac{1}{1+x^2}</me> -->
<!--         </p> -->
<!--       </statement> -->
<!--     </task> -->

<!--     <task> -->
<!--       <statement> -->
<!--         <p> -->
<!--           Use the result of part (c) to find the series representation of  -->
<!--           <me> -->
<!--             f(x)=\frac{x}{1+x^2}  -->
<!--             </me>. -->
<!--         </p> -->
<!--       </statement> -->
<!--     </task>  -->

<!--     <task> -->
<!--       <statement> -->
<!--         <p> -->
<!--           Use the result of part (d) to find the series representation of  -->
<!--           <!-\- <m>\ln\left(1-x^2\right)</m> -\-> -->
<!--           <me> -->
<!--             f(x)= \frac{1}{1-x^2} -->
<!--             </me>. -->
<!--         </p> -->
<!--       </statement> -->
<!--     </task> -->
<!--     <task> -->
<!--       <statement> -->
<!--         <p> -->
<!--           Use the result of part (c) to find the series representation of  -->
<!--           <me>\displaystyle f(x) =\arctan \left(x^3\right)</me> -->
<!--         </p> -->
<!--       </statement> -->
<!--     </task> -->


<!--     <task> -->
<!--       <statement> -->
<!--         <p> -->
<!--           <m>f(x)=a^x</m>, where <m>a\gt 0</m> -->
<!--         </p> -->
<!--       </statement> -->
<!--       <hint> -->
<!--         <p> -->
<!--           <m>a^x=e^{\ln\,\left(a^x\right)}</m> -->
<!--         </p> -->
<!--       </hint> -->
<!--     </task> -->

<!--   </problem> -->

<!--   <!-\- <problem> -\-> -->
<!--   <!-\-   <idx><h>power series</h><h>for <m>a^x</m> expanded about 0</h></idx> -\-> -->
<!--   <!-\-   <statement> -\-> -->

<!--   <!-\-     <p> -\-> -->
<!--   <!-\-       Let <m>a</m> be a positive real number.  Find a power -\-> -->
<!--   <!-\-       series for <m>a^x</m> expanded about 0. -\-> -->
<!--   <!-\-     </p> -\-> -->
<!--   <!-\-   </statement> -\-> -->
<!--   <!-\-   <hint> -\-> -->
<!--   <!-\-     <p> -\-> -->
<!--   <!-\-       <m>a^x=e^{\ln\,\left(a^x\right)}</m> -\-> -->
<!--   <!-\-     </p> -\-> -->
<!--   <!-\-   </hint> -\-> -->
<!--   <!-\- </problem> -\-> -->
<!--   <!-\- <aside> -\-> -->
<!--   <!-\-   <title>Note to Self</title> -\-> -->
<!--   <!-\-   <p> -\-> -->
<!--   <!-\-     Should this problem be in <xref ref="PowerSeriesQuestions-TaylorsFormula" ></xref> since that is where we give the Taylor expansion? -\-> -->
<!--   <!-\-   </p> -\-> -->
<!--   <!-\- </aside> -\-> -->

<!--   <!-\- <problem> -\-> -->
<!--   <!-\-   <idx><h>Taylor series drills</h></idx>  -\-> -->
<!--   <!-\-   <introduction> -\-> -->
<!--   <!-\-     <p> -\-> -->
<!--   <!-\-       <em>Without</em> using Taylor<rsq/>s Theorem, represent the -\-> -->
<!--   <!-\-       following functions as a power series expanded about -\-> -->
<!--   <!-\-       <m>a</m> for the given value of <m>a</m> (i.e., in the -\-> -->
<!--   <!-\-       form <m>\sum_{n=0}^\infty a_n\left(x-a\right)^n</m>). -\-> -->
<!--   <!-\-     </p> -\-> -->
<!--   <!-\-   </introduction> -\-> -->
<!--   <!-\-   <task> -\-> -->
<!--   <!-\-     <statement> -\-> -->
<!--   <!-\-       <p> -\-> -->
<!--   <!-\-         <m>\ln x</m>, <m>a=1</m> -\-> -->
<!--   <!-\-       </p> -\-> -->
<!--   <!-\-     </statement> -\-> -->
<!--   <!-\-     <!-\\-         <hint> -\\-> -\-> -->
<!--   <!-\-     <!-\\-           <p> -\\-> -\-> -->
<!--   <!-\-     <!-\\-             Use <xref ref="PROBLEMLnSeriesFromGeoSeries" ></xref> and -\\-> -\-> -->
<!--   <!-\-     <!-\\-             observe that <m>x=1+(x-1)</m>. -\\-> -\-> -->
<!--   <!-\-     <!-\\-           </p> -\\-> -\-> -->
<!--   <!-\-     <!-\\- </hint> -\\-> -\-> -->

<!--   <!-\-   </task> -\-> -->

<!--   <!-\-   <task> -\-> -->
<!--   <!-\-     <statement> -\-> -->
<!--   <!-\-       <p> -\-> -->
<!--   <!-\-         <m>e^x</m>, <m>a=3</m> -\-> -->
<!--   <!-\-       </p> -\-> -->
<!--   <!-\-     </statement> -\-> -->
<!--   <!-\-   </task> -\-> -->

<!--   <!-\-   <task> -\-> -->
<!--   <!-\-     <statement> -\-> -->
<!--   <!-\-       <p> -\-> -->
<!--   <!-\-         <m>x^3+2x^2+3</m> , <m>a=1</m> -\-> -->
<!--   <!-\-       </p> -\-> -->
<!--   <!-\-     </statement> -\-> -->
<!--   <!-\-   </task> -\-> -->

<!--   <!-\-   <task> -\-> -->
<!--   <!-\-     <statement> -\-> -->
<!--   <!-\-       <p> -\-> -->
<!--   <!-\-         <m>\frac{1}{x}</m> , <m>a=5</m> -\-> -->
<!--   <!-\-       </p> -\-> -->
<!--   <!-\-     </statement> -\-> -->
<!--   <!-\-   </task>  -\-> -->

<!--   <!-\- </problem> -\-> -->

<!--   <problem> -->
<!--     <statement> -->
<!--       <p> -->
<!--         Expand the function -->
<!--         <m>f(x)=x^3+2x^2+3</m> as a series about the point  <m>a=1</m>. -->
<!--       </p> -->
<!--     </statement> -->
<!--     <hint> -->
<!--       <p> -->
<!--         Find the coefficients <m>A_0</m>, <m>A_1</m>, and <m>A_2</m> for which  -->
<!--         <me> -->
<!--           x^3+2x^2+3 = A_2(x-1)^2+A_1(x-1)+A_0 -->
<!--         </me> -->
<!--       </p> -->
<!--     </hint> -->

<!--   </problem> -->

<!--   <problem> -->
<!--     <idx><h>series</h><h>term by term integration of </h></idx> -->
<!--     <introduction> -->
<!--       <p> -->
<!--         Evaluate the following integrals as series. That is, replace each integrand with it<rsq/>s series representation and compute the integral term<ndash/>by<ndash/>term. -->
<!--       </p> -->
<!--     </introduction> -->
<!--     <task> -->
<!--       <statement> -->
<!--         <p> -->
<!--           <m>\displaystyle\int_{x=0}^1e^{x^2}\dx{ x}</m> -->
<!--         </p> -->
<!--       </statement> -->
<!--     </task> -->
<!--     <task> -->
<!--       <statement> -->
<!--         <p> -->
<!--           <m>\displaystyle\int_{x=0}^1\frac{1}{1+x^4}\dx{ x}</m> -->
<!--         </p> -->
<!--       </statement> -->
<!--     </task> -->
<!--     <task> -->
<!--       <statement> -->
<!--         <p> -->
<!--           <m>            \displaystyle\int_{x=0}^1 \sqrt[3]{1-x^3}\dx{x}</m> -->
<!--         </p> -->
<!--       </statement> -->
<!--     </task> -->


<!--   </problem> -->
<!-- </section> -->


</chapter>

