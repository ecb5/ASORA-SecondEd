<chapter xmlns:xi="http://www.w3.org/2001/XInclude"  xml:id="LimDerivIntFTC">
  <title>Limits, Derivatives, Integrals, and the Fundamental Theorem of Calculus</title>

  <introduction>
    <title>Why Now?</title>
    <!-- <p> -->
    <!--   We briefly discussed the invention of Calculus by Newton and -->
    <!--   Leibniz in <xref ref="CalcIn17th18thCentury" ></xref>.  In -->
    <!--   subsequent examples and problems we have used the fundamental -->
    <!--   notions of <term>limit</term>, <term>derivative</term> and -->
    <!--   <term>integral</term> without going to the trouble of rigorously -->
    <!--   defining these concepts  -->
    <!-- </p> -->
    <!-- <p> -->
    <!--   We (the authors) made this choice primarily because   It is also a more historically accurate treatment. -->
    <!-- </p> -->
    <!-- <p> -->
    <!--   For the most part rigor could be dispensed with in a Calculus -->
    <!--   course because the underlying ideas of the <term>limit</term>, -->
    <!--   the <term>derivative</term> and the <term>integral</term>, are -->
    <!--   relatively intuitive. But an intuitive understanding is simply -->
    <!--   not enough for our purposes here. We will need rigor. -->
    <!-- </p> -->

    <p>
      So far, we (the authors) have refrained from giving rigorous
      definitions of the <term>derivative</term>, and the
      <term>integral</term> before using them. We simply assumed that
      you are familiar with the use of these ideas from your Calculus
      course, even if the details of their definitions are still a bit
      hazy.
    </p>
    <p>
      We made this choice consciously, not because it is more
      historically accurate (though it is), but because we feel
      strongly that it is better for beginners to learn to <q>play the
      game</q> of rigorous analysis using epsilons and deltas in the
      simpler domain of convergent sequences before confronting all of
      the nuance and complexity of a fully rigorous treatment of the
      <term>derivative</term> and, especially, the
      <term>integral</term>.
    </p>
    <p>
      Since the <term>derivative</term> and the <term>integral</term>
      were known and had been used quite literally for centuries
      before they were formally defined their definitions were never
      meant to be intuitive. They were meant to be rigorous.  For that
      reason the definitions do not exist to help us use these
      ideas. They exist to establish a rigorous foundation for
      Calculus, a foundation we can fall back on when the intuitive
      approach is inadequate.
    </p>
    <p>
      Thus we did bend the rules a bit when we relied on your
      intuitive understanding of derivatives and integrals to derive
      the Tayor series and its various remainders. To correct this we
      need to circle back and treat these ideas with the proper level
      of rigor.
    </p>

    <!-- <p> -->
    <!--   So we began by relying heavily on the intuitive understanding of -->
    <!--   derivatives and integrals you gained from your Calculus course, -->
    <!--   and we used that to derive  -->

    <!--   <ul> -->
    <!--     <li> -->
    <!--       the integral, Lagrange, and Cauchy -->
    <!--       form of the remainders of the Taylor series, -->
    <!--     </li> -->
    <!--     <li> -->
    <!--       the Intermediate Values Theorem, -->
    <!--     </li> -->
    <!--     <li> -->
    <!--       the Extreme Value Theorem -->
    <!--     </li> -->
    <!--     <li> -->
    <!--       the convergence of sequences,  -->
    <!--     </li> -->
    <!--     <li> -->
    <!--       continuity, and -->
    <!--     </li> -->
    <!--     <li> -->
    <!--       the completeness of the real number system. -->
    <!--     </li> -->
    <!--   </ul> -->
    <!--   Now that we have all of these tools in place it is time for us -->
    <!--   to go back and build the  foundations of Calculus rigorously. -->
    <!-- </p> -->
  </introduction>

  <section xml:id="Continuity-DefLimit"> 
    <title> The Definition of the Limit of a Function </title>

    <p>
      We<rsq/>ve already used the the notion of a limit and its
      associated notation, <m>\limit{n}{\infty}{a_n}</m>, to analyze
      the convergence and divergence of a sequence. And you<rsq/>ve
      almost certainly encountered limits of functions before so it is
      tempting for us (the authors) to assume that you are already
      well<ndash/>versed in the limit concept and simply plow forward
      with little discussion. We won<rsq/>t do that.
    </p>

    <p>
      And it is probably tempting for you to make the same assumption
      and skip the discussion you are about to see. Don<rsq/>t do
      that. The limit concept is subtle. There is more to it than you
      probably believe and it is worth taking time to think about it
      deeply.
    </p>
    <p>
      For example, the statement <m>\limit{n}{\infty}{2^{-n}}=0</m>
      has a very precise meaning. It means that as <m>n</m> increases
      arbitrarily (<m>\rightarrow \infty </m>) the elements of the
      sequence <me>\left(2^{-0}, 2^{-1}, 2^{-2}, 2^{-3},
      \cdots{}\right)</me> get arbitrarily close to zero <m>(=0)</m>.
      Notice that it does not mean that <m>2^{-n}</m> is actually
      equal to zero for any value of <m>n</m>. That is not true.
    </p>
    <p>
      You might reasonably ask, <q>OK, but what exactly is equal to
      zero?</q> The notation <q>=0</q> clearly states that
      <em>something</em> is equal to zero. What? The simple answer to
      that question is as obvious as it is unhelpful.  It is the limit
      of <m>2^{-n}</m> that is equal to zero.
    </p>
    <p>
      More helpfully, suppose the sequence
      <m>\left(a_n\right)_{n=1}^\infty </m> converges to <m>a</m>.  In
      <xref ref="Convergence" ></xref> we usually said that
      <m>\limit{n}{\infty}{a_n}</m> <term>converged</term> to <m>a</m>
      rather than saying that <m>\limit{n}{\infty}{a_n}</m> was equal
      to <m>a</m>. Unfortunately this idea gets rendered notationally
      as
      <me>
        \limit{n}{\infty}{a_n}=a 
      </me> 
      and when we read it aloud we tend to say that <m>a_n</m> <term>equals
      <m> \boldsymbol{a}</m></term> rather than saying that <m>a_n</m> <term>converges to
      <m>\boldsymbol{a}</m></term>. This is not wrong <foreign>per se</foreign> but it
      can be confusing at first. But ultimately it is the
      <term>limit</term> of the sequence that is equal to <m>a</m>,
      not any individual term, and certainly not the sequence itself.
    </p>
    <p>
      The limit concept has always been lurking in the background of
      Calculus.  Because it is a deep and very abstract idea it took
      about <m>200</m> years to bring it forward clearly. And as
      we<rsq/>ve just seen the notation we<rsq/>ve inherited
      (especially the way the equals sign is used) is more befuddling
      than helpful, at least at first.  We will proceed slowly.
    </p>
    <!-- <p> -->
    <!--   We introduced sequences first because the limit of a sequence (especially a sequence converging to zero) is the simplest kind of non<ndash/>trivial limit to understand.  In this section we<rsq/>ll expand <xref ref="def_ConvergenceOfASequence" ></xref> to the limits of functions -->

    <!--   That<rsq/>s why we  -->
    <!--   <xref ref="def_ConvergenceOfASequence" ></xref> we introduced  -->

    <!--   say that In the context of the convergence of sequences and series (in <xref ref="Convergence" ></xref>) . In this section we<rsq/>ll expand <xref ref="def_ConvergenceOfASequence" ></xref> to the limits of functions, but we<rsq/>ll re<ndash/>use the same notation. This generally does not create any problems but it is worth noting that the notations   gelimit expand the concept, you might think it is a little -->
    <!--   strange that we<rsq/>ve chosen to talk about continuity first. -->
    <!--   But historically, the formal definition of a limit came after -->
    <!--   the formal definition of continuity.   The limit  -->
    <!--   concept has become foundational to Calculus.  -->
    <!-- </p> -->
    <!-- <p> -->
    <!--   Speaking very loosely, when we evaluate the limit of the -->
    <!--   sequence <m>\left( a_n\right)_{n=1}^\infty</m> we let <m>n</m> -->
    <!--   get close to infinity. That is, we<rsq/>re interested in what -->
    <!--   happens at the far right side of the sequence, even though there -->
    <!--   is no <q>far right side.</q> -->
    <!-- </p> -->
    <p>
      <idx><h>Newton, Isaac</h></idx>

      In his attempts to justify his calculations, Newton used what he
      called his doctrine of <term>Ultimate Ratios</term>. For
      example, he would have said that the ratio
      <m>\frac{(x+h)^2-x^2}{h} = \frac{2xh+h^2}{h} = 2x+h</m> becomes
      <m>2x</m> <term>ultimately</term>, or at the last instant before
      <m>h</m> becomes zero. Newton would have called <m>h</m> an
      <q>evanescent</q> or <q>vanishing quantity</q> (<xref
      ref="grabiner81__origin_cauch_rigor_calculy"/>, p. 33).
    </p>
    <p>
      <!-- You have seen this sort of expression before, usually with a limit symbol in front of it: -->
      
      <!-- <me> -->
      <!--   \limit{h}{0}{\frac{(x+h)^2-x^2}{h}} = -->
      <!--   \limit{h}{0}{\frac{2xh+h^2}{h}} = \limit{h}{0}{2x+h}=2x  -->
      <!--   </me>.   -->

      Notice that to evaluate the limits <m>\limit{n}{\infty}
      {a_n}</m> and <m>\limit{h}{0}{\frac{(x+h)^2-x^2}{h}}</m>
      requires that we think about the limit parameters <m>n</m> and
      <m>h</m> slightly differently. In the former the limit parameter
      <m>n</m> takes on only integer values, whereas in the latter it
      could be any real number (though we're only interested in values
      close to zero).
      <!-- get close to infinity (which -->
      <!-- is not a number) here we need to let <m>h</m> get infinitely -->
      <!-- close to zero (which is a number).  In evaluating the limit of -->
      <!-- the sequence <m>(a_n)</m> the limit parameter <m>n</m>, only -->
      <!-- took on integer values whereas in evaluating the limit <m> -->
      <!-- \limitt{h}{0}{\frac{(x+h)^2-x^2}{h}}</m> the limit parameter -->
      <!-- <m>h</m> is simply close to zero. Hence it must be a real -->
      <!-- number.  -->
      <!--   While they are clearly related, the two limits, -->
      <!--   <md> -->
      <!--     <mrow>\limit{n}{\infty}{a_n}\amp{}\amp{} \text{and}\amp{}\amp{}\limit{h}{0}{\frac{(x+h)^2-x^2}{h}}</mrow> -->
      <!--     </md>       -->
      <!--     are different kinds of entities. -->
    </p>


    <p>
      <idx><h>Leibniz, Gottfried Wilhelm</h></idx>

      In much the same way that <m>h</m> gets close to zero in <m>
      \limitt{h}{0}{\frac{(x+h)^2-x^2}{h}}</m> Leibniz<rsq/>
      differentials (e.g. <m>\dx{x}</m> and <m>\dx{y}</m>) can be seen
      as an attempt to get close to <m>x</m> and <m>y</m>
      respectively, The idea at the heart of Calculus is the notion of
      <q>infinite closeness.</q> Unfortunately it is very difficult to
      define <q>infinite closeness</q> might mean so to get around
      that difficulty we instead we ask what happens to the expression
      <m>\frac{(x+h)^2-x^2}{h}</m> as <m>h</m> gets
      <term>arbitrarily</term> close to zero. That is the meaning of
      the notation: <m>\limitt{h}{0}{\frac{2xh+h^2}{h}}</m>.
    </p> 
    <p> 
      <idx><h>Lagrange, Joseph-Louis</h></idx>
      <idx><h>Cauchy, Augustin</h></idx> 

      As we saw in <xref ref="PowerSeriesQuestions"></xref>, <url
      href="https://mathshistory.st-andrews.ac.uk/Biographies/Lagrange/"
      visual="mathshistory.st-andrews.ac.uk/Biographies/Lagrange/">Lagrange</url>
      tried to avoid the entire issue of infinite closeness, both in
      the limit and differential forms when, in <m>1797</m>, he
      attempted to make infinite series the foundational concept in
      Calculus.  Although Lagrange<rsq/>s efforts failed, they set the
      stage for <url
      href="https://mathshistory.st-andrews.ac.uk/Biographies/Cauchy/"
      visual="mathshistory.st-andrews.ac.uk/Biographies/Cauchy/">Cauchy</url>
      to provide a definition of derivative which in turn relied on
      his precise formulation of a limit.  Consider the following
      example.
    </p>
    <example xml:id="EXAMPLESyncFunction">
      <p>
        Suppose we wish to determine the slope of the tangent line
        (derivative) of <m>f(x) = \sin x</m> at <m>x=0</m>.  We form
        the usual difference quotient: <m>D(x)=\frac{\sin x - \sin
        0}{x-0}=\frac{\sin x
        }{x}</m>. 
        <!-- Now consider the graph of this difference -->
        <!-- quotient <me>D(x) =\frac{\sin x - \sin 0}{x-0}</me>. -->
      </p>

      <figure xml:id="FIGURESyncFunc" > <caption>Graph of
      <m>\displaystyle D(x)=\frac{\sin x }{x}</m></caption>

      <image width="75%" source="images/SinGraph-1.png" > 
        <shortdescription></shortdescription>
      </image>
      </figure>
      <p> 
        From the graph, it might first appear that <m>D(0) =1</m> but
        we must be careful.  <m>D(0)</m> doesn<rsq/>t even exist!
        Somehow we must convey the idea that <m>D(x)</m> will approach
        <m>1</m> as <m>x</m> approaches <m>0</m>, even though the
        function <m>D(x)=\frac{\sin x }{x}</m> is not defined at
        <m>0</m>.  Cauchy<rsq/>s idea was that even if <m>D(0)</m> is
        meaningless it must be that <m>\limit{x}{0}{D(x)}=1</m>
        because we can make <m>D(x)</m> differ from <m>1</m> by as
        little as we wish by taking <m>x</m> sufficiently close to
        zero <nbsp/>(<xref ref="jahnke03__histor_analy"/>, p. 158).
      </p> 
    </example>
    <p> 
      <idx><h>Weierstrass, Karl</h></idx> 

      
      <url
          href="https://mathshistory.st-andrews.ac.uk/Biographies/Weierstrass/"
          visual="mathshistory.st-andrews.ac.uk/Biographies/Weierstrass/">Karl
      Weierstrass</url> made these ideas precise  and provided us
      with our modern formulation of the limit in his lectures on
      analysis at the University of Berlin (1859-60).
    </p>

    <definition xml:id="def_limit">
      <title>Limit</title> 

      <idx><h>limit</h></idx> 
      <idx><h>Definition</h><h>limit</h></idx> 

      <statement> 
        <p> 
          We say <m>\limit{x}{a}{f(x)} =L</m> provided that for each
          <m>\eps\gt0</m>, there exists a <m>\delta\gt0</m> such that if
          <m>0\lt \abs{x-a}\lt \delta</m> then <m>\abs{f(x)-L}\lt
          \eps</m>.
        </p>
      </statement> 
    </definition> 
    <p> 
      Before we delve into this, notice that <xref ref="def_limit" ></xref>  is very similar to the
      definition of <xref ref="def_continuity"
      text="custom">continuity at a point</xref>. 
      <!-- the definition of the continuity of -->
      <!-- <m>f(x)</m> at <m>x=a</m>.  --> 
      This is because the two concepts are closely very related.  
    </p>

    <p>
      In fact we can readily see that <m>f</m> is continuous at
      <m>x=a</m> if and only if the limit of <m>f(x)</m> as <m>x</m>
      approaches <m>a</m> is <m>f(a)</m>.
    </p> 
    <problem xml:id="DRILLContIFFLimExist">

      <p>
        Use <xref ref="def_continuity" ></xref> and <xref
        ref="def_limit" ></xref> to prove that a function <m>f(x)</m>
        is continuous at <m>x=a</m> if and only if
        <m>\limit{x}{a}{f(x)}=f(a)</m>.

      </p>
    </problem>

    <!-- <aside> -->
    
    <!--   <title>Accumulation Points</title> -->

    <!--   <p> -->
    <!--     This presumes that <m>a</m> is an -->
    <!--     <term>accumulation point</term> of the domain of <m>f</m> (<xref -->
    <!--     ref="def_accumulation-point"></xref>).  We will -->
    <!--     discuss accumulation points in <xref -->
    <!--     ref="BackToFourier"></xref>.   -->
    <!--   </p>  -->
    <!-- </aside> --> 
    <p> 
      There are really only two differences between <xref
      ref="def_limit" ></xref> and <xref ref="def_continuity" ></xref>
      and the differences are related.  The first is that we in the
      definition of a limit <m>L</m> plays the same role that
      <m>f(a)</m> played in the definition of continuity.  This is
      because the function may not be defined at <m>a</m>.  In a sense
      the limiting value <m>L</m> is the value <m>f</m> would have
      if <m>f</m>it were defined and continuous at <m>a</m>.
    </p>
    <p>
      The second difference is that we have replaced <me> \abs{x-a}\lt
      \delta </me> from the continuity definition with <me> 0\lt
      \abs{x-a}\lt \delta </me> in the limit definition. You can see
      why this change is needed from the limit in <xref
      ref="EXAMPLESyncFunction" ></xref>. Since <m>\frac{\sin
      x}{x}</m> is not defined at <m>x=0</m> we need to eliminate that
      possibility from consideration.  This is the only purpose for
      this change.
    </p> 
    <!-- <p> -->
    <!--   As with the definition of convergence of a sequence, the limit -->
    <!--   definition does not determine what the limit will be. It -->
    <!--   doesn<rsq/>t even help you guess what the limit might be.  It -->
    <!--   only verifies that your guess for the value of the limit is -->
    <!--   correct, once you have made a guess by other means. -->
    <!-- </p>  -->
    <!-- <p>  -->
    <!--   Finally, a few comments on the differences and similiarities -->
    <!--   between this limit and the limit of a sequence are in order, -->
    <!--   if for no other reason than because we use the same notation -->
    <!--   (<m>\lim</m>) for both. -->
    <!-- </p>  -->
    <!-- <p>  -->
    <!--   When we were working with sequences in <xref -->
    <!--   ref="Convergence"></xref> and wrote things like -->
    <!--   <m>\limit{n}{\infty}{a_n}</m> we were thinking of <m>n</m> as an -->
    <!--   integer that got bigger and bigger.  To put that more -->
    <!--   mathematically, the limit parameter <m>n</m> was taken from the -->
    <!--   set of positive integers, or <m>n\in \NN</m>. -->
    <!-- </p>  -->
    <!-- <p>  -->
    <!--   For both continuity and the limit of a function we -->
    <!--   write things like <m>\limit{x}{a}{f(x)}</m> and think of -->
    <!--   <m>x</m> as a continuous real variable that gets arbitrarily close to the number -->
    <!--   <m>a</m>.  -->


    <!--   <!-\- Again, to be more mathematical in our language we -\-> -->
    <!--   <!-\- would say that the limit parameter <m>x</m> is taken from the -\-> -->
    <!--   <!-\- <m>\ldots</m> Well, actually, this is interesting isn<rsq/>t it?  Do -\-> -->
    <!--   <!-\- we need to take <m>x</m> from <m>\QQ</m> or from <m>\RR?</m> The -\-> -->
    <!--   <!-\- requirement in both cases is simply that we be able to choose -\-> -->
    <!--   <!-\- <m>x</m> arbitrarily close to <m>a</m>.  From <xref -\-> -->
    <!--   <!-\- ref="thm_IrrationalBetweenIrrationals"></xref> of <xref -\-> -->
    <!--   <!-\- ref="NumbersRealRational"></xref> we see that this is -\-> -->
    <!--   <!-\- possible whether <m>x</m> is rational or not, so it seems either -\-> -->
    <!--   <!-\- will work.  This leads to the pardoxical sounding conclusion -\-> -->
    <!--   <!-\- that we do not need a continuum <m>(\RR)</m> to have continuity. -\-> -->
    <!--   <!-\- This seems strange.   -\-> -->
    <!-- </p>  -->

    <!-- <p> -->
    <!--   Before continuing with <xref ref="EXAMPLESyncFunction" -->
    <!--   ></xref>, let<rsq/>s look at some simpler, algebraic examples -->
    <!--   to see the <xref ref="def_limit" ></xref> in use. -->
    <!-- </p> -->
    <example>
      <statement>
        <p> Consider the
        function <m>D(x)=\frac{x^2-1}{x-1}</m>, <m>x\neq 1</m>.  You
        probably recognize this as the difference quotient used to
        compute the derivative of <m>f(x)=x^2</m> at <m>x=1</m>, so we
        strongly suspect that <m>\limit{x}{1}{\frac{x^2-1}{x-1}}=2</m>.
        Just as when we were dealing with limits of sequences, we should
        be able to use the definition to verify this.  And as before, we
        will start with some scrapwork.  
        </p> 
        <p> 
          <term>SCRAPWORK</term>
        </p>
        <p>
          Let <m>\eps>0</m>.  We wish to find a <m>\delta>0</m>
          such that if <m>0\lt \abs{x-1}\lt \delta</m> then
          <m>\abs{\frac{x^2-1}{x-1}-2}\lt \eps</m>.  With this in mind, we
          perform the following calculations <me>
          \abs{\frac{x^2-1}{x-1}-2}=\abs{(x+1)-2} = \abs{x-1} </me>.  
        </p>
        <p>
          Now we have a handle on <m>\delta</m> that will work in the
          definition and we<rsq/>ll give the formal proof that <me>
          \limit{x}{1}{\frac{x^2-1}{x-1}}=2 </me>.  
        </p> 
      </statement>
    </example>
    <proof> 
      <p>
        Let <m>\eps>0</m> and let <m>\delta=\eps</m>.  If <m>0\lt
        \abs{x-1}\lt \delta</m>, then <me>
        \abs{\frac{x^2-1}{x-1}-2}=\abs{(x+1)-2}=\abs{x-1}\lt
        \delta=\eps </me>.
      </p>
    </proof> 
    <p> 
      As in our previous work with sequences and continuity, notice
      that the scrapwork is not part of the formal proof (though it
      was necessary to determine an appropriate <m>\delta)</m>.  Also,
      notice that <m>0\lt \abs{x-1}</m> was not really used except to
      ensure that <m>x\neq 1</m>.
    </p> 
    <problem>
      
      <idx><h>limit</h><h><m>\limit{x}{a}{\frac{x^2-a^2}{x-a}}=2a</m></h></idx>

      <statement> 
        <p>
          Use the definition of a limit to verify that
          <me> \limit{x}{a}{\frac{x^2-a^2}{x-a}}=2a.{} </me> 
        </p>
      </statement>
    </problem>

    <problem>
      
      <idx><h>limit</h><h>verifying limits via continuity</h></idx>

      <introduction>
        <p> 
          Use the
          definition of a limit to verify each of the following limits.
        </p>
      </introduction>
      <task> 
        <statement>
          <p>
            <m>\limit{x}{1}{\frac{x^3-1}{x-1}}=3</m> 
          </p> 
        </statement>
        <hint>
          <p> 
            <md>
              <mrow> 
                \abs{\frac{x^3-1}{x-1}-3} \amp =
                \abs{x^2+x+1-3} 
              </mrow> 
              <mrow>\amp \leq\abs{x^2-1}+\abs{x-1}
              
              </mrow> 
              <mrow> \amp =\abs{(x-1+1)^2-1}+\abs{x-1} 
              </mrow> 
              <mrow>
                \amp =\abs{(x-1)^2+2(x-1)}+\abs{x-1} 
              </mrow> 
              <mrow>\amp
              \leq\abs{x-1}^2 + 3\abs{x-1} 
              </mrow> 
              </md>.  
          </p> 
        </hint>
      </task> 
      <task>
        <statement> 
          <p>
            <m>\limit{x}{1}{\frac{\sqrt{x}-1}{x-1}}=1/2</m> 
          </p>
        </statement>
        <hint>
          <p>
            <md>
              <mrow>
                \abs{\frac{\sqrt{x}-1}{x-1}-\frac12}\amp =
                \abs{\frac{1}{\sqrt{x}+1}-\frac12} 
              </mrow> 
              <mrow> 
                \amp
                =\abs{\frac{2-\left(\sqrt{x}+1\right)}{2\left(\sqrt{x}+1\right)}}
              </mrow> 
              <mrow> 
                \amp
                =\abs{\frac{1-x}{2\left(1+\sqrt{x}\right)^2}} 
              </mrow> 
              <mrow>
                \amp \leq\frac12\abs{x-1}.  
              </mrow> 
            </md> 
          </p> 
        </hint> 
      </task>
    </problem> 

    <p> 
      Our definition of a limit, although it is rigorous, is quite
      cumbersome to use. What we want to do is develop some tools we
      can use without having to refer directly to <xref
      ref="def_limit"></xref>.  One such tool is <xref
      ref="thm_LimDefOfContinuity"></xref> which allows us to show
      that a function is continuous (or discontinuous) at a point by
      examining certain sequences. We will need others.
    </p>
    <p>
      As we observed earlier, <m>f(x)</m> is continuous at
      <m>x=a</m> if and only if <m>\limit{x}{a}{f(x)} = f(a)</m>. On
      the other hand if <m>f(x)</m> is not continuous at <m>x=a</m>,
      but <m>\limit{x}{a}{f(x)}=L </m>, we can make it continuous by
      arbitrarily assigning <m>f(a)=L</m>. 


      <!-- <m>\limit{x}{a}{f(x)}=L -->
      <!-- </m> then we can make it continuous by defining -->
      <!-- <m>f(a)=L</m>. -->
      Combining this with <xref
      ref="thm_LimDefOfContinuity"></xref> we have the following
      corollary:
      <!--   Here is another.  The key is the -->
      <!--   observation we made after the definition of a limit: <me> f -->
      <!--   \text{ is continuous at } x=a \text{ if and only if } -->
      <!--   \limit{x}{a}{f(x)}=f(a) </me>. -->
      <!-- </p>  -->
      <!-- <p> -->
      <!--   Read another way, we could say that <m>\limit{x}{a}{f(x)}=L</m> -->
      <!--   provided that if we redefine <m>f(a)=L</m> (or define -->
      <!--   <m>f(a)=L</m> in the case where <m>f(a)</m> is not defined) then -->
      <!--   <m>f</m> becomes continuous at <m>a</m>.  This allows us to use -->
      <!--   all of the machinery we proved about continuous functions and -->
      <!--   limits of sequences.   -->
      <!-- </p>  -->
      <!-- <p>  -->
      <!--   For example, the following -->
      <!--   corollary to <xref ref="thm_LimDefOfContinuity"></xref> -->
      <!--   comes virtually for free once we<rsq/>ve made the observation above. -->
    </p> 
    <corollary xml:id="cor_limit-by-sequences"> 
      <statement> 
        <p>
          <m>\limit{x}{a}{f(x)}=L</m> if and only if <m>f</m> satisfies
          the following property: <me> \forall \text{ sequences } (x_n),
          x_n\ne a, \text{ if } \limit{n}{\infty}{x_n}=a \text{ then }
          \limit{n}{\infty}{f(x_n)}=L. {} </me> 
        </p> 
      </statement>
    </corollary> 
    <p> 
      Armed with this, we can prove the following
      familiar limit theorems from Calculus.  
    </p> 

    <theorem xml:id="thm_CalcLimits">

      <idx><h>limit</h><h>properties of</h></idx>

      <statement>
        <p>
          Suppose <m>\limit{x}{a}{f(x)}=L</m> and <m>\limit{x}{a}{g(x)}=M</m>, then

          <ol marker="(a)">
            <li>
              <p>
                <m>\limit{x}{a}{\left(f(x)+g(x)\right)}=L+M</m>
              </p>
            </li>

            <li>
              <p>
                <m>\limit{x}{a}{\left(f(x)\cdot g(x)\right)}=L\cdot M</m>
              </p>
            </li>

            <li>
              <p>
                <m>\limit{x}{a}{\left(\frac{f(x)}{g(x)}\right)}=L/M</m> provided <m>M\ne0</m> and <m>g(x)\ne{}0</m>,
                for <m>x</m> sufficiently close to a
                (but not equal to <m>a</m>).
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </theorem>

    <!-- <theorem xml:id="thm_CalcLimits">  -->
    <!--   <introduction> -->
    <!--     <idx><h>limit</h><h>properties of</h></idx>  -->
    <!--     <p> -->
    <!--       Suppose -->
    <!--       <m>\limit{x}{a}{f(x)}=L</m> and <m>\limit{x}{a}{g(x)}=M</m>, -->
    <!--       then  -->
    <!--     </p> -->
    <!--   </introduction> -->
    <!--   <task> -->
    <!--     <statement> -->
    <!--       <p> -->
    <!--         <m>\limit{x}{a}{\left(f(x)+g(x)\right)}=L+M</m>  -->
    <!--       </p> -->
    <!--     </statement> -->
    <!--   </task> -->
    <!--   <task> -->
    <!--     <statement> -->
    <!--       <p> -->
    <!--         <m>\limit{x}{a}{\left(f(x)\cdot g(x)\right)}=L\cdot M</m> -->
    <!--       </p> -->
    <!--     </statement> -->
    <!--   </task> -->
    <!--   <task> -->
    <!--     <statement> -->
    <!--       <p> -->
    <!--         <m>\limit{x}{a}{\left(\frac{f(x)}{g(x)}\right)}=L/M</m> provided -->
    <!--         <m>M\ne0</m> and <m>g(x)\ne{}0</m>, for <m>x</m> sufficiently -->
    <!--         close to a (but not equal to <m>a</m>).   -->
    <!--       </p> -->
    <!--     </statement> -->
    <!--   </task> -->
    <!-- </theorem> -->


    <!-- <statement>  -->
    <!--   <p> -->
    <!--     <ol marker="(a)">  -->
    <!--       <li>  -->
    <!--         <p> -->

    <!--         </p>  -->
    <!--       </li>  -->
    <!--       <li> -->
    <!--         <p> -->

    <!--         </p> -->
    <!--       </li>  -->
    <!--       <li> -->
    <!--         <p> -->
    <!--         </p>  -->
    <!--       </li>  -->
    <!--     </ol>  -->
    <!--   </p> -->
    <!-- </statement>  -->
    <!-- </theorem>  -->
    <p> 
      We will prove part (a) to give you a
      feel for this and let you prove parts (b) and (c).  
    </p> 
    <proof>
      <p> 
        Let <m>\left(x_n\right)</m> be a sequence such that
        <m>x_n\ne a</m> and <m>\limit{n}{\infty}{x_n}=a</m>.  Since
        <m>\limit{x}{a}{f(x)} = L</m> and <m>\limit{x}{a}{g(x)} = M</m>
        we see that <m>\limit{n}{\infty}{f(x_n)} = L</m> and
        <m>\limit{n}{\infty}{g(x_n)} = M</m>.  By <xref
        ref="thm_SumOfSequences"></xref> of <xref
        ref="Convergence"></xref>, we have
        <m>\limit{n}{\infty}{f(x_n)+g(x_n)}=L+M</m>.  Since
        <m>\left(x_n\right)</m> was an arbitrary sequence with
        <m>x_n\ne a</m> and <m>\limit{n}{\infty}{x_n} = a</m> we have
        <me> \limit{x}{a}{\left(f(x)+g(x)\right)} = L+M </me>.  
      </p> 
    </proof>

    <problem> 

      <idx><h>limit</h><h>properties of</h></idx>
      <idx><h>limit</h><h>verify limit laws from Calculus</h></idx>

      <statement> 
        <p> 
          Prove parts (b) and (c) of <xref
          ref="thm_CalcLimits"></xref>.  
        </p> 
      </statement>
    </problem> 
    <p> 
      More in line with our current needs, we have a
      reformulation of the Squeeze Theorem.  
    </p> 
    <theorem xml:id="thm_SqueezeTheoremFunctions"> 
      <title>Squeeze Theorem for Functions</title>

      <idx><h>Squeeze Theorem</h><h>for functions</h></idx>

      <statement>
        


        <p>
          Suppose
          <m>f(x)\le g(x) \le h(x)</m>, for <m>x</m> sufficiently close to
          <m>a</m> (but not equal to <m>a</m>).  If
          <m>\limit{x}{a}{f(x)}=L=\limit{x}{a}{h(x)}</m>, then
          <m>\limit{x}{a}{g(x)}=L</m> also.  
        </p> 
      </statement> 
    </theorem>

    <problem> 

      <idx><h>Squeeze Theorem</h><h>for functions</h></idx>

      <statement> 
        <p>
          Prove the <xref
          ref="thm_SqueezeTheoremFunctions" text="custom">Squeeze
          Theorem for functions</xref>.  
        </p>
      </statement>
      <hint> 
        <p> Use <xref
        ref="thm_SqueezeTheorem"></xref>, the Squeeze Theorem for
        sequences from <xref ref="Convergence"></xref>.  
        </p>
      </hint> 
    </problem> 
    <figure  xml:id="FIGUREUnitCircleSinOverX">
      <caption></caption>
      <image width="60%" source="images/UnitCircle.png" > 
        <shortdescription></shortdescription>
      </image>
    </figure>

    <problem>
      <idx><h>limit</h><h><m>\limit{x}{0}{\textstyle\frac{\sin
      x}{x}}=1</m></h></idx>
      <introduction>
        <p> 
          Returning to <xref ref="EXAMPLESyncFunction"></xref> we see
          that the Squeeze Theorem is just what we need.  First notice
          that since <m>D(x)=\frac{\sin x}{x}</m> is an even function, we only
          need to focus on <m>x\gt0</m> in our inequalities.  Consider the
          unit circle.
        </p>
      </introduction>
      <task>
        <statement>
          <p>
            Use <xref ref="FIGUREUnitCircleSinOverX" ></xref> to show that
            <me>

              \text{ area } (\Delta OAC)\lt \text{ area } (\text{ sector }
            OAC)\lt \text{ area } (\Delta OAB) </me>
          </p>
        </statement>
      </task>


      <task>
        <statement>

          <p>
            Use the result in part (a) to show that if <m>0\lt x\lt
            \frac{\pi}{2}</m>, then 
            <men xml:id="EQUATIONCosLSinOverx">
              \cos x\lt \frac{\sin x}{x}\lt 1 
              </men>.
          </p>
        </statement>
      </task>
      <task>
        <statement>

          <p>

            Use the fact that <m>\cos x</m> and <m>\frac{\sin
            x}{x}</m> are both even functions to show that  <xref
            ref="EQUATIONCosLSinOverx" >equation</xref> is also true
            for <m>-\frac{\pi}{2}\lt x\lt 0</m>

          </p>
        </statement>
      </task>
      <task>
        <statement>

          <p>
            Use <xref ref="EQUATIONCosLSinOverx" >equation</xref> and
            the <xref ref="thm_SqueezeTheoremFunctions"
            text="custom">Squeeze Theorem</xref> for Functions to show
            <m>\limitt{x}{0}{\textstyle\frac{\sin x}{x}}=1</m>.

          </p>
        </statement>
      </task> 
    </problem> 

    <problem xml:id="PROBLEMBasicLimits">
      <introduction>
        <p>
          Suppose <m>\limitt{x}{a}{ f(x)}=L</m>.  
        </p>
      </introduction>
      <task>
        <statement>
          <p>
            Prove that if <m>L\gt0</m>,
            then there exists a <m>\delta >0</m>, such that if
            <m>0\lt\left|x-a\right|\lt\delta </m>, then
            <m>f\left(x\right)>0</m>.  
          </p>
        </statement>
        <hint>
          <p> 
            Try
            <m>\eps =\frac{L}{2}</m>.  
          </p> 
        </hint> 
      </task> 
      <task>
        <statement> 
          <p> 
            Prove that if <m>L\lt0</m>, then there exists a
            <m>\delta >0</m>, such that if <m>0\lt\left|x-a\right|\lt\delta
            </m>, then <m>f\left(x\right)\lt0</m>.  
          </p>
        </statement>
        <hint>
          <p>
            Consider <m>-f(x)</m>.  
          </p> 
        </hint>
      </task> 
      <task>
        <statement>
          <p>
            Notice that if <m>\limit{x}{a}{f(x)}=L</m>, then the contrapositive of part (a) says that
            if for each <m>\delta >0</m>, there is an <m>x</m> with
            <m>0\lt\left|x-a\right|\lt\delta </m> and <m>f\left(x\right)\le
            0</m>, then <m>L\le 0</m>.  
          </p> 
          <p> 
            What does the contrapositive of part (b) say?
          </p> 
        </statement> 
      </task>
    </problem> 

<definition xml:id="DEFINITIONNear">
  <title>Near</title> 
  <statement> 
    <p>
      <idx><h>Definition</h><h>near</h></idx>     
      <idx><h>near</h></idx>     

      We say that a function
      <m>f(x)</m> has a property for <m>x</m> <term>near</term> <m>a</m>, if there
      exists a <m>{\delta }_0>0</m> such that <m>f(x)</m> has that
      property for all <m>x</m> with <m>0\lt\left|x-a\right|\lt{\delta
      }_0</m>

    </p>

  </statement>

</definition>

<problem xml:id="PROBLEMLimUppLow">
  <introduction>

    <p>

      Prove that each of the following statements is also a
      consequence of <xref ref="PROBLEMBasicLimits"></xref>.
      Suppose <m>\limit{x}{a}{f(x)}=L</m>.

    </p>
  </introduction>
  <task>
    <statement>

      <p>
        If <m>f\left(x\right)\le 0</m> for <m>x</m> near <m>a</m>, then <m>L\le 0</m>.
      </p>

    </statement>
  </task>
  <task>
    <statement>
      <p>
        If <m>f\left(x\right)\ge 0</m> for <m>x</m> near <m>a</m>, then <m>L\ge 0</m>.
      </p>
    </statement>
  </task>
</problem>

  </section>




<section  xml:id="Continuity-DerivativeAfterthought">
  <title>The Definition of  the Derivative and the Mean Value Theorem</title>
  <p>
    As we mentioned in <xref
    ref="CalcIn17th18thCentury-NewtLeibStart"></xref> Leibniz invented
    his <foreign>calculus differentialis</foreign> (differential
    calculus <mdash/> literally <q>rules for (infinitely small)
    differences</q>) in the <m>1600</m>s.
  </p>
  <p>
    In the late <m>1700</m>s Lagrange tried to provide a rigorous
    foundation for Calculus by discarding differential ratios like
    the expression <m>\dfdx{y}{x} </m> in favor of his own <q>prime
    notation</q> (<m>f^\prime(x) </m>). Thus it was Lagrange who
    established functions and limits, rather than the curves and
    infinitesimals favored by Leibniz and Newton, as fundamental.
  </p>
  

  <p>
    When you took Calculus you spent at least an entire semester
    learning about the properties of the derivative and how to use
    them to explore the properties of functions so there is no need to
    repeat that effort here. Instead we will establish the underlying,
    rigorous, formal foundation for the derivative concept in terms of
    limits.
    
  </p>

  <definition xml:id="def_derivative">
    <title>The Derivative</title>

    <idx><h>differentiation</h><h>definition of the derivative</h></idx>
    <idx><h>Definition</h><h>derivative</h></idx>

    <statement>
      <p>
        Given a function <m>f(x)</m> defined on an interval
        <m>(a,b)</m> we define <me> f^\prime(x) =
        \limit{h}{0}{\frac{f(x+h)-f(x)}{h}}.{} </me> 
      </p>
    </statement>
  </definition>

  <p> 
    There are a few fairly obvious facts about this definition which
    are nevertheless worth noticing explicitly: 
  </p> 
  <p> 
    <ol> 
      <li>
        If the limit <m>f^\prime (x)</m> exists at <m>x</m>, then we say that
        <m>f</m> is <term>differentiable</term> at <m>x</m>.
      </li>
      <li>
        <p> 
          The derivative is defined <em>at a point.</em> If the derivative
          of <m>f(x)</m>  is
          defined at every point in an interval <m>(a,b)</m> then
          we say that <m>f</m> is <term>differentiable on the interval</term> <m>(a,b)</m>.
        </p>
      </li> 
      <li> 
        <p> 
          Since it is defined at a point it is at least
          theoretically possible for a function to be
          differentiable at a single point in its entire domain.
        </p>
      </li>
      <li> 
        <p> 
          Since it is defined as a limit and not all limits
          exist, functions are not necessarily differentiable.
        </p>
      </li> 
      <li> 
        <p>
          Since it is defined as a limit,
          <xref ref="cor_limit-by-sequences"></xref>
          applies.  That is, <m>f^\prime(x)</m> exists if and
          only if <m>\forall \text{ sequences } (h_n),\, h_n\ne
          0</m>, if <m>\limit{n}{\infty}{h_n}=0</m> then <me>
          f^\prime{(x)} =
          \limit{n}{\infty}{\frac{f(x+h_n)-f(x)}{h_n}} </me>.
          Since <m>\limit{n}{\infty}{h_n}=0</m> this could also
          be written as <me>f^\prime{(x)} =
          \limit{h_n}{0}{\frac{f(x+h_n)-f(x)}{h_n}}</me>.  
        </p>
      </li> 
    </ol>
  </p>
  <p>
    If we make the substitution <m>y=x+h</m> in <xref
    ref="def_derivative"></xref>  we obtain the following equivalent
    definition, which is sometimes easier to use.
  </p>

  <definition xml:id="DEFINITIONdef_derivative">
    <title>The Derivative, An Alternative Definition</title>

    <idx><h>differentiation</h><h>definition of the derivative</h></idx>
    <idx><h>Definition</h><h>derivative</h></idx>

    <statement>
      <p>
        Given a function <m>f(x)</m> defined on an interval <m>(a,b)</m>,
        and a point <m>x\in (a,b)</m>,
        the derivative of <m>f</m> is given by 
        <me>f^\prime(x)=\limit{y}{x}{\frac{f(y)-f(x)}{y-x}}</me>.
      </p>

    </statement>
  </definition>

  <theorem xml:id="thm_DiffImpCont">
  
    <idx><h>continuity</h><h>implied by differentiability</h></idx>
    <idx><h>differentiation</h><h>differentiability implies
    continuity</h></idx>

    <statement> 
      <p> 
        <alert>Differentiability Implies Continuity</alert>
      </p> 
      <p> 
        If <m>f</m> is differentiable at a point <m>c</m>
        then <m>f</m> is continuous at <m>c</m> as well.
      </p>
    </statement> 
  </theorem>

  <problem> 
    
    <idx><h>differentiation</h><h>differentiability implies
    continuity</h></idx>

    <statement> 
      <p> 
        Prove <xref ref="thm_DiffImpCont"></xref> 
      </p>
    </statement> 
  </problem>

  <p> 
    As we mentioned, the derivative is an extraordinarily useful
    mathematical tool but it is not our intention to learn to
    <em>use</em> it here.  Our purpose here is to define it
    rigorously (done) and to show that our formal definition does in
    fact recover the useful properties you came to know and love in
    your Calculus course.
  </p>

  <p> 
    The first such property is known as Fermat<rsq/>s Theorem.  
  </p>

  <!-- <theorem xml:id="thm_FermatsTheorem">  -->
  <!--   <title>Fermat<rsq/>s   Theorem</title>  -->
  <!--   <idx><h>Fermat<rsq/>s Theorem</h></idx>  -->

  <!--   <statement>  -->
  <!--     <p> -->
  <!--       Suppose <m>f</m> is differentiable in some interval <m>(a,b)</m> -->
  <!--       containing <m>c</m>.  If <m>f(c)\ge f(x)</m> for every <m>x</m> in -->
  <!--       <m>(a,b)</m>, then <m>f^\prime(c)=0</m>. -->
  <!--     </p>  -->
  <!--   </statement>  -->
  <!-- </theorem> -->

  <!-- <proof>  -->
  <!--   <p>  -->
  <!--     Since <m>f^\prime(c)</m> exists we know that if -->
  <!--     <m>\left(h_n\right)_{n=1}^\infty</m> converges to zero then -->
  <!--     the sequence <m>a_n = -->
  <!--     \frac{f\left(c+h_n\right)-f(c)}{h_n}</m> converges to -->
  <!--     <m>f^\prime(c)</m>.  The proof consists of showing that -->
  <!--     <m>f^\prime(c)\leq 0</m> <em>and</em> that -->
  <!--     <m>f^\prime(c)\geq 0</m> from which we conclude that -->
  <!--     <m>f^\prime(c)= 0</m>.  We will only show the first part. -->
  <!--     The second is left as an exercise. -->
  <!--   </p> -->

  <!--   <p>  -->
  <!--     <em>Claim:</em> <m>f^\prime(c)\leq 0</m>.   -->
  <!--   </p> -->

  <!--   <p>  -->
  <!--     Let <m>n_0</m> be sufficiently large that <m>\frac{1}{n_0}\lt -->
  <!--     b-c</m> and take -->
  <!--     <m>\left(h_n\right)=\left(\frac{1}{n}\right)_{n=n_0}^\infty</m>. -->
  <!--     Then <m>f\left(c+\frac1n\right)-f(c) \leq 0</m> and -->
  <!--     <m>\frac1n>0</m>, so that <me> -->
  <!--     \frac{f\left(c+h_n\right)-f(c)}{h_n}\leq 0,   \forall n=n_0, -->
  <!--     n_0+1, \ldots </me> -->
  <!--   </p> -->

  <!--   <p>  -->
  <!--     Therefore -->
  <!--     <m> -->
  <!--       f^\prime(c) = -->
  <!--       \limit{h_n}{0}{\frac{f\left(c+h_n\right)-f(c)}{h_n}} \leq  0 -->
  <!--       </m> also.   -->
  <!--   </p>  -->
  <!-- </proof> -->

  <!-- <problem>  -->
  <!--   <statement>  -->
  <!--     <p>  -->
  <!--       <idx><h>Fermat<rsq/>rsq/>s Theorem</h><h>if <m>f(a)</m> is a maximum -->
  <!--       then <m>f^\prime(a)=0</m></h></idx> -->

  <!--       Show that <m>f^\prime(c) \geq 0</m> and conclude that -->
  <!--       <m>f^\prime(c) =0</m>.   -->
  <!--     </p>  -->
  <!--   </statement>  -->
  <!-- </problem> -->

  <!-- <problem>  -->
  <!--   <statement>  -->
  <!--     <p>  -->
  <!--       <idx><h>Fermat<rsq/>s Theorem</h><h>if <m>f(a)</m> is a minimum -->
  <!--       then <m>f^\prime(a)=0</m></h></idx> -->

  <!--       Show that if <m>f(c) \leq f(x)</m> for all <m>x</m> in -->
  <!--       some interval <m>(a,b)</m> then <m>f^\prime(c) =0</m> too. -->
  <!--     </p>  -->
  <!--   </statement> -->
  <!-- </problem> -->

  <theorem xml:id="thm_FermatsTheorem">
    <title>Fermat<rsq/>s   Theorem</title>
  
    <idx><h>Fermat<rsq/>s Theorem</h></idx>

    <statement>
      <p>
        Suppose <m>f</m> is differentiable on <m>(a,b)</m> and <m>f</m> has an extremum at
        <m>c\in (a,b)</m>.  Then <m>f^\prime\left(c\right)=0</m>.
      </p>
    </statement>
  </theorem>


  <proof>
    <title>Sketch of Proof</title>

    <p>
      There are two cases: 
      <dl>
        <li>
          <title>Case 1:</title>
          <p>
            <m>f(c)</m> is a maximum, and
          </p>
        </li>
        <li>
          <title>Case 2:</title>
          <p>
            <m>f(c)</m> is a minimum.
          </p>
        </li>
      </dl> 
    </p>

    <p>
      Suppose <m>f(c)</m> is a maximum so that <m>f\left(c\right)\ge
      f(x)</m> for all <m>x\in (a,b)</m>.  Since <m>f</m> is
      differentiable at <m>c</m>, we have
      <me>f^\prime\left(c\right)=\limit{x}{c}{
      \frac{f(x)-f(c)}{x-c}}</me>
    </p>

    <p>
      To show <m>f^\prime\left(c\right)=0</m>, we need to show that

      <md>
        <mrow>f^\prime\left(c\right)\le 0 \amp{}\amp{}
        \text{and}\amp{}\amp{} f^\prime\left(c\right)\ge 0.</mrow>
      </md>

      These facts follow from <xref ref="PROBLEMBasicLimits"></xref>.
    </p>
    <p> 
      The case where <m>f(c)</m> is a minimum can be handled by
      looking at <m>-f</m>.
    </p>
  </proof>


  <problem xml:id="PROBLEMFermatsTheorem">

    <statement>
      <p>
        Provide a formal proof for <xref
        ref="thm_FermatsTheorem" text="custom">Fermat<rsq/>s Theorem</xref>.
      </p>

    </statement>
  </problem>

  <p> 
    Many of the most important properties of the derivative follow
    from what is called the <term>Mean Value Theorem</term> (MVT) stated
    below.
  </p>

  <theorem xml:id="thm_MVT"> 
    <title>The Mean Value Theorem (<init>MVT</init>)</title>
  
    <idx><h>Mean Value Theorem, the</h></idx>

    <statement> 
      <!-- <p>  -->
      <!--   <term>The Mean Value Theorem (<init>MVT</init>)</term> -->
      <!-- </p>  -->
      <p>
        Suppose <m>f^\prime</m>
        exists for every <m>x\in(a,b)</m> and <m>f</m> is
        continuous on <m>[a,b]</m>.  Then there is a real number
        <m>c\in(a,b)</m> such that <me>
        f^\prime(c)=\frac{f(b)-f(a)}{b-a}.{} </me> 
      </p>
    </statement> 
  </theorem>

  <p> 
    It would be difficult to prove the MVT right now,  so
    we will first state and prove Rolle<rsq/>s Theorem, which can be seen as a
    special case of the MVT. The proof of the MVT will then follow easily.
  </p>

  <p>
    <idx><h>Newton, Isaac</h></idx>
    <idx><h>Leibniz, Gottfried Wilhelm</h></idx>

    <url
    href="https://mathshistory.st-andrews.ac.uk/Biographies/Rolle/"
    visual="mathshistory.st-andrews.ac.uk/Biographies/Rolle/">Michel
    Rolle</url> (1652<ndash/>1719) first stated the following theorem
    in 1691.  Given this date and the nature of the theorem it would
    be reasonable to suppose that Rolle was one of the early
    developers of Calculus but this is not so.  In fact, Rolle was
    disdainful of both Newton and Leibniz<rsq/> versions of Calculus,
    once deriding them as a collection of <q>ingenious fallacies.</q>
    It is a bit ironic that his theorem is so fundamental to the
    modern development of the Calculus he ridiculed.
  </p>

  <theorem xml:id="thm_Rolle_s_Theorem">
    <title>Rolle<rsq/>s Theorem</title>
  
    <idx><h>Rolle<rsq/>s Theorem</h></idx> 
    
    <statement> 
      <p>
        Suppose <m>f^\prime</m> exists for every <m>x\in(a,b)</m>,
        <m>f</m> is continuous on <m>[a,b]</m>, and <me> f(a)=f(b) </me>.
      </p>
      
      <p> 
        Then there is a real number <m>c\in(a,b)</m> such that
        <me> f^\prime(c)=0 </me>.  
      </p> 
    </statement>
  </theorem>

  <!-- <proof>  -->
  <!--   <p>  -->
  <!--     Since -->
  <!--     <m>f</m> is continuous on <m>[a,b]</m> we see, by the Extreme Value -->
  <!--     Theorem,<idx><h>Extreme Value Theorem (EVT)</h><h>Rolle<rsq/>s Theorem, -->
  <!--     and</h></idx> that <m>f</m> has both a maximum and a minimum on -->
  <!--     <m>[a,b]</m>.  Denote the maximum by <m>M</m> and the minimum by -->
  <!--     <m>m</m>.  There are several cases: -->

  <!--     <dl>  -->
  <!--       <li>  -->
  <!--         <title>Case 1:</title> -->
  <!--         <p> -->
  <!--           <m>f(a)=f(b)=M=m</m>.  In -->
  <!--           this case <m>f(x)</m> is constant (why?).  Therefore -->
  <!--           <m>f^\prime(x)=0</m> for every <m>x\in(a,b)</m>.   -->
  <!--         </p>  -->
  <!--       </li> -->

  <!--       <li>  -->
  <!--         <title>Case 2:</title>  -->
  <!--         <p>  -->
  <!--           <m>f(a)=f(b)=M\neq m</m>. -->
  <!--           In this case there is a real number <m>c\in(a,b)</m> such that -->
  <!--           <m>f(c)</m> is a local minimum.  By Fermat<rsq/>s Theorem, -->
  <!--           <m>f^\prime(c)=0</m>.   -->
  <!--         </p>  -->
  <!--       </li> -->

  <!--       <li> -->
  <!--         <title>Case 3:</title> -->
  <!--         <p> -->
  <!--           <m>f(a)=f(b)=m\neq M</m>. -->
  <!--           In this case there is a real number <m>c\in(a,b)</m> such that -->
  <!--           <m>f(c)</m> is a local maximum.  By Fermat<rsq/>s Theorem, -->
  <!--           <m>f^\prime(c)=0</m>. -->
  <!--         </p> -->
  <!--       </li> -->

  <!--       <li> -->
  <!--         <title>Case 4:</title>  -->
  <!--         <p>  -->
  <!--           <m>f(a)=f(b)</m> is neither a maximum nor a minimum.  In this -->
  <!--           case there is a real number <m>c_1\in(a,b)</m> such that -->
  <!--           <m>f(c_1)</m> is a local maximum, and a real number -->
  <!--           <m>c_2\in(a,b)</m> such that <m>f(c_2)</m> is a local minimum. -->
  <!--           By Fermat<rsq/>s Theorem, <m>f^\prime(c_1)=f^\prime(c_2)=0</m>. -->
  <!--         </p>  -->
  <!--       </li>  -->
  <!--     </dl>  -->
  <!--   </p>  -->
  <!-- </proof> -->
  <proof>
    
    <title>Sketch of Proof</title>

    <p>
      By the <xref ref="thm_EVT" text="custom">EVT</xref>, we know that <m>f</m> has a
      maximum <m>M</m>, and a minimum <m>m</m>, on
      <m>[a,b]</m>.  Suppose that both occur at the endpoints.  This would say
      that <m>m=M</m> and <m>f</m> is constant on <m>[a,b]</m>.  What does this say about
      <m>f^\prime</m>?  
    </p>
    <p>
      On the other hand, what does <xref
      ref="thm_FermatsTheorem" text="custom">Fermat<rsq/>s Theorem</xref> say if one or
      both of these extrema is not at an endpoint?
    </p>
  </proof>

  <problem xml:id="PROBLEMRollesTheorem">
    <title>Rolle<rsq/>s Theorem</title>
    <p>
      Turn the ideas in the sketch above into a proof of <xref
      ref="thm_Rolle_s_Theorem" text="custom">Rolle<rsq/>s
      Theorem</xref>.
    </p>
  </problem>


  <p>
    We can now prove the MVT as a corollary of Rolle<rsq/>s Theorem.  We
    only need to find the right function to apply Rolle<rsq/>s Theorem
    to.  The following figure shows a function, <m>f(x)</m>, cut by a
    secant line, <m>L(x)</m>, from <m>(a, f(a))</m> to <m>(b,f(b))</m>.
  </p> 

  <image width="40%" source="images/MVT.png" > 
    <shortdescription>A straight line, L(x), and a wavy line f(x), both
    starting at (a, f(a)) and ending at (b, f(b)). At a point x between
    a and b the vertical distance between the two lines (from (x, L(x))
    to (x, f(x))) is labeled phi(x).</shortdescription>
  </image>

  <p>
    The vertical difference from <m>f(x)</m> to the secant line,
    indicated by <m>\phi(x)</m> in the figure should do the trick.  You
    take it from there.
  </p>

  <problem> 
    
    <idx><h>Mean Value Theorem, the</h></idx> 

    <statement> 
      <p> 
        Prove the <xref ref="thm_MVT" text="custom">Mean Value Theorem</xref>.  
      </p> 
    </statement>
  </problem>
  <p>
    Notice that the MVT is a generalization of Rolle<rsq/>s Theorem or,
    put another way, Rolle<rsq/>s Theorem is a special case of the  MVT.
  </p>

  <p> 
    The Mean Value Theorem is extraordinarily useful.  Almost all of
    the properties of the derivative that you used in Calculus follow
    more or less directly from it.  For example the following is true.
  </p>

  <corollary xml:id="cor_PosDerivIncFunc1"> 
    <statement> 
      <p> 
        If <m>f^\prime(x) > 0</m> for every <m>x</m> in the interval
        <m>(a,b)</m> then for every <m>c,d\in(a,b)</m> where
        <m>d>c</m> we have <me> f(d) > f(c) </me>.
      </p>
      
      <p> 
        That is, <m>f</m> is increasing on <m>(a,b)</m>. 
      </p>
    </statement> 
  </corollary>

  <proof> 
    <p> 
      Suppose <m>c</m> and <m>d</m> are as described in the corollary.
      Then by the Mean Value Theorem there is some number, say
      <m>\alpha\in(c,d)\subseteq(a,b)</m> such that <me>
      f^\prime(\alpha)=\frac{f(d)-f(c)}{d-c} </me>.
    </p>

    <p>
      Since <m>f^\prime(\alpha)>0</m> and <m>d-c>0</m> we have
      <m>f(d)-f(c)>0</m>, or <m>f(d)>f(c)</m>.  
    </p> 
  </proof>

  <problem>

    <idx><h>differentiation</h><h>if <m>f^\prime\lt 0</m> on an
    interval then <m>f</m> is decreasing</h></idx>

    <statement>
      <p>
        Show that if <m>f^\prime(x) \lt 0</m> for every <m>x</m> in the
        interval <m>(a,b)</m> then <m>f</m> is decreasing on
        <m>(a,b)</m>.
      </p>
</statement>
  </problem>

  <corollary xml:id="cor_PosDerivIncFunc2">
    <statement>
      <p>
        Suppose <m>f</m> is differentiable on some interval
        <m>(a,b)</m>, <m>f^\prime</m> is continuous on <m>(a,b)</m>,
        and that <m>f^\prime(c)>0</m> for some <m>c\in (a,b)</m>.
        Then there is an interval, <m>I\subset (a,b)</m>, containing
        <m>c</m> such that for every <m>x, y</m> in <m>I</m> where
        <m>x\ge y</m>, <m>f(x)\ge f(y)</m>.
      </p>
    </statement>
  </corollary>

  <problem>

    <idx><h>differentiation</h><h><m>f^\prime(a)>0</m> implies
    <m>f</m> is increasing nearby</h></idx>

    <statement>
    <p>
      

      Prove <xref ref="cor_PosDerivIncFunc2"></xref>.
    </p>
    </statement>
  </problem>

  <problem>

    <idx> <h>differentiation</h>
    <h><m>f^\prime(a)\lt 0</m> implies <m>f</m> is decreasing
    nearby</h>
    </idx>

    <statement>
      <p>
        Prove the following.
      </p>
      <p>
        Suppose that <m>f</m> is differentiable on some interval
        <m>(a,b)</m>, and <m>f^\prime </m> is continuous on
        <m>(a,b)</m>. If <m>f^\prime(c)\lt 0</m> for some <m>c\in
        (a,b)</m> then there is an interval, <m>I\subset (a,b)</m>,
        containing <m>c</m> such that for every <m>x, y</m> in
      <m>I</m> where <m>x\ge y</m>, <m>f(x)\le f(y)</m>.  </p>
    </statement>
  </problem>

  <problem xml:id="DRILLZeroDerivImpConst">
    <task>
      <statement>
        <p>
          Suppose <m>f(x)</m> is continuous on <m>[a,b]</m> and <m>f^\prime(x)=0</m> on <m>(a,b)</m>.  Show that <m>f(x)</m> is constant on <m>[a,b]</m>.


        </p>
      </statement>
      <hint>
        <p>
          Show that for any <m>x, y\in [a,b]</m>, <m>x\neq y</m>, <m>f(x)=f(y)</m>.
        </p>
      </hint>
    </task>
    <task>
      <statement>
        <p>
          Consider
          <me>
            f(x)=
            \begin{cases}
            \frac{\abs{x} }{x}\amp \text{ if } x\neq 0\\
            0\amp \text{ if } x=0
            \end{cases}
          </me>

          <!-- <me>f(x)=\left\{ \begin{array}{cc} -->
          <!-- \frac{\left|x\right|}{x} & x\neq 0 \  -->
          <!--        0 & x=0 \end{array} -->
          <!--        \right.</me>  -->
        </p>
        <p>
          Show that <m>f^\prime(x)=0</m> for <m>x\neq 0</m>.  Why
          doesn<rsq/>t this contradict part (a)?
        </p>
      </statement>
    </task>
    <task>
      <statement>
        <p>
          Suppose <m>f(x)</m> and <m>g(x)</m> are continuous on
          <m>[a,b]</m> with <m>f^\prime(x)=g^\prime(x)</m> on
          <m>(a,b)</m>.  Show that <m>f(x)=g(x)+C</m> for some constant
          <m>C</m> on <m>[a,b]</m>.
        </p>
      </statement>
    </task>
  </problem>
</section>

<section xml:id="SECTIONFTC">
  <title>The Fundamental Theorem of Calculus</title>
  <p>
    If you look back at our derivation of the Integral Form of the
    Remainder for Taylor Series (<xref ref="TaylorsTheorem"></xref>)
    you<rsq/>ll see that the <xref ref="THEOREMFTCCauchy"
    text="custom">Fundamental Theorem of Calculus</xref> provided  our
    anchoring step:
<me>
  f(x)=f(a)+\int_{t=a}^xf^\prime(t)\dx{t}=f(a)+\frac{1}{0!}f^{(1)}(t)(x-t)^0\dx{t}
  </me>.
  </p>
  <p>
    The Fundamental Theorem of Calculus was understood (in at least
    the limited context of polynomials) before Newton and Leibniz
    invented Calculus. 
    <!-- As a result neither of them dubbed it a -->
    <!-- <q>Fundamental Theorem.</q>  -->
    <!-- For them it was simply one of those -->
    <!-- known results that their Calculus re<ndash/>captured in a more -->
    <!-- elegant form than had previously been known. -->

    They both provided derivations of it via their versions of
    Calculus, but again neither of them dubbed it <q>The Fundamental
    Theorem.</q> That name was an innovation of twentieth century
    Calculus textbooks.  Both Newton and Leibniz considered it very
    natural and obvious that areas can be found by
    antidifferentiation.
  </p>

  <p>
    Using the differential and integral notation that Leibniz invented
    (and we still use today) it is easy to see why. If we suppose that
    <me>\dfdx{Y}{x}=y</me>, then it follows that

    <men xml:id="EQUATIONFTCDifferentialEquality">
      y\dx{x}=\dx{Y} 
      </men>.  

      Notice that <xref
      ref="EQUATIONFTCDifferentialEquality">equation</xref> states
      that two differentials are equal. Thus it seems apparent that if
      we add (integrate) together all such differentials between
      <m>x=a</m> and <m>x=b</m> we have (again employing Leibniz<rsq/>
      notation)
      <men xml:id="EQUATIONFTCIntDiffer">
      \int^b_{x=a}{y\dx{x}}=\int^b_{x=a}{\dx{Y}}
      </men>
  </p>
  <p>
    <xref ref="EQUATIONFTCIntDiffer" >Equation</xref> indicates
    that we are summing all of the respective differentials between
    <m>x=a</m> and <m>x=b</m>.
    </p>

    <aside>
      
      <title>
        Integral Notation: Upper and Lower Indices
      </title>
      <p>
        We said that <xref ref="EQUATIONFTCIntDiffer" >equation</xref>
        uses Leibniz<rsq/> notation but this is not entirely
        correct. Fourier innovated the use of upper and lower indices on
        the integral sign to show the limits of the integration
        approximately 150 years later. Leibniz didn<rsq/>t use
        them.
      </p>
    </aside>
    

    <p>
      Since  a finite sum of finite differences collapses into the
      difference of the extremes:

    <me>\left(a_2-a_1\right)+\left(a_3-a_2\right)+\dots
    +\left(a_{n-1}-a_{n-2}\right)+\left(a_n-a_{n-1}\right)=a_n-a_1</me>.
    Leibniz assumed that this is also true for an infinite sum of
    infinitesimals. This is probably the most intuitive understanding
    of the Fundamental Theorem of Calculus. In Leibniz<rsq/> notation
    it is

    <men xml:id="EQUATIONFTCLeibniz">\int^b_{x=a}{y\dx{x}}=\int^b_{x=a}{\dx{Y}}=Y\left(b\right)-Y(a)</men>.
  </p>
  <p>
    For Leibniz, this is all so natural and obvious that when he wrote
    about it in his 1693 paper
    <foreign>
      Supplementum geometriae dimensoriae, seu generalissima omnium
      tetragonismorum effectio per motum: similiterque multiplex
      constructio lineae ex data tangentium conditione</foreign>,
      <!-- (More on geometric measurement, or most generally of all -->
      <!-- practicing of quadrilateralization through motion: likewise -->
      <!-- many ways to construct a curve from a given condition on its -->
      <!-- tangents) --> he called it a
      <q>supplementum</q> (supplement, or corollary) rather than
      something more  imposing <mdash/> like the <term>Fundamental Theorem of Calculus</term>.
  </p>
  <p>
    Leibniz included a diagram to support this result, but he rather
    famously favored very complex diagrams in his publications.  We
    provide a simpler, more modern rendition below. 
  </p>


  <figure  xml:id="FIGUREFTC">
    <caption>A visual interpretation of the Fundamental Theorem of
    Calculus as it was understood by Leibniz.  The relationship
    between the curves is that the function on the  left,
    <m>y=y(x)</m>, is the  derivative of the function  on the right,
    <m>Y=Y(x)</m>.</caption>

    <sidebyside widths="45% 45%" margins="auto" valign="middle">
      <image source="images/FTC1.png" width="45%">
        <shortdescription></shortdescription>
      </image>

      <image source="images/FTC2.png" width="45%">
        <shortdescription></shortdescription>
      </image>

    </sidebyside>
  </figure>
  <!-- <image source="images/Integration1.png" width="90%"> -->
  <!--   <shortdescription></shortdescription> -->
  <!-- </image> -->
  <!-- \includegraphics*[width=5.43in, height=2.27in]{image15} -->

  <p>
    In <xref ref="FIGUREFTC" ></xref> the area of the infinitely thin
    rectangle on the left is given by <m>y\dx{x}</m> and is
    numerically equal to the infinitely small length <m>\dx{Y}</m> on
    the right.  Adding the areas on the left gives the area under the
    curve <m>y(x)</m> between <m>a</m> and <m>b</m>.  The sum of the
    lengths on the right gives the length of the line segment, also
    between <m>Y(a)</m> and <m>Y(b)</m>: <m>Y\left(b\right)-Y(a)</m>.
  </p>
  <p>
    Such an approach does not pass modern, or even <m>19</m>th century,
    standards of rigor.  Even in the <m>17</m>th century it was known that
    there are logical problems with interpreting an integral in terms
    of infinitiesimals. But the infinitesimal approach was adequate to
    the needs of the time so a closer investigation into the nature of
    the integral was left until infinitesimals were no longer
    sufficient.
  </p>
  <problem>
    <p>
      One question which eventually led to such a closer investigation
      was, <q>Does every continuous function have an antiderivative.</q>
      What do you think?
    </p>
    <!-- <p> -->
    <!--   Decide whether you believe that every continuous function must -->
    <!--   necessarily have an antiderivative or not and explain your -->
    <!--   reasoning. -->
    <!-- </p> -->

  </problem>
  <p>
    At this point we have the tools necessary for a rigorous
    proof of the Fundamental Theorem of Calculus. What we do not have
    is an adequate definition of the integral.
    <!-- As we said it turns out -->
    <!-- that thinking of the integral as an antiderivative is -->
    <!-- insufficient.  -->
    We<rsq/>ll need a definition that is independent of
    differentiation, but which recovers all of the properties of
    integration that you are already familiar with from your Calculus
    course, including the Fundamental Theorem of Calculus.
  </p>
  <p>
    We<rsq/>ll provide such a definition in <xref
    ref="SECTIONDefiningIntegral" ></xref> (the next section), but
    since we are already familiar with the properties we<rsq/>ll need
    we will proceed with the proof of the Fundamental Theorem of
    Calculus now.
  </p>
  <p>
    The following formulation and proof of the Fundamental Theorem of
    Calculus is from Cauchy<rsq/>s 1823 publication <foreign>Rsum
    des leons donns  l<rsq/> cole royale polytechnique sur le
    calcul infinitesimal</foreign> (Summary of the lessons given at
    the Royal Polytechnic School on infinitesimal calculus).
  </p>

  <theorem xml:id="THEOREMFTCCauchy">
    <title>The Fundamental Theorem of Calculus (Cauchy)</title>

    <idx><h>Fundamental Theorem of Calculus, The</h></idx>

    <p>
      Suppose <m>f\left(x\right)</m> is continuous on <m>[a,b]</m> and define
    </p>
    <p>


      <me>I\left(x\right)=\int^x_{t=a}{f\left(t\right)\dx{t}}</me>
      for <m>x\in [a,b]</m>.  Then <m>I</m> is continuous on <m>[a,b]</m>, differentiable on <m>(a,b)</m> and
    </p>
    <p>


      <me>I^\prime\left(x\right)=f(x)</me>
    </p>
  </theorem>
  <p>
    Notice that <xref
    ref="THEOREMFTCCauchy"></xref> and <xref ref="EQUATIONFTCLeibniz"
    >equation</xref> are very closely related though they come at the
    question of integration from different viewpoints.
  </p>
  <p>
    <xref ref="EQUATIONFTCLeibniz" >Equation</xref> starts with the
    assumption that <m>\dfdx{Y}{x}=y </m>. It says that if we
    sum the differentials <m>y\dx{x}</m> from  <m>x=a</m> to
    <m>x=b</m> then the sum collapses to the difference of the
    extremes: <m>Y(b)-Y(a)</m>.
  </p>


  <!-- <p> -->
  <!--   <xref -->
  <!--       ref="EQUATIONFTCLeibniz" >Equation</xref> is understood to -->
  <!--   mean that we are summing the differentials between <m>x=a</m> and -->
  <!--   <m>x=b</m> sequentially, starting at <m>x=a</m>. And the result -->
  <!--   of this is a sum which collapses to the difference of the -->
  <!--   extremities: <m>Y(b)-Y(a)</m>. The connection to the -->
  <!--   derivative of <m>Y</m> comes from the differential equation we -->
  <!--   started with: <m>\dfdx{Y}{x}=y</m>. -->
  <!-- </p> -->
  <p>
    On the other hand <xref ref="THEOREMFTCCauchy" ></xref> starts
    with the assumption that <m>\int_{t=a}^x f(t)\dx{t}</m> is well
    defined, uses this to define the function <m>I(x)</m> ( which is
    simply <m> Y(x)</m> by another name), and then concludes where
    Leibniz began <mdash/> with the statement that <m>I^\prime
    (x)=f(x)</m>, (or <m>\dfdx{Y}{x}=y</m>). Our task in the next
    section will be to provide a definition that will support that
    conclusion.
  </p>
  <p>
    In most Calculus texts <xref ref="EQUATIONFTCLeibniz" >equation</xref> is
    called a <term>definite integral</term>, and the function defined
    in <xref ref="THEOREMFTCCauchy" ></xref> is called an
    <term>indefinite integral</term>. The two of them are often
    referred to as parts <m>1</m> and <m>2</m> of the Fundamental Theorem of
    Calculus.
  </p>
  <p>
    We will now proceed with the proof of Cauchy<rsq/>s version of the
    Fundamental Theorem of Calculus, with the caveat that the proof is
    not complete until we have defined the function
    <m>I(x)=\int_{t=a}^x f(t)\dx{t}</m> and shown that under our
    definition it has the properties we expect from an integral. We
    will need some of these properties in the proof below.
  </p>
  <proof>
    <title>Sketch of Proof</title>
<p>
  There are two statements to prove:
  <ol>
    <li>
      That <m>I(x)</m> is continuous on the closed interval
      <m>[a,b]</m>,
    </li>
    <li>
      and that <m>I(x)</m> is differentiable on the open interval
      <m>(a,b)</m>.
    </li>
  </ol> 
  We will prove differentiablity on <m>(a,b)</m> first. From that
  continuity on <m>[a,b]</m> follows immediately (why?). Continuity at
  the endpoints will be addressed separately.
</p>

    <p>
      Let <m>x\in (a,b)</m>. To find <m>I^\prime (x)</m> we apply
      <xref ref="def_derivative" ></xref>. Thus 

      <md>
        <mrow>I^\prime (x)\amp{} = \limit{h}{0}{\frac{I(x+h)-I(x)}{h}}</mrow>
        <mrow>
          \amp{} =       \limit{h}{0}{\frac{\int_{t=a}^{x+h}f(t)\dx{t}-\int_{t=a}^x
        f(t)\dx{t}}{h}}</mrow>
        <mrow>
          \amp{}=\limit{h}{0}{\frac{\int_{t=x}^{x+h}f(t)\dx{t}}{h}}.
        </mrow>
      </md>

      <!-- First, we want to show that -->
      <!--     </p> -->
      <!-- <p> -->
      <!-- <me> -->
      <!-- I^\prime(x) = -->
      <!-- \limit{h}{0}{\frac{\int_{t=a}^{x+h}f(t)\dx{t}-\int_{t=a}^x -->
      <!-- f(t)\dx{t}}{h}}=\limit{h}{0}{\frac{\int_{t=x}^{x+h}f(t)\dx{t}}{h}}=f(x) -->
      <!-- </me>. -->
      Since <m>f(t)</m> was assumed to be continuous on <m>[a,b]</m>
      it is also continuous on the closed interval with endpoints
      <m>x</m> and <m>x+h</m>. We know from the <xref ref="thm_EVT"
      text="custom">Extreme Value Theorem</xref> that there are points
      <m>c</m>, and <m>C</m>, in the same interval such that <m>f(c)</m>
      and <m>f(C)</m> are the global minimum and maximum of <m>f</m>
      on the closed interval with endpoints <m>x</m> and  <m>x+h</m>, respectively.
    </p>
    <p>
      Thus
      if <m>h\gt0</m> we have

      <me>f\left(c\right)\cdot h\le
      \int^{x+h}_{t=x}{f(t)\dx{t}}\le f(C)\cdot h</me>
      or
      <me>f\left(c\right)\le \frac{\int^{x+h}_{t=x}{f(t)\dx{t}}}{h}\le
      f\left(C\right)</me>.
    </p>
    <p>



      If <m>h\lt 0</m>, we have <m>-h>0</m>, and so
      <me>f\left(c\right)\cdot \left(-h\right)\le
      \int^x_{t=x+h}{f(t)\dx{t}}\le f\left(C\right)\cdot (-h)</me>
      <me>f(c)\le \frac{\int^x_{t=x+h}{f(t)\dx{t}}}{-h}\le f(C)</me>
      In either case we have
      <me>f(c)\le \frac{\int^{x+h}_{t=x}{f(t)\dx{t}}}{h}\le f\left(C\right)</me>
    </p>
    <p>
      Applying the <xref ref="thm_SqueezeTheoremFunctions" text="custom">Squeeze Theorem</xref> and <xref ref="def_continuity" ></xref> 
      of <m>f</m> at <m>x</m> should do the trick.
    </p>
    <!-- <p> -->
    <!--   The above outline shows that <m>I(x)</m> is differentiable on -->
    <!--   <m>(a,b)</m>, so it follows that <m>I(x)</m> is also -->
    <!--   continuous on <m>[a,b]</m> (why?). -->
    <!-- </p> -->
    <p>
      To show that <m>I(x)</m> is  continuous  at the endpoints
      <m>a</m> and <m>b</m>, we will appeal to <xref
      ref="thm_LimDefOfContinuity" ></xref>.
    </p>
    <p>
      Consider any sequence <m>(x_n)</m> contained in <m>[a,b]</m>
      and converging to <m>a</m>. We want to show that
      <me>
        \limit{n}{\infty }{\left(\int^{x_n}_{t=a}{f(t)\dx{t}}-\int^a_{t=a}{f(t)\dx{t}}\right)}
        =\limit{n}{\infty }{
      \left(\int^{x_n}_{t=a}{f(t)\dx{t}}\right)}=0 </me>
    </p>
    <p>
      To get continuity at <m>b</m>, consider any sequence
      <m>\left(y_n\right)</m> in <m>[a,b]</m> converging to
      <m>b</m>.  We want to show that
      <me>
        \limit{n}{ \infty }
        {\left(\int^b_{t=a}{f(t)\dx{t}}-\int^{y_n}_{t=a}{f(t)\dx{t}}\right)
        }=\limit{n}{\infty } {\left(\int^b_{t=y_n}{f(t)\dx{t}}\right)}=0
      </me>
    </p>
<p>
  It will be useful to recognize that on <m>[a,b]</m> the function
  <m>f(x)</m> will have both a maximum and a minimum value (why?).
</p>

    <!-- <p> -->
    <!--   We can use an argument similar to what we did before, but in this -->
    <!--   case, since we are not dividing, we can just consider the minimum -->
    <!--   <m>m</m> and maximum <m>M</m> of <m>f</m> on <m>[a,b]</m> so that -->
    <!--   <m>m\le f\left(t\right)\le M</m>. -->
    <!-- </p> -->
  </proof>


  <problem>
    <p>
      Turn the above ideas into a proof of <xref
      ref="THEOREMFTCCauchy"></xref>. Don<rsq/>t forget to justify
      every step in the <q>Sketch of Proof</q> above.
    </p>
  </problem>

  <problem>

    <introduction>

      <p>
      Suppose <m>f(x)</m> is continuous on <m>[a,b]</m> and
      <m>I(x)</m> is the antiderivative of <m>f(x)</m> from <xref
      ref="THEOREMFTCCauchy" ></xref>. Suppose further that
      <m>F(x)</m> is continuous on <m>[a,b]</m> with <m>F^\prime
      (x)=f(x)</m> on <m>(a,b)</m>. 
    </p>
    </introduction>
    <task>
      <statement>

        <p>

          Prove that for any <m>x\in
        [a,b]</m>,
        <me>
          I(x)=\int^x_{t=a}{f\left(t\right)\dx{t}}=F\left(x\right)-F(a)
        </me>

        </p>
      </statement>
    </task>
    <task>
      <statement>
        <p>
          Use the result in part (a) to show that
          
          <me>
            \int^b_{t=a}{f\left(t\right)\dx{t}}=F\left(b\right)-F(a)
          </me>
        </p>
      </statement>
      <hint>
        <p>          You have two antiderivatives of <m>f(x)</m>.  By part (c) of
        <xref ref="DRILLZeroDerivImpConst"></xref>, these must differ
        by a constant.  What must this constant be?
        </p>
      </hint>
    </task>


  </problem>
  <!-- <problem> -->

  <!--   <statement> -->
  <!--     <p> -->
  <!--       Suppose <m>F(x)</m> is continuous on <m>[a,b]</m> and -->
  <!--       <m>F^\prime\left(x\right)=f(x)</m> on <m>(a,b)</m>. Prove that for -->
  <!--       any <m>x\in [a,b]</m>, -->



  <!--       <me>\int^x_{t=a}{f\left(t\right)\dx{t}}=F\left(x\right)-F(a)</me> -->
  <!--       In particular, -->



  <!--       <me>\int^b_{t=a}{f\left(t\right)\dx{t}}=F\left(b\right)-F(a)</me> -->
  <!--     </p> -->
  <!--   </statement> -->
  <!--   <hint> -->
  <!--     <p> -->
  <!--       You have two antiderivatives of <m>f(x)</m>.  By part (c) of -->
  <!--       <xref ref="DRILLZeroDerivImpConst"></xref>, these must differ -->
  <!--       by a constant.  What must this constant be? -->
  <!--     </p> -->
  <!--   </hint> -->

  <!-- </problem> -->
  <p>
    An obvious question is, how do we know that a continuous function
    on a closed interval has an antiderivative? That is, how do
    we know that <me>I\left(x\right)=\int^x_{t=a}{f(t)\dx{t}}</me>
    actually exists?
  </p>
</section>

<section xml:id="SECTIONDefiningIntegral">
  <title>The Definition of the Integral</title>
  <introduction>

    <p>
      <!-- The seeds of what we call integration were planted long ago. --> 
      In a
      letter to 
      <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Eratosthenes/" visual="mathshistory.st-andrews.ac.uk/Biographies/Eratosthenes/">Eratosthenes</url>
      (circa 250 BC), 
      <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Archimedes/" visual="mathshistory.st-andrews.ac.uk/Biographies/Archimedes/">Archimedes</url>
      described what he called a
      mechanical method for finding areas and volumes.  His method
      consisted of mentally dividing objects into infinitely thin slices
      and balancing these slices on an imaginary balance.  Archimedes noted
      that while his method was not rigorous it was still quite useful:
    </p>
    <blockquote>
      <p>
        <q>. . . I thought it might be appropriate to write down for you a
        special method, by means of which you will be able to recognize
        certain mathematical questions with the aid of mechanics.  I am
        convinced that this is no less useful than finding the proofs of
        these same theorems.</q>
      </p>
      <p>
        <q>Some things, which first became clear to me by the mechanical
        method, were afterwards proved geometrically, because their
        investigation by that method does not furnish an actual
        demonstration.  It is easier to supply the proof when we have
        previously acquired, by the method, some knowledge of the
        questions than it is to find it without any previous
        knowledge . . .</q>
      </p>
    </blockquote>

    <p>
      Closer to our time 
      <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Cavalieri/" visual="mathshistory.st-andrews.ac.uk/Biographies/Cavalieri/">Cavalieri</url>, 
      <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Torricelli/" visual="mathshistory.st-andrews.ac.uk/Biographies/Torricelli/">Torricelli</url>, 
      <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Kepler/" visual="mathshistory.st-andrews.ac.uk/Biographies/Kepler/">Kepler</url>, 
      <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Galileo/" visual="mathshistory.st-andrews.ac.uk/Biographies/Galileo/">Galileo</url>,
      <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Roberval/" visual="mathshistory.st-andrews.ac.uk/Biographies/Roberval/"> Roberval</url>,
      and others explored a similar idea. They too mentally  cut geometric areas (and volumes) into infinitely thin slices.  But rather than using an imaginary balance they compared them geometrically.
    </p>
    <p>
      This division of objects into infinitely small pieces in order to
      analyze them is the essential idea underlying Calculus but as
      Archimedes observed it is questionable as a method of proof. For practical applicatons
      this is still a useful way to think.
    </p>
    <p>
      It had always been recognized that infinitesimals were an
      inadequate foundation for Calculus     but it was  
      at the beginning of the 19th century, in large part due to the
      work of Fourier, that it became imperative to replace it.
    </p>
    <p>
      Fourier<rsq/>s work raises the question: How random> can a
      function defined on an interval be and still be represented by a
      Fourier series?  Since the coefficients are computed via
      integration, a closely related question is how random can a
      function be and still be integrable?
    </p>
    <p>
      Since this text is intended as a one semester introduction to real
      analysis we will not be able to fully answer that question here.
      But we will give one rigorous definition of the integral (there
      are many) and use it to show that a continuous function on a
      closed interval is integrable.  This will close the gap in
      our proof of the <xref ref="THEOREMFTCCauchy" text="custom" >Fundamental Theorem of Calculus</xref>.
      <!--  and show that a -->
      <!-- continuous function on an interval has an antiderivative. --> 
      <!-- We
           have a warning that the details in this -->
      <!-- rigorous approach can get a bit overwhelming at times, but keep in -->
      <!-- mind the idea of trying to compute an area under a curve without -->
      <!-- resorting to infinitesimals underlies these ideas. -->
    </p>
</introduction>
<subsection xml:id="SUBSECTIONCauchyRiemannInt">
  <title>Cauchy<rsq/>s Integral Definition</title>


  <p>
    One of the first mathematicians to provide a rigorous definition
    of a definite integral was <url
    href="https://mathshistory.st-andrews.ac.uk/Biographies/Cauchy/"
    visual="mathshistory.st-andrews.ac.uk/Biographies/Cauchy/">Augustin Louis
    Cauchy</url> in 1823. 
    Cauchy used the limit idea to bridge the gap between finite sums
    of (finitely many) very small (but still finite) pieces and
    infinite sums of infinitesimals.
  </p>
  <p>
    It was common practice to
    approximate an integral whose antiderivative was not readily
    computable by a finite sum as seen in <xref
    ref="FIGURECauchyIntFinitSum" ></xref>.
    
  </p>

    <figure  xml:id="FIGURECauchyIntFinitSum">
      <caption></caption>
      <image source="images/Integration2.png" width="45%">
        <shortdescription></shortdescription>
      </image>
    </figure>
    <p>

      To approximate <m>\int^b_{x=a}{f(x)\dx{x}}</m>, Cauchy started
      by partitioning <m>P</m> of the interval<m> [a,b]</m> into a
      finite number of subintervals.  Basically, the partition
      <m>P</m> is a finite sequence of numbers 
      <me>
        a=x_0\lt x_1\lt x_2\lt\dots \lt x_{n-1}\lt x_n=b 
        </me>.  
        In the figure <m>n=5</m>. He then formed the sum

        <md>
          <mrow>
            f\left(x_0\right)\left(x_1-x_0\right)\amp{}+f\left(x_1\right)\left(x_2-x_1\right)+\dots
          </mrow>
          <mrow>
            \amp{}\dots
            +f\left(x_{n-1}\right)\left(x_n-x_{n-1}\right)=\sum^{n-1}_{k=0}{f(x_k)(x_{k+1}-x_k)}
          </mrow>

        </md>
    </p>
    <p>


      If <m>f\left(x\right)\ge 0</m> as in <xref
      ref="FIGURECauchyIntFinitSum" ></xref> we see that we are
      approximating the area under the curve <m>y=f(x)</m> with the
      area of a finite sum of boxes whose bases are the subintervals
      <m>[x_k,x_{k+1}]</m> and whose heights are obtained by
      evaluating <m>f</m> at some point in <m>[x_k,x_{k+1}]</m>. In
      our figure we used the left endpoint <m>x_k</m> for
      convenience.       Notice that the subintervals need not be the
      same length.
    </p>
    <p>
      Diagrams like this are the source of the common
      misunderstanding that an integral computes area. In certain
      special cases it does, but not always. Yet it is often helpful
      to think of an integral that way.
    </p>
    <p>

      We
      define the <term>norm</term> of the partition <m>\norm{P}</m>
        to be to be the length of the largest subinterval:
        <me>
          \norm{P}=\max_{k=0, 1, \dots    n-1}(x_{k+1}-x_k) 
          </me>.

          Cauchy said that a function
          <m>f(x)</m> defined on <m>[a,b]</m> was integrable if there was a
          number <m>I</m> such that for all <m>\eps >0</m>, there is a
          <m>\delta >0</m> such that whenever the norm of the partition,
          <m>\norm{P}</m> is less than <m>\delta{}</m> the difference
          between <m>I</m> and the  associated sum will be less than
          <m>\eps{}</m>. Symbolically this is 
          <me>\norm{P}\lt \delta \imp \abs{\sum^{n-1}_{k=0}{f\left(x_k\right)\left(x_{k+1}-x_k\right)}-I}\lt
          \eps </me>.
          Notice that <m>P</m> can be any partition as long as
          <m>\norm{P}\lt\delta </m>.
      </p>
        <p>
          In this case we write <me>I=\int^b_{x=a}{f(x)\dx{x}}</me>
        </p>
        <p>
          Using this definition Cauchy was then able to show that any
          continuous function is (Cauchy) integrable and was able to
          prove the <xref ref="THEOREMFTCCauchy"
          text="custom">Fundamental Theorem of Calculus</xref> as we
          indicated in the last section. More formally, Cauchy made
          the following definition.
        </p>

        <definition xml:id="DEFINITIONRiemannIntegral">
          <title>The Riemann Integral</title>

          <idx><h>Definition</h><h>Riemann Integral</h></idx>
          <idx><h>Riemann Integral</h></idx>

          <statement>

            <p>
              Given a function <m>f(x)</m> defined on the interval
              <m>[a,b]</m>, we say <m>f</m> is integrable on
              <m>[a,b]</m> if and only if there is a number <m>I</m>
              such that for each <m>\epsilon >0</m>, there is a
              <m>\delta >0</m> such that for any partition
              <m>P=\{x_0, x_1, \cdots, x_n\}</m> of <m>[a,b]</m>
              with <m>\norm{P}\lt\delta </m>, we
              have


              <me>\left|\sum^{n-1}_{k=0}{f\left(x^*_k\right)\left(x_{k+1}-x_k\right)}-I\right|\lt\epsilon </me> 


               for any choice of <m>x^*_k</m> where <m>x_k\le x^*_k\le x_{k+1}</m>.

            </p>
            <!-- <p> -->
            <!--   Given a function <m>f(x)</m>, defined on the interval -->
            <!--   <m>[a,b]</m> we say that <m>f</m> is integrable on -->
            <!--   <m>[a,b]</m> if and only if, for every <m>\eps\gt 0</m>, -->
            <!--   and  every partition -->

            <!--   <me> -->
            <!--     P=\left\{x_0, x_1, x_2, \cdots, x_n\right\}  -->
            <!--     </me> -->
            <!--     where <m>x_0=a</m>, <m>x_n=b</m>,1 and <m>x_k\lt -->
            <!--     x_{k+1}</m> <m>\forall</m> <m>k=0,\ldots,n-1</m>,  -->
            <!--     <m>\exists</m> -->
            <!--     <m> -->
            <!--     \delta \gt 0</m> such that -->
            <!--     <me>\norm{P}\lt\delta \imp -->
            <!--     \abs{\sum^{n-1}_{k=0}{f\left(x_k^*\right)\left(x_{k+1}-x_k\right)}-I}\lt -->
            <!--     \eps </me> -->
            <!--     where <m>x_k\lt{}x_k^*\lt x_{k+1}</m>. -->

            <!-- </p> -->
            <aside>
              <title>Historical Context</title>
              <p>
                No doubt you are wondering why this is called the
                <term>Riemann Integral</term> when it was devised by
                Cauchy. 
              </p>
              <p>
                It often happens in mathematics that important
                concepts do not get named for the person who invents
                them. In this instance Cauchy showed that continuous
                functions are integrable under his definition, but
                left open the question, <q>Is it necessary for a
                function to be continuous for it to be integrable?</q>
              </p>
              <p>
                The answer to that question is emphatically <q>No!</q>
                as Riemann showed in 1868. In fact Riemann was able to
                provide necessary and sufficient conditions for a
                function to be integrable under Cauchy<rsq/>s
                definition which are far weaker than continuity.  We
                won<rsq/>t go into that as it does not serve our
                purposes here, but as a result of Riemann<rsq/>s work
                Cauchy<rsq/>s definition of the integral came to be
                called the <term>Riemann integral</term>.
              </p>
              <!-- <p> -->
              <!--   In that case we say that  -->
              <!--   <me>\int_a^b f(x)\dx{x} =I</me>. -->
              <!-- </p> -->
            </aside>

          </statement>
        </definition>
        <!-- <p> -->

        <!--   In 1854, <url -->
        <!--   href="https://mathshistory.st-andrews.ac.uk/Biographies/Riemann/" -->
        <!--   visual="mathshistory.st-andrews.ac.uk/Biographies/Riemann/">Bernhard -->
        <!--   Riemann</url> presented a generalization of Cauchy<rsq/>s idea for -->
        <!--   a definite integral to the faculty at the University of Gttingen -->
        <!--   which was later published in 1868.  His idea was similar to -->
        <!--   Cauchy<rsq/>s, except that instead of restricting the heights of -->
        <!--   the boxes to the left endpoints of the subintervals, he allowed -->
        <!--   the height of <m>k</m>th box to be <m>f(x^*_k)</m> where -->
        <!--   <m>x^*_k</m> is any point in the subinterval <m>[x_k,x_{k+1}]</m>. -->
        <!--   Any Riemann integrable function is automatically Cauchy integrable -->
        <!--   and in this case the integrals are the same.  Furthermore, Riemann -->
        <!--   was able to provide necessary and sufficient conditions for when a -->
        <!--   function was Riemann integrable.  We won<rsq/>t go into that as it -->
        <!--   does not serve our purposes here. -->
        <!-- </p> -->

        <p>
          The similarity between <xref ref="DEFINITIONRiemannIntegral"
          ></xref>  and the definition of a limit is hard to miss so
          sometimes the Riemann integral is defined via the limit
          symbol as 
          <men xml:id="EQUATIONCauchyIntAsLim">
            \int_a^bf(x)\dx{x} =
            \limit{\norm{P}}{0}{\sum^{n-1}_{k=0}{f\left(x_k\right)\left(x_{k+1}-x_k\right)}}
          </men>
          but in our (the authors<rsq/>) opinions this notatation serves to hide the
          important ideas rather than elucidate them because the limit
          in <xref ref="EQUATIONCauchyIntAsLim" >equation</xref> is
          very different from the ones we<rsq/>ve encountered
          before.
        </p>
        <p>
          In the past, when we had limits like
          <m>\limitt{x}{2}{\frac{x^2-4}{x-2}}</m> we only had to think
          about letting the single variable <m>x</m> get <q>close
          to</q> <m>2</m>. But the limit in <xref
          ref="EQUATIONCauchyIntAsLim" >equation</xref> is far more
          complex. It asks us to simultaneously think about all
          possible partitions with the property that
          <m>\norm{P}\lt\delta{}</m> and  what is happening when
          <m>\norm{P} \rightarrow 0</m>.
        </p>
        <p>
          Because of these issues we will use an equivalent
          formulation of the definite integral. One which makes use of a
          concepts we<rsq/>ve already familiar with: the <xref ref="thm_LUB" text="custom">least upper bound</xref>
          and <xref ref="PROBLEMGreatestLowerBound" text="custom">greatest lower bound</xref> properties of the real number
          system.

          <!-- It can be done, but we<rsq/>d prefer something a -->
          <!-- little easier to think about. Darboux<rsq/>s definition is a -->
          <!-- bit simpler in that we only need to think about a least -->
          <!-- upper bounds and a greatest lower bound -->
        </p>
      </subsection>
<subsection xml:id="SUBSECTIONDarbouxIntegral">
  <title>Darboux<rsq/>s Integral Definition</title>

  <p>
    Notice that neither nor the definition of the <xref
    ref="DEFINITIONRiemannIntegral" text="custom">integral</xref>
    nor the definition of the <xref ref="def_derivative"
    text="custom">derivative</xref> tells us how to compute the
    quantity in question. In the 19th century how to compute both
    integrals and derivatives was as well known as it is today.  These
    definitions are about providing a rigorous foundation for these
    ideas, not computing them.
  </p>

  <figure  xml:id="FIGUREDarbouxPortrait">
    <caption><url href="https://mathshistory.st-andrews.ac.uk/Biographies/Darboux/" visual="mathshistory.st-andrews.ac.uk/Biographies/Darboux/">Jean Gaston Darboux</url></caption>

    <idx><h>Portraits</h><h>Darboux</h></idx>
    <idx><h>Darboux, Jean</h><h>portrait of</h></idx>

    <image source="images/Darboux.png" width="35%">
      <shortdescription></shortdescription>
    </image>
  </figure>

  <p>
    In 1875 Jean Gaston Darboux developed a different (yet equivalent)
    definition of the Riemann integral which uses the least upper and
    greatest lower bounds we learned about in <xref ref="IVTandEVT"
    ></xref>.  There are discontinuous functions which are Darboux
    (and Riemann) integrable, but to keep things simple we will
    restrict our attention to continuous functions.
  </p>
  <p>
    As before, we will start with a partition <m>P=\{x_0, x_1,x_2,
    \dots , x_n\}</m> of the interval <m>[a,b]</m> where <m>a=x_0\lt
    x_1\lt \dots \lt x_{n-1}\lt x_n=b</m>.  Let <m>m_k</m> and
    <m>M_k</m> denote the minimum and maximum of <m>f(x)</m> on
    <m>[x_k,x_{k+1}]</m>, respectively.  Define the lower (Darboux)
    sum <m>L(P)</m> by
    <me>L\left(P\right)=\sum^{n-1}_{k=0}{m_k\left(x_{k+1}-x_k\right)}</me>
    and upper (Darboux) sum <m>U(P)</m> by
    <me>
      U\left(P\right)=\sum^{n-1}_{k=0}{M_k\left(x_{k+1}-x_k\right)}</me>.
  </p>
  <p>


    Notice that <alert>if</alert> the integral 
    <me>
      \int^b_{x=a}{f(x)\dx{x}}
    </me>
    exists, then it is intuitively clear that <me>L\left(P\right)\le
    \int^b_{x=a}{f(x)\dx{x}}\le U(P)</me> It is also intuitively clear
    that as the number of intervals gets larger, these bounds get
    closer to the actual integral (again, if it exists). If you
    don<rsq/>t see this try drawing a few representative examples.
  </p>
  
  <definition xml:id="DEFINITIONPartitionRefine">
    <title>Partition Refinement</title>

    <idx><h>partition refinement</h></idx>
    <idx><h>Definition</h><h>partition refinement</h></idx>

    <statement>

      <p>
        Given two partitions
        <me>
          P^\prime=\{x^\prime_0, x^\prime_1,
          x^\prime_2, \dots , x^\prime_{m-1},x^\prime_m\}
        </me>
        and
        <me>
          P=\{x_0, x_1, x_2, \dots , x_{m-1},x_m\}
        </me>
        <m>P^\prime </m> is said to be a <term>refinement</term> of
        <m> P</m> if every point in <m>P</m> is also a point in <m>
        P^\prime</m>. That is, <m>P\subset P^\prime </m>.
      </p>

    </statement>
  </definition>



  <problem>

    <statement>

      <p>
        Show that if <m>P^\prime</m> is a
        refinement of <m>P</m>, then

        <me>
          L\left(P\right)\le L\left(P^\prime\right)\le
          U\left(P^\prime\right)\le U(P)
        </me>
      </p>

</statement>


    <hint>
      <p>
        First, show that this is true if <m>P^\prime</m> is obtained
        by adding one point to <m>P</m>.
      </p>
    </hint>

  </problem>

      <problem xml:id="PROBLEMPartitionIneq1">

        <statement>
          <p>
            Let <m>P </m>and <m> \Pi </m> be any two partitions of
            <m>[a,b]</m>.  Show that <me>L\left(P\right)\le
            U(\Pi{})</me>
          </p>

        </statement>
        <hint>
          <p>
            Consider that <m>Q=P\cup \Pi </m> is a refinement of both
            <m>P</m> and <m>\Pi </m>, and use the previous result.
          </p>
        </hint>
      </problem>

      <p>
        Next observe  that the set of lower sums over all
        partitions of <m>[a,b]</m> is a non<ndash/>empty set of real
        numbers which is bounded above.  Therefore by <xref ref="thm_LUB" ></xref>  it has a least upper
        bound.  We define the lower (Darboux) integral by
        <me>
          \underline{\int^b_{x=a}}{f(x)}\dx{x}=\sup_{P}
          \left(L(P)\right)
        </me>
      </p>

      <p>
        Similarly, the set of all upper sums is a non<ndash/>empty set of
        real numbers which is bounded below and therefore has a
        greatest lower bound. We define the upper (Darboux) integral
        as the greatest lower bound of this set

        <me>
          \overline{\int^{b}_{x=a}}{f(x)\dx{x}}=\inf_{P} \left(U(P)\right)
        </me>
        </p>

        <problem>

          <statement>
            <p>
              Show that <me>\underline{\int^b_{x=a}}{f(x)}\dx{x}\le
              \overline{\int^{b}_{x=a}}{f(x)\dx{x}}</me>
            </p>

          </statement>
          <hint>
            <p>
              Let <m>P^\prime</m> be a fixed partition of <m>[a,b]</m>.
              By <xref ref="PROBLEMPartitionIneq1"></xref>, <m>U(P^\prime)</m> is an upper bound for the
              set of all lower sums.  This says 
              <me>
                \underline{\int^b_{  x=a  }}{f(x)}\dx{x}\le U(P^\prime)
                </me>.
            </p>
            <p>
              However, <m>P^\prime</m> is an arbitrary partition of
              <m>[a,b]</m>.]
            </p>
          </hint>

        </problem>

        <definition xml:id="DEFINITIONDarbouxIntegral">
          <title>Darboux Integrability</title>

          <idx><h>Darboux Integrability</h></idx>
          <idx><h>Definition</h><h>Darboux Integrability</h></idx>

          <statement>
            <p>
              A function is said to be (Darboux) integrable provided
              <me>
                \underline{\int^b_{x=a}}{f(x)}\dx{x}=\overline{\int^{b}_{x=a}}{f(x)\dx{x}}
              </me>
            </p>
            <p>

              In this case we define the (Darboux) integral by

              <me>
                \int^b_{x=a}{f(x)\dx{x}}=\underline{\int^b_{x=a}}{f(x)}\dx{x}=\overline{\int^{b}_{x=a}}{f(x)\dx{x}}
              </me>
            </p>

          </statement>
        </definition>

        <p>
          We have restricted our attention to continuous functions so
          that in the definitions of the upper <m>U(P)</m> and lower
          <m>L(P)</m> sums we could ensure that on each subinterval
          the function in question has a minimum <m>m_k</m> and a
          maximum <m>M_k</m>, per the <xref ref="thm_EVT"
          text="custom">Extreme Value Theorem</xref>.  By tweaking
          <xref ref="DEFINITIONDarbouxIntegral" ></xref> a bit it is
          possible to extend integrability to some discontinuous
          functions, but not all of them. For example a slight
          modification of either Cauchy<rsq/>s or Darboux<rsq/>s
          definition of the Riemann integral will allow us to include
          the step function:
          <me>
              S(x)=
              \begin{cases}
              0\amp \text{ if } x\lt0\\
              1\amp \text{ if } x\gt0
              \end{cases}
              </me>
              but we will not pursue these extensions any
              further.
        </p>
        <p>
          The next problem displays a function invented by <url
          href="https://mathshistory.st-andrews.ac.uk/Biographies/Dirichlet/"
          visual="mathshistory.st-andrews.ac.uk/Biographies/Dirichlet/">Lejeune
          Dirichlet</url> (1805<ndash/>1859) in 1837 which is not Riemann integrable.


          <!--     bounded, but not necessarily continuous functions. This would require using the infimum and -->
          <!-- supremum of <m>f(x)</m> on each subinterval rather than the -->
          <!-- minimum and maximum, but we won<rsq/>t pursue that extention -->
          <!-- here. -->

        </p>

        <figure >
          <caption>Lejeune Dirichlet</caption>

          <idx><h>Portraits</h><h>Dirichelet</h></idx>
          <idx><h>Dirichelet, Lejeune</h><h>portrait of</h></idx>

          <image source="images/Dirichlet.png" width="35%">
            <shortdescription>Portrait of Dirichlet</shortdescription>
          </image>
        </figure>


        <problem xml:id="PROBLEMDiricheletCountrExamp">

          <statement>
            <p>
              In honor of Dirichlet his function is often denoted
              <m>D(x)</m>. It is defined as follows:
              <me>
                D(x)=
                \begin{cases}
                0\amp \text{ if } x \text{ is rational}\\
                1\amp \text{ if } x \text{ is irrational}
                \end{cases}
                </me>.

            </p>
            <p>
              Show that <m>D(x)</m> is not Riemann integrable on
              <m>[0,1]</m>.
            </p>

</statement>

          <hint>
            <p>
              Use <xref ref="DEFINITIONDarbouxIntegral" text="custom">
              Darboux<rsq/>s definition</xref> of the Riemann
              integral.
            </p>
          </hint>
        </problem>

        <p>
          As we mentioned earlier, Darboux<rsq/>s definition of the
          integral is equivalent to the Riemann integral, in the sense
          that any function which is Riemann integrable is also
          Darboux integral and <foreign>vice versa</foreign>. This is
          similar to having both an <xref ref="def_continuity"
          text="custom">analytic definition of continuity</xref> and a
          <xref ref="thm_LimDefOfContinuity"
                text="custom">sequence<ndash/>based definition of
          continuity</xref>. We can use whichever definition works
          better for the problem at hand.  
        </p>
        <p>
          For example, it is straightforward to derive the properties
          of definite integrals that you learned in Calculus using
          Cauchy<rsq/>s formulation.  But to show that a continuous
          function is integrable it is a little simpler to use
          Darboux<rsq/>s formulation as we will see next.
        </p>

        <theorem xml:id="THEROEMContImpInt">
          <p>
            If <m>f\left(x\right)</m> is continuous
            on <m>[a,b]</m>, then


            <me>\underline{\int^b_{x=a}}{f(x)}\dx{x}=\overline{\int^{b}_{x=a}}{f(x)\dx{x}}</me>
            so <m>f(x)</m> is integrable.
          </p>
        </theorem>

        <proof  xml:id="THEOREMContImpIntProof">
          <title>Sketch of Alleged Proof</title>
          
          <p>
            Note that we<rsq/>re calling this an <q>alleged</q>
            proof. That means that it contains a flaw somewhere. As
            you read it see if you can find where it goes wrong.
          </p>

          <p>
            We already know that
            <me>\underline{\int^b_{x=a}}{f(x)}\dx{x}\le
            \overline{\int^{b}_{x=a}}{f(x)\dx{x}}</me>
          </p>
          <p>
            If we can also show that

            <me>
              \overline{\int^{b}_{x=a}}{f(x)\dx{x}}\le
              \underline{\int^b_{x=a}}{f(x)}\dx{x}
            </me>
            then the conclusion follows immediately.
          </p>
          <!-- <p> -->

          <!--   To show that these are equal, we will show that for any <m>\eps >0</m>, -->

          <!--   <me> -->
          <!--     \overline{\int^{b}_{x=a}}{f(x)\dx{x}}-\underline{\int^b_{x=a}}{f(x)}\dx{x}\lt -->
          <!--     \eps -->
          <!--   </me> -->
          <!-- </p> -->
          <!-- <p> -->

          <!--   Since <m>\eps >0</m> is arbitrary, this will say that -->
          <!--   <me> -->
          <!--     \overline{\int^{b}_{x=a}}{f(x)\dx{x}}-\underline{\int^b_{x=a}}{f(x)}\dx{x}\le -->
          <!--     0 -->
          <!--   </me> -->
          <!--   or -->
          <!--   <m>\overline{\int^{b}_{x=a}}{f(x)\dx{x}}\le -->
          <!--   \underline{\int^b_{x=a}}{f(x)}\dx{x}</m> -->
          <!-- </p> -->
          <p>

            Let <m>\eps \gt 0</m> be given.  Since <m>f\left(x\right)</m> is continuous at each
            point in <m>[a,b]</m>, we can choose a <m>\delta >0</m> such that if
            <men xml:id="EQUATIONUniContFlaw1">
              \left|x-y\right|\lt \delta 
              </men>,
              then
              <men xml:id="EQUATIONUniContFlaw2">
                \left|f\left(x\right)-f\left(y\right)\right|\lt
                \frac{\eps }{b-a} 
                </men>.  
          </p>
          <p>
            Since <m>f</m> is continuous on <m>[a,b]</m> it is
            continuous on each subinterval.  Next define <m>m_k</m>
            and <m>M_k</m> to be the respective minimum and maximum of
            <m>f(x)</m> on the subinterval <m>[x_k, x_{k+1}]</m>.  If
            we choose a partition 
            <me>
              P_0=\{x_0, x_1, x_2,\dots , x_{n-1}, x_n\}
            </me>
            such that <m>\norm{P_0}\lt
            \delta </m>, then on each subinterval <m>[x_k,
            x_{k+1}]</m>, we have
            <me>
              M_k-m_k\lt \frac{\eps }{b-a}
              </me>.
          </p>
          <p>
            Thus 

            <md>
              <mrow>
                \overline{\int^{b}_{x=a}}{f(x)\dx{x}}-\underline{\int^b_{x=a}}{f(x)}\dx{x}\amp=\inf_{P}
                \left(U(P)\right) -\sup_{P} \left(L(P)\right)
              </mrow> 
              <mrow>
                \amp{}     \le U\left(P_0\right)-L(P_0)
              </mrow>
              <mrow>
                \amp{}      =\sum^{n-1}_{k=0}{M_k\left(x_{k+1}-x_k\right)}-\sum^{n-1}_{k=0}{m_k\left(x_{k+1}-x_k\right)}
              </mrow>
              <mrow>
                \amp{}      =\sum^{n-1}_{k=0}{\left(M_k-m_k\right)\left(x_{k+1}-x_k\right)}
              </mrow>
              <mrow>
                \amp{}        \lt \sum^{n-1}_{k=0}{\frac{\eps
                }{b-a}\left(x_{k+1}-x_k\right)}
              </mrow>
              <mrow>
                \amp{}        =\frac{\eps
                }{b-a}\sum^{n-1}_{k=0}{\left(x_{k+1}-x_k\right)}
              </mrow>
              <mrow>
                \amp{}=\frac{\eps }{b-a}\left(b-a\right)
              </mrow>
              <mrow>
                \amp{}=\eps
              </mrow>
            </md>
          </p>
          <p>

            <alert>QED?</alert>
          </p>
        </proof>
        <p>
          Did you find the flaw in the proof? If not, read it
          carefully once more before reading on.
        </p>
        <!-- <p> -->

        <!--   Recall that <xref ref="def_continuity" -->
        <!--   text="custom">continuity</xref> is defined at a point, not on an -->
        <!--   interval. That means that that for each <m>x</m>, -->
        <!--   there is a <m>{\delta }_x>0</m> which guaratees that <xref ref="EQUATIONUniContFlaw2" -->
        <!--   >equation</xref> will be true. It does <em>not</em> mean -->
        <!--   that the same <m>\delta_x</m> will work for all values of -->
        <!--   <m>x</m> in <m>[a,b]</m>.  -->
        <!-- </p> -->
        <p>
          We say alleged proof because there is a subtle problem.  Because
          <m>f(x)</m> is continuous on <m>[a,b]</m>  it is continuous at
          each point <m>x\in [a,b]</m>.  This says that for each <m>x</m>,
          there is a <m>{\delta }_x\gt 0</m>, such that if
          <m>\abs{x-y}\lt\delta_x</m> then
          <m>\left|f\left(x\right)-f\left(y\right)\right|\lt\frac{\epsilon
          }{b-a}</m>. But if you look at our sketch of the alleged proof you see that we need a
          single <m>\delta >0</m> which works <term>uniformly</term> for all
          such <m>x, y\in [a,b]</m>.  This leads to the following definition.

        </p>

        <!-- <p> -->
        <!--   But our alleged proof needs a single <m>\delta >0</m> which -->
        <!--   works uniformly for all such <m>x, y</m>. Since this is not -->
        <!--   guaranteed  <xref ref="EQUATIONUniContFlaw2" -->
        <!--   >equation</xref> is not necessarily true for every <m>x\in -->
        <!--   [a,b]</m>,  and so our proof fails. -->
        <!-- </p> -->
        <!-- <p> -->
        <!--   That observation suggests  the following definition. -->
        <!-- </p> -->

        <definition xml:id="DEFINITIONUnifCont">
          <title>Uniform Continuity</title>

          <idx><h>Uniform Continuity</h></idx>
          <idx><h>Definition</h><h>Uniform Continuity</h></idx>

          <statement>
            <p>
              Suppose <m>S\subset \RR</m>.  We say that <m>f(x)</m> is
              uniformly continuous on <m>S</m> provided that for all <m>\eps
              >0</m>, there is a <m>\delta >0</m> such that
              <m>\left|f\left(x\right)-f\left(y\right)\right|\lt \eps </m> for
              all <m>x, y\in S</m> with <m>\left|x-y\right|\lt \delta </m>.
            </p>

          </statement>
        </definition>
        <p>
          This is called <term>uniform continuity</term> because  a single value of 
          <m>\delta </m>  works uniformly for all <m>x,y\in S</m>, whereas
          in regular continuity,  <m>\delta </m> may depend on the value of
          <m>x</m>.  It is  clear that any function which is uniformly
          continuous on a set <m>S</m> is continuous on <m>S</m>, but   the
          converse is not always true.

          That is,  <term>uniform continuity</term> is a stronger property than <term>continuity</term>.
        </p>

        <problem xml:id="PROBLEMContVSUnifCont">
          <p>
            Consider <m>f(x)=x^2</m> on
            <m>[0,\infty )</m>.  Show that for any <m>\delta >0</m>,
            <me>
              \left(x+\frac{\delta }{2}\right)^2-x^2>1
            </me>
            whenever 
            <me>
              x>\frac{1-\frac{{\delta }^2}{4}}{\delta }
              </me>.
              Explain why this says that <m>f\left(x\right)=x^2</m> is not
              uniformly continuous on <m>[0,\infty )</m>.

          </p>
        </problem>

        <!-- <p> -->
        <!--   It is clear why this is called uniform continuity, since the -->
        <!--   <m>\delta </m> must work uniformly for all <m>x,y\in S</m>, -->
        <!--   whereas in regular continuity, the <m>\delta </m> may depend on -->
        <!--   the value of <m>x</m>.  It is also clear that any function which -->
        <!--   is uniformly continuous on a set <m>S</m> is continuous on -->
        <!--   <m>S</m>.  The converse is not always true. -->
        <!-- </p> -->
        <!-- <problem> -->
        <!--   <p> -->
        <!--     Consider <m>f(x)=x^2</m> on -->
        <!--     <m>[0,\infty )</m>.  Show that for any <m>\delta >0</m>, -->
        <!--     <me> -->
        <!--       \left(x+\frac{\delta }{2}\right)^2-x^2>1 -->
        <!--     </me> -->
        <!--     whenever  -->
        <!--     <me> -->
        <!--       x>\frac{1-\frac{{\delta }^2}{4}}{\delta } -->
        <!--       </me>. -->
        <!--       Explain why this says that <m>f\left(x\right)=x^2</m> is not -->
        <!--       uniformly continuous on <m>[0,\infty )</m>. -->
        <!--   </p> -->
        <!-- </problem> -->

  <p>
    Our <q>alleged proof</q> of <xref ref="THEROEMContImpInt" ></xref>
    is in fact a valid proof that a uniformly continuous function is
    (Darboux) integrable.  But as <xref ref="PROBLEMContVSUnifCont"
    ></xref> points out, a continuous function need not be uniformly
    continuous.  The hypothesis of <xref ref="THEROEMContImpInt"
    ></xref> requires that the function be defined on a closed bounded
    interval, so the difficulty in <xref ref="PROBLEMContVSUnifCont"
    ></xref> is that the interval <m>[0,\infty)</m> while closed, is
    unbounded.  The following lemma closes the gap.

    <!-- Our <xref ref="THEOREMContImpIntProof" text="custom"><q>alleged proof</q></xref>  of <xref ref="THEROEMContImpInt" ></xref> is, in fact, a valid proof that a -->
    <!-- uniformly continuous function is (Darboux) integrable. But since a continuous function is not necessarily uniformly continuous the <q>alleged proof</q> does -->
    <!-- not prove that a merely continuous function is (Darboux) -->
    <!-- integrable. To fix the proof we need to show that  the conditions of <xref ref="THEROEMContImpInt" ></xref> a continuous function is also uniformly continuous. The following lemma will do the trick. -->
  </p>


  <lemma xml:id="LEMMAUnifContImpCont">
    <p>
      If <m>f(x)</m> is continuous on the closed, bounded interval <m>[a,b]</m>, then <m>f(x)</m> is uniformly continuous on <m>[a,b]</m>.
    </p>

  </lemma>

          <proof>
            <title> Sketch of Proof  </title>

            <p>
              We will do a proof by contradiction.  Suppose <m>f(x)</m> is not
              uniformly continuous on <m>[a,b]</m>.  Then there is an <m>\eps
              >0</m> such that for any <m>\delta >0</m>, there are <m>x,y</m>
              with <m>\left|x-y\right|\lt \delta </m> , but
              <m>\left|f\left(x\right)-f\left(y\right)\right|\ge \eps </m>.  If
              we let <m>\delta =\frac{1}{n}, n\in \mathbb{N}</m>, then we can
              create two sequences <m>\left(x_n\right), (y_n)</m> with
              <m>\left|x_n-y_n\right|\lt \frac{1}{n}</m>, but
              <m>\left|f\left(x_n\right)-f\left(y_n\right)\right|\ge \eps .</m>
              By the Bolzano<ndash/>Weierstrass Theorem, there is a <m>c\in [a,b]</m>
              and a subsequence <m>(x_{n_k})</m> with
              <m>\limit{k}{\infty }{ x_{n_k} }=c</m>.  Given
              how <m>(y_n)</m> was constructed, <m>\limit{k}{\infty }{ y_{n_k} }=c</m>.  Since <m>f(x)</m> is continuous at
              <m>c</m>, you should be able to get a contradiction out of this.
            </p>

          </proof>

          <problem>
            <p>
              Turn the above outline into a proof of <xref
              ref="LEMMAUnifContImpCont"></xref>.
            </p>
          </problem>
          <problem>
            <p>
              Prove <xref ref="THEROEMContImpInt"></xref>.
            </p>
          </problem>




          <p>
            The evolution of the modern definition of a function is
            parallel to, and intertwined with, the definition of the
            definite integral.  Issues of integrability were not
            prevalent in 18th century because for most of that time
            the words <q>integral</q> and <q>antiderivative</q> were
            synonymous. Thus the only <q>integrable</q> functions were
            the ones that were derivatives of some other function. But
            in the 19th century, and especially after Fourier, we
            needed to integrate functions that were not clearly
            derivatives of something else. As a result the need for a
            precise definition of function became more and more
            pressing as the years went by.  For example, here is the
            definition of a function from Euler<rsq/>s
            <pubtitle>
              <foreign>Introductio in Analysin Infinitorum</foreign>
            </pubtitle>
            (1748).
          </p>

          <blockquote>
            <p>
              <q>A function of a variable quantity is an analytic
              expression composed in any way whatsoever of the variable quantity
              and numbers or constant quantities.</q>
            </p>
          </blockquote>

          <p>
            This is the <q>function as input/output machine</q>
            metaphor that you<rsq/>ve probably used all of your life
            so far: A number goes in, gears turn or electronic
            circuits are activated and a new number is generatied as
            output from those actions. The advent of Fourier series
            ushered in a need for a much more general definition.
            Here is Fourier<rsq/>s definition from his
            <pubtitle>
              <foreign>Thorie analytique de la Chaleur</foreign>
            </pubtitle>
            (1822).
          </p>

          <blockquote>
            <p>
              <q>In general, the function <m>f(x)</m> represents a
              succession of values or ordinates each of which is arbitrary. An
              infinity of values being given of the abscissa <m>x</m>,
              there are an equal number of ordinates<m>f(x)</m>. All
              have actual numerical values, either positive or negative or
              nul. We do not suppose these ordinates to be subject to a common
              law; they succeed each other in any manner whatever, and each of
              them is given as it were a single quantity.</q>
            </p>
          </blockquote>

          <p>
            This is closer to our modern approach. In the modern
            definition for each <m>x</m> in the domain, there is a
            unique <m>f(x)</m> assigned to it.  It is different from
            Euler<rsq/>s definition in that no formula, and no
            metaphorical machine is needed to generate an output. A
            function can be defined by simply giving a list of ordered
            pairs: (input, output). No other particular rule is needed.
          </p>
          <p>



            As you can see, making the idea of an integral rigorous was a
            delicate matter.  Perhaps this is why it took so long to develop.
          </p>

        </subsection>

        <!-- <subsection xml:id="SUBSECTIONStieltjesIntegral"> -->
        <!--   <title>The Stieltjes Integral</title> -->

        <!--   <figure  xml:id="FIGUREStieltjesPortrait"> -->
        <!--     <caption>Thomas Jan Stieltjes</caption> -->
        <!--     <image source="images/Stieltjes.png" width="35%"> -->
        <!--       <shortdescription>Portrait of Thomas Jan Stieltjes</shortdescription> -->
        <!--     </image> -->
        <!--   </figure> -->

        <!--   <p> -->
        <!--     The various integral definitions we<rsq/>ve seen so far have -->
        <!--     all been equivalent in the sense that the value of the -->
        <!--     integral is the same regardless of which definition is -->
        <!--     used. This is not true of the Stieltjes integral. -->
        <!--   </p> -->
        <!--   <p> -->
        <!--     To begin to understand the Stieltjes integral consider the -->
        <!--     Riemann integral:  -->
        <!--     <me> -->
        <!--       \int_{x=1}^2x^2(2x) \dx{x} -->
        <!--       </me>.  In your -->
        <!--     Calculus you learned how to evaluate this integral as -->
        <!--     follows. First we let <m>\alpha(x) = x^2</m> so that -->
        <!--     <m>\dx{\alpha}=2x\dx{x}</m>, <m>\alpha (1)=1</m>, and -->
        <!--     <m>\alpha (2)=4</m>, so that  -->
        <!--     <me> -->
        <!--       \int_{x=1}^1x^2(2x) \dx{x} = -->
        <!--       \int_{ \alpha =1}^4  \alpha \dx{\alpha }=3 -->
        <!--       </me>.   -->
        <!--   </p> -->
        <!-- </subsection> -->
    </section>


<!-- <section xml:id="Continuity-AddProb"> -->
<!--   <title>Additional Problems</title> -->
<!--   <problem> -->
<!--     <idx><h>continuous functions</h><h>a constant function is continuous</h></idx> -->

<!--     <statement> -->
<!--       <p> -->
<!--         Use the definition of continuity to prove that the constant -->
<!--         function <m>g(x)=c</m> is continuous at any point <m>a</m>. -->
<!--       </p> -->
<!--     </statement> -->
<!--   </problem> -->

<!--   <problem> -->
<!--     <idx><h>continuous functions</h><h><m>\ln x</m> is continuous everywhere</h></idx> -->
<!--     <task> -->
<!--       <statement> -->
<!--         <p> -->
<!--           Use the definition of continuity to prove that <m>\ln x</m> -->
<!--           is continuous at <m>1</m>. -->
<!--         </p> -->
<!--       </statement> -->
<!--       <hint> -->
<!--         <p> -->
<!--           You may want to use the fact <m>\abs{\ln x}\lt -->
<!--           \eps\,\Leftrightarrow-\eps\lt \ln x\lt \eps</m> to find a -->
<!--           <m>\delta</m>. -->
<!--         </p> -->
<!-- </hint> -->
<!-- </task> -->
<!-- <task> -->
<!--   <statement> -->
<!--     <p> -->
<!--       Use part (a) to prove that <m>\ln x</m> is continuous at any -->
<!--       positive real number <m>a</m>. -->
<!--     </p> -->
<!-- </statement> -->
<!-- <hint> -->
<!--   <p> -->
<!--     <m>\ln(x)=\ln(x/a)+\ln(a)</m>.  This is a combination of functions -->
<!--     which are continuous at <m>a</m>.  Be sure to explain how you know -->
<!--     that <m>\ln(x/a)</m> is continuous at <m>a</m>. -->
<!--   </p> -->
<!-- </hint> -->
<!-- </task> -->
<!-- </problem> -->

<!-- <problem> -->
<!--   <statement> -->
<!--     <p> -->
<!-- <idx><h>continuity</h><h>formal definition of discontinuity</h></idx> -->

<!-- Write a formal definition of the statement <m>f</m> is not continuous -->
<!-- at <m>a</m>, and use it to prove that the function -->
<!-- <me> -->
<!--   f(x)=  -->
<!--   \begin{cases} -->
<!--   x\amp \text{ if } x\neq 1\\ -->
<!--   0\amp \text{if } x=1  -->
<!--   \end{cases} -->
<!-- </me> -->
<!-- is not continuous at <m>a=1</m>. -->
<!--     </p> -->
<!-- </statement> -->
<!-- </problem> -->
<!-- </section> -->


</chapter>

