<chapter xmlns:xi="http://www.w3.org/2001/XInclude"  xml:id="PowerSeriesQuestions">
  <title>Questions Concerning Power Series</title>


  <section  xml:id="PowerSeriesQuestions-TaylorsFormula">
    <title>Taylor<rsq/>s Formula</title>
    <p>
      As we saw in <xref ref="SECTIONPowSerWOTaylor"></xref>,
      representing functions as power series was a fruitful strategy for
      mathematicans in the eighteenth century (as it still is).
      Differentiating and integrating power series term by term was
      relatively easy, <em>seemed</em> to work, and led to many
      applications.  Furthermore, power series representations for all
      of the elementary functions could be obtained if one was clever
      enough. However, cleverness is an unreliable tool.  It would be
      better to have some systematic way to find a power series for a
      given function that doesn<rsq/>t rely on being sufficiently cleve.
    </p>

    <p>
      To be sure, there were nagging questions. For example. even if we
      can find a power series representation of some function, how do we
      know that the series we<rsq/>ve created represents the function we
      started with?  Even worse, is it possible for a function to have
      more than one power series representation centered at a given
      value <m>a?</m> This uniqueness issue is addressed by the
      following theorem.
    </p>

    <!-- <theorem xml:id="TaylorSeriesThm"> -->
    <theorem xml:id="THEOREMTaylorPoly">
      <title>Taylor<rsq/>s Formula</title>

      <idx><h>Taylor<rsq/>s Formula</h></idx>

      <statement>
        <p>
          If <m>f(x)=\sum_{n=0}^\infty a_n(x-a)^n</m>,
          then <m>a_n=\frac{f^{(n)}(a)}{n!}</m>,
          where <m>f^{(n)}(a)</m> represents the <m>n</m>th derivative of <m>f</m> evaluated at <m>a</m>.
        </p>
      </statement>
    </theorem>

    <p>
      A few comments about <xref ref="THEOREMTaylorPoly"></xref> are in order.  Notice
      that we did <em>not</em> start with a function and derive its
      series representation.  Instead we <em>defined</em> <m>f(x)</m> to
      be the series we wrote down.  This assumes that the expression
      <m>\sum_{n=0}^\infty a_n(x-a)^n</m> actually has meaning (that it
      converges).  At this point we have every reason to expect that it
      does, however expectation is not proof so we note that this is an
      assumption, not an established truth.  We<rsq/>ve also assumed
      that we can differentiate an infinite polynomial term-by-term as
      we would a finite polynomial.  As before, we follow in the
      footsteps of our 18th century forebears in making these
      assumptions.  For now.
    </p>

    <problem>
      <statement>
        <p>
          <idx><h>Taylor<rsq/>s Formula</h></idx>
          Prove <xref ref="THEOREMTaylorPoly"></xref>. 
        </p>
      </statement>
      <hint>
        <p>
          The <q>zeroth</q> derivative (the function itself) at <m>a</m> is given by
          <me>
            f(a)=a_0+a_1(a-a)+a_2(a-a)^2+\cdots=a_0
            </me>. 
            Differentiate to obtain the other terms.
        </p>
      </hint>
    </problem>

    <p>
      From <xref ref="THEOREMTaylorPoly"></xref> we see that if we do
      start with the function <m>f(x)</m> then no matter how we obtain
      its power series, the result will always be the same.  The series

      <mdn>
        <mrow number="no">
          \sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n=f(a)+\amp{}f^\prime(a)(x-a)+\frac{f^{\prime\prime}(a)}{2!}(x-a)^2
        </mrow>
        <mrow xml:id="TaylorsSeries">
          \amp{}+\frac{f^{\prime\prime\prime}(a)}{3!}(x-a)^3+\cdots
        </mrow>
      </mdn>
    </p>

    <p>
      is called the <em><alert>Taylor series</alert> for <m>f</m>
      expanded about (or centered at) <m>a</m></em>.  Although this
      systematic <q>machine</q> for obtaining power series for a
      function seems to have been known to a number of mathematicians in
      the early 1700s, <url
      href="https://mathshistory.st-andrews.ac.uk/Biographies/Taylor/"
      visual="mathshistory.st-andrews.ac.uk/Biographies/Taylor/">Brook
      Taylor</url> (1685<ndash/>1731) was the first to publish this
      result in his <foreign>Methodus Incrementorum</foreign> (1715).
      The special case when <m>a=0</m> was included by 
      <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Maclaurin/" visual="mathshistory.st-andrews.ac.uk/Biographies/Maclaurin/">Colin Maclaurin</url>
      (1698<ndash/>1746) in his <em>Treatise of Fluxions</em> (1742).
      Thus when <m>a=0</m>, the series
      <m>\sum_{n=0}^\infty\frac{f^{(n)}(0)}{n!}x^n</m> is often called
      the <term>Maclaurin Series</term> for <m>f</m>.
    </p>

    <figure>
      <caption> A postage stamp from the Netherlands honoring Brook  Taylor and Colin Maclaurin</caption>

      <idx><h>Taylor, Brook</h><h>portrait of</h></idx>
      <idx><h>Maclaurin, Colin</h><h>portrait of</h></idx>
      <idx><h>Portraits</h><h>Taylor</h></idx>
      <idx><h>Portraits</h><h>Maclaurin</h></idx>

      <image width="45%" source="images/TaylorAndMaclaurin.png" >
        <shortdescription>A postage stamp with images of Taylor and Maclaurin and their formulas.</shortdescription>
      </image>
    </figure>

    <problem>
      <idx><h>Taylor<rsq/>s Formula</h><h>drill problems </h></idx>
      <introduction>
        <p>
          Use Taylor<rsq/>s formula to find the Taylor series of the
          given function expanded about the given point <m>a</m>.
        </p>
      </introduction>
      <task>
        <statement>
          <p>
            <m>f(x)=\ln\left(1+x\right)</m>, <m>a=0</m>
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            <m>f(x)=e^x</m>, <m>a=-1</m>
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            <m>f(x)=\sin(x) </m>, <m>a=\frac{\pi }{2}</m>
          </p>
        </statement>
      </task>

      <task>
        <statement>
          <p>
            <m>f(x)=x^3+x^2+x+1</m>, <m>a=0</m>
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            <m>f(x)=x^3+x^2+x+1</m>, <m>a=1</m>
          </p>
        </statement>
      </task>
    </problem>



    <p>
      The <term>prime notation</term> for the derivative was not used by
      Taylor, Maclaurin or their contemporaries.  It was introduced by
      Joseph Louis Lagrange in his 1779 work <foreign>Thèorie des
      Fonctions Analytiques</foreign>. In that work, Lagrange sought to
      get rid of Leibniz<rsq/> infinitesimals and base Calculus on the
      power series idea.  His idea was that by representing every
      function as a power series, Calculus could be done
      algebraically by manipulating power series and examining
      various aspects of the series representation instead of appealing
      to the controversial notion of infinitesimals.  He
      implicitly assumed that every continuous function could be
      replaced with its power series representation.
    </p>


    <figure>
      <caption><url href="https://mathshistory.st-andrews.ac.uk/Biographies/Lagrange/" visual="mathshistory.st-andrews.ac.uk/Biographies/Lagrange/">Joseph-Louis Lagrange</url></caption>

      <idx><h>Lagrange, Joseph-Louis</h><h>portrait of</h></idx>
      <idx><h>Portraits</h><h>Lagrange</h></idx>

      <image width="35%" source="images/Lagrange.png" >
        <shortdescription></shortdescription>
      </image>
    </figure>

    <p>
      That is, he wanted to think of the Taylor series as a
      <q>great big polynomial,</q>
      because polynomials are easy to work with.
      It was a very simple, yet exceedingly clever and far-reaching idea.
      Since <m>e^x = 1 +x +x^2/2 +\ldots</m>, for example,
      why not just define the exponential to be the series and work with the series.
      After all, the series is just a very long polynomial.
    </p>

    <p>
      This idea did not come out of nowhere. Particular infinite series,
      such as the <xref ref="EQUATIONGeometricSeries"
      text="custom">Geometric Series</xref> had been known and studied
      for many years.  Later, in the 18th century Leonhard Euler used
      infinite series to solve many problems, and some of his solutions
      are still quite breath<ndash/>taking when you first see them<nbsp
      /><xref ref="sandifer07__early_mathem_leonar_euler" />.
    </p>




    <p>
      Taking his cue from the Taylor series
      <men xml:id="EQUATIONTaylorSeriesGeneric">
        f(x) = \sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n
      </men>
      Lagrange observed that the coefficient of <m>(x-a)^n</m> provides
      the <m>n</m>th derivative of <m>f</m> at <m>a</m> (divided by <m>n!</m>).
      Modifying <xref ref="EQUATIONTaylorSeriesGeneric" >formula</xref>
      to suit his purpose, Lagrange supposed that every differentiable
      function could be represented as
      <me>
        f(x) = \sum_{n=0}^\infty g_n(a)(x-a)^n
        </me>.
    </p>

    <p>
      In that case <m>g_1(a)</m> is the derivative of <m>f</m> at
      <m>a</m>, <m>f^{\prime\prime}(a)=2g_2(a)</m> and generally
      <me>
        f^{(n)}(a)=n! g_n(a)
        </me>.
    </p>

    <p>
      Lagrange dubbed his function <m>g_1</m> the
      <foreign><q>fonction dérivée</q></foreign>
      from which we get the modern name <term>derivative</term>.
    </p>

    <problem>

      <introduction>

        <p>

          Let <m>a\neq 0</m> be a fixed number.
          We saw in <xref ref="PROBLEMOneOverXTayl" ></xref>  that the power series of <m>\frac{1}{x}</m> expanded
          about <m>a</m> is given by 
          <me>
            \sum^{\infty}_{n=0}{\frac{{\left(-1\right)}^n}{a^{n+1}}{\left(x-a\right)}^n}
            =\frac{1}{a}-\frac{1}{a^2}\left(x-a\right)
            +\frac{1}{a^3}{\left(x-a\right)}^2
            -\frac{1}{a^4}{\left(x-a\right)}^3+\dots
          </me>
        </p>
      </introduction>
      <!--   </statement> -->
      <!--   <hint> -->
      <!--     <p> -->
      <!--       Write  <m>f\left(x\right)=\frac{1}{x}</m>  as  -->
      <!--       <me> -->
      <!--         f(x)=\frac1x=\frac{1}{a+x-a}=\frac{1}{a}\left(\frac{1}{1+\frac{x-a}{a}}\right) -->
      <!--       </me>  -->
      <!--       and use the <xref ref="EQUATIONGeometricSeries" text="custom">Geometric Series</xref>. -->
      <!--     </p> -->
      <!--   </hint> -->
      <!-- </task> -->
      <!-- <task> -->
      <!--   <statement> -->
      <task>
        <statement>

          <p>
            Apply Lagrange's idea to show that
            <m>f^{\left(n\right)}\left(a\right)=\frac{{\left(-1\right)}^nn!}{a^{n+1}}</m>.
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Now compute <m>f^{\left(n\right)}\left(a\right)</m> by
            directly by differentiating.  Do you get the same result?
          </p>
        </statement>
      </task>
    </problem>
    
    <problem>
      <idx><h><m>e^x</m></h><h>Taylor<rsq/>s series for</h></idx>
      <idx><h>series</h><h>Taylor<rsq/>s series</h><h>expansion of <m>e^x, \sin x</m>, and <m>\cos x</m></h></idx>
      <idx><h><m>\sin x</m></h><h>Taylor<rsq/>s series for</h></idx>
      <idx><h><m>\cos x</m></h><h>Taylor<rsq/>s series for</h></idx>
      <task>
        <statement>
          <p>
            In <xref ref="PROBLEMSinExpandedAta" ></xref>, you determined the power series
            expansion

            <md>
              <mrow>
                \sin (x)=\sin (a)+\amp{}\cos\left(a\right)\left(x-a\right)
              </mrow>
              <mrow>
                \amp{}-\frac{1}{2!}\sin
                \left(a\right){\left(x-a\right)}^2
              </mrow>
              <mrow>
                \amp{}\ \ \ -\frac{1}{3!}\cos
                \left(a\right){\left(x-a\right)}^3
              </mrow>
              <mrow>
                \amp{}\ \ \ \ \ \ +\frac{1}{4!}\sin
                \left(a\right){\left(x-a\right)}^4
              </mrow>
              <mrow>
                \amp{}\ \ \ \ \ \ \ \ \  +\frac{1}{5!}\cos
                \left(a\right){\left(x-a\right)}^5+\cdots.
              </mrow>
            </md>
          </p>
          <p>
            Apply Lagrange<rsq/>s idea to determine the <m>n</m>th
            derivative of <m>\sin x</m>. Compare with the results you get
            from differentiating <m>\sin x</m> directly.
          </p>
        </statement>
      </task>
      <!-- <task> -->
      <!--   <statement> -->
      <!--     <p> -->
      <!--       Compute the Taylor series of <m>\cos x</m> expanded about -->
      <!--       <m>a</m>. Then use Lagrange<rsq/>s idea to determine the -->
      <!--       <m>n</m>th derivative of <m>\sin x</m>. Compare with the -->
      <!--       results you get from differentiating <m>\sin x</m> directly. -->
      <!--     </p> -->
      <!--     </statement> -->
      <!-- </task> -->
      <task>
        <statement>
          <p>
            In <xref ref="PROBLEMExpTaylAta"></xref> you
            determined the power series expansion of <m>e^x</m> about
            <m>a</m>.  Apply Lagrange's idea to show that every derivative
            of <m>e^x</m> is given by <m>e^x</m>.

          </p>
        </statement>
      </task>
      <task>
        <statement>


          <p>
            In part (b) of <xref ref="PROBLEMOneOverXTayl" ></xref> you
            determined the power series expansion of <m>\ln(x)</m>
            about <m>a>0</m>. Apply Lagrange's idea to show that
            the derivative of <m>\ln (x)</m> is <m>\frac{1}{x}</m>.

          </p>
        </statement>
      </task>


    </problem>


    <p>

      As we observed in <xref
      ref="CalcIn17th18thCentury-NewtLeibStart" ></xref> Leibniz and
      his peers would have regarded the expression <m>\dfdx{z}{x}
      </m> as a fraction (of differentials) so that the formula
      
      <men xml:id="EQUATIONChainRuleLeibniz">
        \dfdx{z}{y}\dfdx{y}{x}=\dfdx{z}{x}
      </men> 

      could be obtained by simply canceling the <m>\dx{y}</m> that appears
      in the numerator and denominator, just as we did in <xref
      ref="CalcIn17th18thCentury-NewtLeibStart" ></xref>.
      Mathematicians of that era would have regarded this operation as
      basic algebra.
    </p>
    <p>
      Eighteenth and nineteenth century arithmetic primers used the
      phrase <q>chain rule</q> to describe the daisy<ndash/>chain of
      symbolic cancellations that occurs when we convert
      units. For example, to convert yards to inches we compute
      <me>
        \left(2\cancel{\text{ yards}}\right)\left(3\frac{\cancel{\text{feet}}}{\cancel{\text{yard}}}\right)\left(12\frac{\text{inches}}{\cancel{\text{foot}}}\right)=72\text{ inches}
        </me>.
    </p>
<p>
  Since  the cancellations in  <xref ref="EQUATIONChainRuleLeibniz" >equation</xref> appeared to be just  another example of the older <q>chain rule</q> the name was adopted in this context as well. 
</p>

    <p>
      Letting <m>z=f(y)</m> and <m>y=g(x)</m> and 
      using Lagrange<rsq/>s  prime notation we see that in modern form the chain rule for Calculus is:

      <me>\dfdx{z}{x}=\underbrace{f^\prime\left(g\left(x\right)\right)}_{\dfdx{z}{y}}\underbrace{g^\prime(x)}_{\dfdx{y}{x}{}}=\left(f\circ
        g\right)^\prime\left(x\right)</me> 

        In his <foreign>Théorie des  fonctions analytiques</foreign> (1797)
        Lagrange  provided a derivation of the Chain Rule in these more 
        modern terms.   The following problem captures his idea. 


    </p>
     
    <problem>
      <introduction>
        <p>

        </p>
      </introduction>
      <task>
        <statement>
          <p>
            Expand <m>y=g(x)</m> as a power series expanded about <m>a</m>.
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Expand <m>f(y)</m> as a power series expanded about <m>y=g(a)</m> and substitute your answer from part (a) for <m>y</m>.
          </p>
        </statement>
      </task>
      <task>
        <statement>
          <p>
            Show that the coefficient of the linear term <m>(x-a)</m> in part (b) is given by
            <me>f^\prime\left(g\left(a\right)\right)g^\prime(a)</me> 
          </p>
        </statement>
      </task>

    </problem>



    <p>
      All in all, Lagrange<rsq/>s idea was very clever and
      insightful. It<rsq/>s only real flaw is that its fundamental,
      underlying assumption is not true.  It turns out that not every
      differentiable function can be represented as a Taylor series.
      This was demonstrated very dramatically by Augustin
      Cauchy<rsq/>s famous counter-example
      <men xml:id="eq_CauchyCounterEx">
        f(x) = \begin{cases} e^{-\frac{1}{x^2}}\amp  x\ne0\\ 0 \amp x=0 \end{cases}
        </men>.
    </p>

    <p> 
      This function is actually infinitely differentiable everywhere
      but its Maclaurin series (that is, its Taylor series with
      <m>a=0</m>) does not converge to <m>f</m> (except, trivially, at
      the origin) because all of its derivatives at the origin are
      equal to zero. That is <m>f^{(n)}(0) = 0, \forall\, n \in
      \NN</m>.
    </p>


    <p>
      To begin with, let<rsq/>s compute a few derivatives when <m>x
      \neq 0</m>.
      <md>
        <mrow>f^{(0)}(x) \amp = e^{x^{-2}}</mrow>
        <mrow>f^{(1)}(x) \amp = 2x^{-3}e^{-x^{-2}}</mrow>
        <mrow>f^{(2)}(x) \amp = \left(4x^{-6}-6x^{-4}\right)e^{-x^{-2}}</mrow>
        </md>.
    </p>

    <p>
      As you can see the calculations are already getting a little complicated and we<rsq/>ve only taken the second derivative.
    </p>
    <p>
      Conceptually, it is not difficult to compute these derivatives
      using the tools you learned in Calculus but the formulas involved
      do become complicated rather quickly.  Some care must be taken to
      avoid error.

      To streamline things a bit we take <m>y= x^{-1}</m>,
      and define <m>p_2(x) = 4x^6-6x^4</m> so that
      <me>
        f^{(2)}(x) = p_2(x^{-1})e^{-x^{-2}} = p_2(y)e^{-y^2}
        </me>.
    </p>

    <problem xml:id="DRILLCauchyCounterPartI"><title>Cauchy<rsq/>s Counterexample, Part 1</title>
    <idx><h>Cauchy, Augustin</h><h>Cauchy<rsq/>s counterexample</h><h> part 1</h></idx>
    <introduction>
      <p>
        In this problem and in <xref
        ref="PROBLEMCauchyCounterPartII"></xref> <m>f(x)</m> is the function defined in <xref ref="eq_CauchyCounterEx" >equation</xref>. 
      </p>
      <p>

        To simplify the notation we make the following definitions:
        <m>y=x^{-1}</m> and <m>f^{(n)}(x) =p_n(y)e^{-y^2}</m>.

      </p>
    </introduction>

    <task>
      <statement>
        <p>
          Find <m>p_{n+1}(y)</m> in terms of
          <m>p_{n}(y)</m>. 
        </p>
      </statement>
      <hint>
        <p>
          You are differentiating with respect to <m>x</m>, not <m>y</m>.
        </p>
      </hint>
    </task>
    <task>
      <statement>
        <p>
          Use induction on <m>n</m> to show that <m>p_n(y)</m> is a polynomial for all <m>n\in\NN</m>.
        </p>
      </statement>
    </task>
    </problem>

    <p>
      Unfortunately everything we<rsq/>ve done so far only gives us the derivatives we need when <m>x</m> is <em>not</em> zero,
      and we need the derivatives when <m>x</m> <em>is</em> zero.
      To find these we need to get back to very basic ideas.
    </p>

    <p>
      Let<rsq/>s assume for the moment that we know that <m>f^{(n)}(0)=0</m> and recall that
      <md>
        <mrow>f^{(n+1)}(0) \amp = \limit{x}{0}{\frac{f^{(n)}(x)-f^{(n)}(0)}{x-0}}</mrow>
        <mrow>f^{(n+1)}(0) \amp = \limit{x}{0}{x^{-1}p_n(x^{-1})e^{-x^{-2}}}</mrow>
        <mrow>f^{(n+1)}(0) \amp = \limit{y}{\pm\infty}{\frac{yp_n(y)}{e^{y^2}}}</mrow>
        </md>.
    </p>

    <p>
      We can close the deal with the following problem.
    </p>

    <problem xml:id="PROBLEMCauchyCounterPartII"><title>Cauchy<rsq/>s Counterexample, Part 2</title>
    <idx><h>Cauchy, Augustin</h><h>Cauchy<rsq/>s counterexample</h><h>part 2</h></idx>

    <task>
      <statement>
        <p>
          Let <m>m</m> be a nonnegative integer.
          Show that <m>\limit{y}{\pm\infty}{\frac{y^m}{e^{y^2}}}=0</m>. 
        </p>
      </statement>
      <hint>
        <p>
          Induction and a dash of L<rsq/>H&#xf4;pital's rule should do the trick.
        </p>
      </hint>
    </task>
    <task>
      <statement>
        <p>
          Prove that
          <m>\limit{y}{\pm\infty}{\frac{q(y)}{e^{y^2}}}=0</m> for any
          polynomial <m>q</m>.
        </p>
      </statement>
    </task>
    <task>
      <statement>
        <p>
          Let <m>f(x)</m> be as in <xref ref="eq_CauchyCounterEx">equation</xref>
          and show that for every nonnegative integer <m>n</m>, <m>f^{(n)}(0)=0</m>.
        </p>
      </statement>
    </task>
    </problem>

    <p>
      This example showed that while it was fruitful to exploit Taylor
      series representations of various functions, basing the
      foundations of Calculus on power series was not a sound idea.
    </p>

    <p>
      While Lagrange<rsq/>s approach wasn't totally successful, it was
      a major step away from infinitesimals and toward the modern
      approach.  We still use aspects of it today.  For instance we
      still use his prime notation to denote the derivative.
    </p>

    <p>
      Turning Lagrange<rsq/>s idea on its head it is clear that if we
      know how to compute derivatives, we can use this <q>machine</q>
      to obtain a power series when we are not clever enough to obtain
      the series by other (typically shorter) means.  For example,
      consider Newton<rsq/>s binomial series when
      <m>\alpha=\frac{1}{2}</m>.  Originally, we obtained this series
      by extending the binomial theorem to non-integer exponents.
      Taylor<rsq/>s formula provides a more systematic procedure. First we compute the derivatives of <m>f</m> at zero:
      <md>
        <mrow>f(x)\amp =(1+x)^{\frac{1}{2}};\amp f(0)\amp =1</mrow>
        <mrow>f^\prime(x)\amp =\frac{1}{2}(1+x)^{\frac{1}{2}-1};\amp  f^\prime(0)\amp =\frac{1}{2}</mrow>
        <mrow>f^{\prime\prime}(x)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)(1+x)^{\frac{1}{2}-2}\amp f^{\prime\prime}(0)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)</mrow>
      </md>
      and in general since
      <md>
        <mrow>f^{(n)}(x)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-(n-1)\right)(1+x)^{\frac{1}{2}-n}</mrow>
        <intertext>we have</intertext>
        <mrow>f^{(n)}(0)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-(n-1)\right)</mrow>
        </md>.
    </p>

    <p>
      Next we substitute  <m>f^{(n)}(0)</m> into  Taylor<rsq/>s formula to obtain the series

      <md>
        <mrow>
          \sum_{n=0}^\infty\frac{f^{(n)}(0)}{n!}x^n \amp{}= 1+\sum_{n=1}^\infty\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\cdots\left( \frac{1}{2}-(n-1)\right)}{n!}x^n
        </mrow>
        <mrow>
          \amp{}= 1+\sum_{n=1}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n
        </mrow>

      </md>
      which agrees with <xref ref="eq_BinomialSeries">equation</xref> in the previous chapter.
    </p>

    <problem>
      <statement>
        <p>
          <idx><h>Taylor<rsq/>s Formula</h><h>use to obtain the general binomial series</h></idx>
          Use Taylor<rsq/>s formula to obtain the general binomial series
          <me>
            (1+x)^\alpha=1+\sum_{n=1}^\infty\frac{\prod_{j=0}^{n-1}\left(\alpha-j\right)}{n!}x^n.{}
          </me>
        </p>
      </statement>
    </problem>


    <p>
      As you can see, Taylor<rsq/>s <q>machine</q> will produce the
      power series for a function (if it has one), but is tedious to
      perform.  We will find, generally, that this tediousness can be
      an obstacle to understanding.  In many cases it will be better,
      or at least quicker, to be clever if we can.  However, it is
      comforting to have Taylor<rsq/>s formula available as a last
      resort.
    </p>

    <p>
      The existence of a Taylor series is addressed (to some degree)
      by the following.
    </p>

    <theorem xml:id="TaylorsTheorem">
      <title>Taylor<rsq/>s Theorem</title>

      <idx><h>Taylor<rsq/>s Theorem</h></idx>

      <statement>
        <p>
          If <m>f^\prime, f^{\prime\prime}, \ldots, f^{(n+1)}</m> are all continuous on an interval containing <m>a</m> and <m>x</m>, then
          <md>
            <mrow>f(x)=f(a)+\amp{}\frac{f^{\prime}(a)}{1!}(x-a)+\frac{f^{\prime \prime}(a)}{2!}(x-a)^2 +</mrow>
            <mrow>
            \amp{}\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n + \frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{t}</mrow>
            </md>.
            The last term in this expression,
            <me>\frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{t}</me>,
            is called the <term>Integral
            Form of the Remainder</term> of the Taylor series.
        </p>
      </statement>
    </theorem>

    <p>
      Before we address the proof, notice that the <m>n</m>th degree polynomial
      <men xml:id="EQUATIONNthDegTaylPoly">
        f(a)+\frac{f^{\prime}(a)}{1!}(x-a)+\frac{f^{\,\prime\prime}(a)}{2!}(x-a)^2+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n
      </men>
      resembles the Taylor series and, in fact, is called the <term>
      <m>\boldsymbol n</m>th degree Taylor polynomial of <m>f</m>
      about <m>\boldsymbol a</m></term>.  <xref
      ref="TaylorsTheorem"></xref> says that a function can be written
      as the sum of this polynomial and a specific integral which we
      will analyze in <xref ref="TaylorSeries" ></xref> where we will
      get the proof started and leave the formal induction proof as an
      exercise.
    </p>

    <p>
      Notice that when <m>n=0</m> <xref
      ref="EQUATIONNthDegTaylPoly">equation</xref> is really a
      restatement of the <term>Fundamental Theorem of Calculus</term>:
      <me>
        \int_{t=a}^xf^\prime(t)\dx{t}=f(x)-f(a)
        </me>.
        which we 
        rewrite  as
        <me>
          f(x)=f(a)+\frac{1}{0!}\int_{t=a}^xf^\prime(t)(x-t)^0\dx{t}
        </me>
        in order to provide the anchor step for our induction.
    </p>

    <p>
      To derive the case where <m>n=1</m>,
      we use integration by parts.
      If we let
      <md>
        <mrow>u\amp =f^\prime(t)\amp  d v\amp =(x-t)^0d t</mrow>
        <mrow>d u\amp =f^{\prime\prime}(t)d t\amp  v\amp =-\frac{1}{1}(x-t)^1</mrow>
      </md>
      we obtain
      <md>
        <mrow>f(x)\amp =f(a)+\frac{1}{0!}\left(-\frac{1}{1}f^\prime(t)(x-t)^1|_{t=a}^{^x}+\frac{1}{1} \int_{t=a}^xf^{\prime\prime}(t)(x-t)^1\dx{ t}\right)</mrow>
        <mrow>\amp =f(a)+\frac{1}{0!}\left(-\frac{1}{1}f^\prime(x)(x-x)^1+ \frac{1}{1}f^\prime(a)(x-a)^1+\frac{1}{1}\int_{t=a}^xf^{\prime\prime}(t)(x-t)^1\dx{ t}\right)</mrow>
        <mrow>\amp =f(a)+\frac{1}{1!}f^\prime(a)\left(x-a\right)^1 + \frac{1}{1!}\int_{t=a}^xf^{\prime\prime}(t)(x-t)^1\dx{ t}</mrow>
        </md>.
    </p>

    <problem>
      <statement>
        <p>
          <idx><h>Taylor<rsq/>s Theorem</h></idx>
          Provide a formal induction proof for <xref ref="TaylorsTheorem"></xref>.
        </p>
      </statement>
    </problem>
  </section>



  <section  xml:id="PowerSeriesQuestions-SeriesAnomalies">
    <title>Series Anomalies</title>
    <p>
      Up to this point, we have been somewhat frivolous in our approach
      to series.  This approach mirrors the approach of the eighteenth
      century mathematicians who ingeniously exploited Calculus and
      series to provide mathematical and physical results which were
      virtually unobtainable before.  Mathematicans were eager to push
      these techniques as far as they could to obtain their results and
      they often showed good intuition regarding what was mathematically
      acceptable and what was not.  However, as the envelope was pushed
      further and further, substantial questions about the validity of
      their methods surfaced.
    </p>

    <p>
      As an illustration consider the series expansion
      <me>
        \frac{1}{1+x}=1-x+x^2-x^3+\cdots
        </me>.
    </p>

    <p>
      If we substitute <m>x=1</m> into this equation, we obtain
      <me>
        \frac{1}{2}=1-1+1-1+\cdots
        </me>.
    </p>

    <p>
      If we group the terms as follows <m>(1-1)+(1-1)+\cdots</m>, the
      series would equal <m>0</m>.  A regrouping of
      <m>1+(-1+1)+(-1+1)+\cdots</m> provides an answer of <m>1</m>.
      This violation of the associative law of addition did not escape
      the mathematicians of the 1700s.  In his 1760 paper <pubtitle>On
      Divergent Series</pubtitle> Euler said:
    </p>

    <blockquote>
      <p>
        <idx><h>Leibniz, Gottfried Wilhelm</h></idx>

        Notable enough, however are the controversies over the series
        <m>1-1+1-1+\text{etc}</m>, whose sum was given by Leibniz as
        <m>\frac{1}{2}</m>, although others disagree . . .
        Understanding of this question is to be sought in the word
        <q>sum;</q> this idea, if thus conceived <mdash/> namely, the sum of a
        series is said to be that quantity to which it is brought closer
        as more terms of a series are taken <mdash/> has relevance only for the
        convergent series, and we should in general give up this idea of
        sum for divergent series.  On the other hand, as series in
        analysis arise from the expansion of fractions or irrational
        quantities or even of transcendentals, it will, in turn, be
        permissible in calculation to substitute in place of such series
        that quantity out of whose development it is produced.
      </p>
    </blockquote>

    <p>
      Even with this formal approach to series, an interesting question
      arises.  The series for the antiderivative of <m>\frac{1}{1+x}</m>
      does converge for <m>x=1</m> while this one does not.
      Specifically, one antiderivative of the above series is
      <me>
        \ln(1+x)=x-\frac{1}{2}x^2+\frac{1}{3}x^3-\cdots
        </me>.
    </p>

    <p>
      If we substitute <m>x=1</m> into this series, we obtain <m>\ln
      2=1-\frac{1}{2}+\frac{1}{3}-\cdots</m>.  It is not hard to see
      that such an alternating series converges.  The following picture
      shows why.  In this diagram, <m>S_n</m> denotes the partial sum
      <m>1-\frac{1}{2}+\frac{1}{3}-\cdots+\frac{(-1)^{n+1}}{n}</m>.
    </p>

    <!-- <aside> -->
    <!--   <title>Note to self</title> -->
    <!--   <p> -->
    <!--     This image should be interactive. -->
    <!--   </p> -->
    <!-- </aside> -->

    <image width="90%" source="images/AltHarmonic.png" >
      <shortdescription></shortdescription>
    </image>

    <p>
      From the diagram we can see
      <m>S_2\leq S_4\leq S_6\leq\cdots\leq\cdots\leq S_5\leq S_3\leq S_1</m> and <m>S_{2k+1}-S_{2k}=\frac{1}{2k+1}</m>.
      It seems that the sequence of partial sums will converge to whatever is in the <q>middle.</q>
      Our diagram indicates that it is ln <m>2</m> in the middle but actually this is not obvious.
      Nonetheless it is interesting that one series converges for <m>x=1</m> but the other does not.
    </p>

    <problem>
      <statement>
        <p>
          <idx><h>series</h><h>Taylor<rsq/>s series</h><h>used to approximate <m>\ln 2</m></h></idx>
          Use the fact that
          <me>
            1-\frac{1}{2}+\frac{1}{3}-\cdots+\frac{(-1)^{2k+1}}{2k}\leq\ln 2\leq 1-\frac{1}{2}+\frac{1}{3}-\cdots+\frac{(-1)^{2k+2}}{2k+1}
          </me>
          to determine how many terms of the series
          <m>\sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}</m> should be added together to approximate <m>\ln 2</m> to within <m>.0001</m> without actually computing what <m>\ln 2</m> is.
        </p>
      </statement>
    </problem>

    <p>
      These examples illustrate something even more perplexing.  As
      we<rsq/>ve observed the divergent infinite sum
      <m>1-1+1-1+\cdots</m> does not appear satisfy the associative law
      for addition. Since the sum diverges (is meaningless) this is not
      too surprising.  On the other hand while the convergent series
      <m>1-\frac{1}{2}+\frac{1}{3}-\cdots</m> does satisfy the
      associative law as expected, it does not satisfy the
      commutative law.  In fact, <xref ref="thm_rearrangements"></xref>
      shows that it fails to be commutative rather spectacularly.
    </p>

    <p>
      <idx><h>Riemann, Bernhard</h></idx>

      A generalization of the following result was stated and proved by
      <url
          href="https://mathshistory.st-andrews.ac.uk/Biographies/Riemann/"
          visual="mathshistory.st-andrews.ac.uk/Biographies/Riemann/">Bernhard
      Riemann</url>  (1826<ndash/>1866) in 1854.
    </p>


    <figure  xml:id="FIGURERiemann">
      <caption>Bernhard Riemann</caption>
      <idx><h>Portraits</h><h>Riemann</h></idx>
      <idx><h>Riemann, Bernhard</h><h>portrait of</h></idx>

      <image source="images/Riemann.png" width="35%">
        <shortdescription>Portrait of Bernhard Riemann</shortdescription>
      </image>
    </figure>

    <theorem xml:id="thm_rearrangements">
      <idx><h>series</h><h>Alternating Harmonic Series</h><h>rearrangements of</h></idx>
      <statement>
        <p>
          Let <m>a</m> be any real number.
          There exists a rearrangement of the <term>Alternating Harmonic Series</term>
          <m>1-\frac{1}{2}+\frac{1}{3}-\cdots</m> which converges to <m>a</m>.
        </p>
      </statement>
    </theorem>

    <p>
      This theorem shows that a series is most decidedly not a great
      big sum.  It follows that power series are not just
      great big polynomials.
    </p>

    <p>
      <idx><h>series</h><h>Harmonic Series</h></idx>

      To set the stage, consider the <term>Harmonic Series</term>
      <me>
        \sum_{n=1}^\infty\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\cdots
        </me>.
    </p>

    <p>
      Even though the individual terms in this series converge to <m>0</m>,
      the series still diverges
      (to infinity)
      as evidenced by the inequality
      <md>
        <mrow>\left(1+\frac{1}{2}\right)\amp +\left(\frac{1}{3}+\frac{1}{4}\right)+\left(\frac{1}{5}+\frac{1}{6}+ \frac{1}{7}+\frac{1}{8}\right)+\left(\frac{1}{9}+\cdots+\frac{1}{16}\right)+\cdots</mrow>
        <mrow>\amp >\frac{1}{2}+\left(\frac{1}{4}+\frac{1}{4}\right)+\left(\frac{1}{8}+ \frac{1}{8}+\frac{1}{8}+\frac{1}{8}\right)+\left(\frac{1}{16}+\cdots+\frac{1}{16}\right)+\cdots</mrow>
        <mrow>\amp =\frac{1}{2}+\frac{1}{2}+\frac{1}{2}+\frac{1}{2}+\cdots</mrow>
        <mrow>\amp =   \infty</mrow>
        </md>.
    </p>

    <p>
      Armed with this fact,
      we can see why <xref ref="thm_rearrangements"></xref> is true.
      First note that
      <me>
        -\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\cdots=-\frac{1}{2}(1+\frac{1}{2}+ \frac{1}{3}+\cdots)=-\infty
      </me>
      and
      <me>
        1+\frac{1}{3}+\frac{1}{5}+\cdots\geq\frac{1}{2}+\frac{1}{4}+\frac{1}{6}+\ldots= \infty
        </me>.
    </p>

    <p>
      This says that if we add enough terms of
      <m>-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\cdots</m> we can make such a sum as small as we wish and if we add enough terms of
      <m>1+\frac{1}{3}+\frac{1}{5}+\cdots</m> we can make such a sum as large as we wish.
      This provides us with the general outline of the proof.
      The trick is to add just enough positive terms until the sum is just greater than <m>a</m>.
      Then we start to add on negative terms until the sum is just less than <m>a</m>.
      Picking up where we left off with the positive terms,
      we add on just enough positive terms until we are just above <m>a</m> again.
      We then add on negative terms until we are below <m>a</m>.
      In essence, we are bouncing back and forth around <m>a</m>.
      If we do this carefully,
      then we can get this rearrangement to converge to <m>a</m>.
      The notation in the proof below gets a bit hairy,
      but keep this general idea in mind as you read through it.
    </p>

    <p>
      Let <m>O_1</m> be the first odd integer such that <m>1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}>a</m>.
      Now choose <m>E_1</m> to be the first even integer such that
      <me>
        -\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\cdots-\frac{1}{E_1} \lt a-\left(1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}\right)
        </me>.
    </p>

    <p>
      Thus
      <me>
        1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4} - \frac{1}{6}-\cdots-\frac{1}{E_1}\lt a
        </me>.
    </p>

    <p>
      Notice that we still have <m>\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots=\infty</m>.
      With this in mind, choose <m>O_2</m> to be the first odd integer with

      <md>
        <mrow> 
          \frac{1}{O_1+2}\amp{}+\frac{1}{O_1+4}+\cdots\frac{1}{O_2}
        </mrow>
        <mrow>
          \amp{}\gt          a-\left(1+\frac{1}{3}+
          \frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\cdots-
          \frac{1}{E_1}\right).
        </mrow>

      </md>
    </p>

    <p>
      Thus we have

      <md>
        <mrow>
          a\lt
          1+\frac{1}{3}+\frac{1}{5}+\amp{}\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}-
          \frac{1}{6}-
        </mrow>
        <mrow>
          \amp{}\cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+
          \frac{1}{O_2}.
        </mrow>
      </md>
    </p>

    <p>
      Furthermore, since

      <md>
        <mrow>
          1+\frac{1}{3}+\frac{1}{5}+\amp{}\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}-
          \frac{1}{6}-
        </mrow>
        <mrow>
          \amp{}\cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+
        </mrow>
        <mrow>
          \amp{}\cdots+ \frac{1}{O_2-2}\lt a
        </mrow>

      </md>
      we have
      <md>
        <mrow> \amp{}\left|1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}- \frac{1}{6}-\cdots\right.</mrow>
        <mrow>\amp \left.-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+ \frac{1}{O_2}-a\right|\lt \frac{1}{O_2}</mrow>
      </md>
    </p>

    <p>
      In a similar fashion choose <m>E_2</m> to be the first even integer such that
      <md>
        <mrow>1+\frac{1}{3}+\frac{1}{5}+\cdots\amp +\frac{1}{O_1}-\frac{1}{2}- \frac{1}{4}-\frac{1}{6}-</mrow>
        <mrow>\amp \cdots -\frac{1}{E_1}+ \frac{1}{O_1+2}+\frac{1}{O_1+4}+</mrow>
        <mrow>\amp \cdots+\frac{1}{O_2}-\frac{1}{E_1+2}-\frac{1}{E_1+4}-</mrow>
        <mrow>\amp \cdots-\frac{1}{E_2}\lt a</mrow>
        </md>.
    </p>

    <p>
      Since
      <md>
        <mrow>1+\frac{1}{3}+\frac{1}{5} +\cdots\amp{}+\frac{1}{O_1}-\frac{1}{2}- \frac{1}{4}-\frac{1}{6}-</mrow>
        <mrow>
          \cdots\amp{}-\frac{1}{E_1}+ \frac{1}{O_1+2}+\frac{1}{O_1+4}+
        </mrow>
        <mrow>
        \cdots\amp{}+\frac{1}{O_2}- \frac{1}{E_1+2}-\frac{1}{E_1+4}-\cdots-\frac{1}{E_2-2}>a</mrow>
      </md>
      then
      <md>
        <mrow>
          \left|1+\frac{1}{3}\right.+\frac{1}{5}+
          \amp \cdots+\frac{1}{O_1}-\frac{1}{2}-
          \frac{1}{4}-\frac{1}{6}-
        </mrow>
        <mrow>
          \amp{}\cdots-\frac{1}{E_1} +\frac{1}{O_1+2}+\frac{1}{O_1+4}+
        </mrow>
        <mrow>
          \amp{}\cdots+\frac{1}{O_2}-
          \frac{1}{E_1+2}-\frac{1}{E_1+4}-
        </mrow>
        <mrow>
          \amp{}\cdots-\left.\frac{1}{E_2}-a\right|
        \lt \frac{1}{E_2}.</mrow>
      </md>


    </p>

    <p>
      Again choose <m>O_3</m> to be the first odd integer such that
      <md>
        <mrow>
          a\lt 1+\frac{1}{3} +\frac{1}{5}+
          \amp{}\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}- \frac{1}{6}-
        </mrow>
        <mrow>
          \amp{}\cdots
          -\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+
        </mrow>
        <mrow>
          \amp{}\cdots+\frac{1}{E_1+2}-\frac{1}{E_1+4}-\cdots -\frac{1}{E_2} +
        </mrow>
        <mrow>
          \amp{} \cdots \frac{1}{O_2+2} + \frac{1}{O_2+4}\cdots+\frac{1}{O_3}
        </mrow>
      </md>
      and notice that
      <md>
        <mrow>
          \left|1+\frac{1}{3}\right.
          +\frac{1}{5}+\amp{}\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-
        </mrow>
        <mrow>
          \amp
          \cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+
        </mrow>
        <mrow>
          \amp \cdots+ \frac{1}{O_2}+
          -\frac{1}{E_1+2}-\frac{1}{E_1+4}-
        </mrow>
        <mrow>
          \amp{}\cdots-\frac{1}{E_2}+\frac{1}{O_2+2}+
          \frac{1}{O_2+4}+\cdots+\left.\frac{1}{O_3}-a\right|
        </mrow>
        <mrow>
          \amp \lt \frac{1}{O_3}
        </mrow>
        </md>.
    </p>

    <p>
      Continue defining <m>O_k</m> and <m>E_k</m> in this fashion.
      Since <m>\lim_{k\rightarrow\infty}\frac{1}{O_k}=\,\lim_{k\rightarrow\infty} \frac{1}{E_k}=0</m>,
      it is evident that the partial sums
      <md>
        <mrow>1+\frac{1}{3} +\frac{1}{5}+\amp{}\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-</mrow>
        <mrow>\amp \cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+</mrow>
        <mrow>\amp \cdots \frac{1}{O_2}+\cdots -\frac{1}{E_{k-2}+2}-\frac{1}{E_{k-2}+4}-</mrow>
        <mrow>\amp \cdots-\frac{1}{E_{k-1}}+  \frac{1}{O_{k-1}+2}+\frac{1}{O_{k-1}+4}+\cdots+\frac{1}{O_k}</mrow>
      </md>
      and
      <md>
        <mrow>
          1+\frac{1}{3}\amp
          +\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}-
          \frac{1}{6}-
        </mrow>
        <mrow>
          \amp \cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+
        \cdots{} \frac{1}{O_2}+</mrow>
        <mrow>
          \amp
          \cdots-\frac{1}{E_{k-2}+2}-\frac{1}{E_{k-2}+4}-\cdots-\frac{1}{E_{k-1}}
        </mrow>
      </md>
      must converge to <m>a</m>.
      Furthermore,
      it is evident that every partial sum of the rearrangement
      <md>
        <mrow>1+\frac{1}{3} +\frac{1}{5}+\amp\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}- \frac{1}{6}-</mrow>
        <mrow>\amp{}-\cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+ \frac{1}{O_2}+\cdots</mrow>
      </md>
      is trapped between two such extreme partial sums.
      This forces the entire rearranged series to converge to <m>a</m>.
    </p>

    <p>
      The next two problems are similar to the above,
      but notationally are easier since we don<rsq/>t need to worry about converging to an actual number.
      We only need to make the rearrangement grow
      (or shrink in the case of <xref ref="prob_RearrangeDivToNegInf"></xref>)
      without bound.
    </p>

    <problem>
      <idx><h>series</h><h>Alternating Harmonic Series</h><h>rearrangements of</h></idx>
      <statement>
        <p>
          Show that there is a rearrangement of the <term>Alternating Harmonic Series</term>,

          <me>1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots</me>

          which diverges to <m>\infty</m>.
        </p>
      </statement>
    </problem>

    <problem xml:id="prob_RearrangeDivToNegInf">
      <idx><h>series</h><h>Alternating Harmonic Series</h><h>rearrangements of</h></idx>
      <statement>
        <p>
          Show that there is a rearrangement of
          <term>Alternating Harmonic Series</term>,

          <me>1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots</me> which
          diverges to <m>-\infty</m>.
        </p>
      </statement>
    </problem>

    <p>
      It is fun to know that we can rearrange some series to make them
      add up to anything we like but there is a more fundamental idea
      at play here.  That the negative terms of the alternating
      Harmonic Series diverge to negative infinity and the positive
      terms diverge to positive infinity make the convergence of the
      alternating series very special.
    </p>

    <p>
      Consider, first we add <m>1</m>. This is one of the positive terms so our sum is starting to increase without bound.
      Next we add <m>-1/2</m> which is one of the negative terms so our sum has turned around and is now starting to decrease without bound.
      Then another positive term is added:
      increasing without bound.
      Then another negative term: decreasing.
      And so on.
      The convergence of the alternating Harmonic Series is the result of a delicate balance between a tendency to run off to positive infinity and back to negative infinity.
      When viewed in this light it is not really too surprising that rearranging the terms can destroy this delicate balance.
    </p>

    <p>
      Naturally, the alternating Harmonic Series is not the only such series.
      Any such series is said to converge <q>conditionally</q>
      <mdash /> the condition being the specific arrangement of the terms.
    </p>

    <p>
      To stir the pot a bit more, some series do satisfy the commutative property.
      More specifically, one can show that any rearrangement of the series
      <m>1-\frac{1}{2^2}+\frac{1}{3^2}-\cdots</m> must converge to the same value as the original series (which happens to be <m>\int_{x=0}^1\frac{\text{ ln } (1+x)}{x}dx\approx.8224670334</m>).
      Why does one series behave so nicely whereas the other does not?
    </p>

    <p>
      Issues such as these and, more generally, the validity of using
      the infinitely small and infinitely large certainly existed in the
      1700s, but they were overshadowed by the utility of the Calculus.
      Indeed, foundational questions raised by the above examples, while
      certainly interesting and of importance, did not significantly
      deter the exploitation of Calculus in studying physical phenomena.
      However, the envelope eventually was pushed to the point that not
      even the most practically oriented mathematician could avoid the
      foundational issues.
    </p>
  </section>

  <!-- <section  xml:id="PowerSeriesQuestions-AddProb"> -->
  <!--   <title>Additional Problems</title> -->
  <!--   <problem> -->
  <!--     <idx><h>Taylor<rsq/>s Formula</h><h>drill problems </h></idx> -->
  <!--     <introduction> -->
  <!--       <p> -->
  <!--         Use Taylor<rsq/>s formula to find the Taylor series of the -->
  <!--         given function expanded about the given point <m>a</m>. -->
  <!--       </p> -->
  <!--     </introduction> -->
  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           <m>f(x)=\ln\left(1+x\right)</m>, <m>a=0</m> -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--     </task> -->
  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           <m>f(x)=e^x</m>, <m>a=-1</m> -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--     </task> -->
  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           <m>f(x)=x^3+x^2+x+1</m>, <m>a=0</m> -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--     </task> -->
  <!--     <task> -->
  <!--       <statement> -->
  <!--         <p> -->
  <!--           <m>f(x)=x^3+x^2+x+1</m>, <m>a=1</m> -->
  <!--         </p> -->
  <!--       </statement> -->
  <!--     </task> -->
  <!--   </problem> -->

  <!-- </section> -->



</chapter>

